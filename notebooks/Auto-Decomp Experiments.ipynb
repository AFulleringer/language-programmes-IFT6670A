{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a803c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "with open(os.path.expanduser('~/.openai_api_key'), 'r') as file:\n",
    "    openai.api_key = file.read().replace('\\n', '')\n",
    "\n",
    "import adatest\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import seqio\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = \"/etc/ssl/certs/ca-bundle.crt\"\n",
    "from bigbench.bbseqio import tasks\n",
    "vocabulary=seqio.SentencePieceVocabulary(\"/gscratch/zlab/bparan/projects/cascades/models/t5-spiece.model\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "# from utils.constants import OPENAI_API_KEY\n",
    "\n",
    "import tqdm\n",
    "\n",
    "with open(os.path.expanduser('~/.openai_api_key'), 'r') as file:\n",
    "    openai.api_key = file.read().replace('\\n', '')\n",
    "print(openai.api_key)\n",
    "\n",
    "cache_dir = '/gscratch/zlab/bparan/projects/cascades/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862d9d6",
   "metadata": {},
   "source": [
    "### GPT-3 Model for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65579be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIModel(adatest.Model):\n",
    "    def __init__(self, model=\"text-davinci-002\", quote=\"\\\"\", temperature=0.7, top_p=1, max_length=30, n=1):\n",
    "        self.model = model\n",
    "        self.api_key = openai.api_key\n",
    "        self.quote = quote\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.max_length = max_length\n",
    "        self.n = n\n",
    "    def __call__(self, strings):\n",
    "        resp = openai.Completion.create(\n",
    "            model=self.model,\n",
    "            prompt=strings,\n",
    "            max_tokens=self.max_length,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            n=self.n,\n",
    "            stop=self.quote,\n",
    "        )\n",
    "        return [x[\"text\"] for x in resp['choices']]\n",
    "\n",
    "gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=200, quote='', n=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a1631",
   "metadata": {},
   "source": [
    "### Prompt to propose an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7717d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_decomposition(decomp_prompt, io_pairs, n=20):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=n)\n",
    "    prompt = '''%s. Here are examples of input-output pairs for the task I'm trying to break down.\n",
    "----\n",
    "%s\n",
    "----\n",
    "Steps:\n",
    "1.'''%(decomp_prompt, io_pairs)\n",
    "    return gpt3(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf5052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_instruction(instruct_prompt, io_pairs, n=20):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=n)\n",
    "    prompt = '''%s. Here are examples of input-output pairs for this task.\n",
    "----\n",
    "%s\n",
    "----\n",
    "I can do this task by'''%(instruct_prompt, io_pairs)\n",
    "    return gpt3(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca45444",
   "metadata": {},
   "source": [
    "### Automatic Decomposition Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2af46cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941160e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(inputs, labels, n=100):\n",
    "    idxs = np.random.choice(len(inputs), n, replace=False)\n",
    "    labs = np.array([labels[i] for i in idxs])\n",
    "    subset = [inputs[i] for i in idxs]\n",
    "    return labs, subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1225c",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "\n",
    "For each tasks, we compute:\n",
    "* Best human decomposition performance over N runs: Known decomps or ones that we come up with. A further variant of this is (a) Decompositing into individual GPT-3 calls with few-shot prompting (decompositional prompting) and (b) Making and integrating external affordance calls when needed.\n",
    "* Automatic instruction generation (APE): Reporting on top-K instructions. APE reports average over top-10 for 200 instructions. They also have an efficient score estimation technique whereby promising candidates (evaluated based on a small subset) are given more compute resource. \n",
    "* Automatic decomposition generation, followed by zero-shot application to downstream task. Reporting average performance over top-k decompositions\n",
    "* \n",
    "\n",
    "Things to keep track of:\n",
    "* Evaluation metric computation\n",
    "* Generated sequence length "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f1095",
   "metadata": {},
   "source": [
    "#### Anachronisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c16e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/mmfs1/home/bparan/.cache/huggingface/datasets/bigbench/anachronisms/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe7608bbb6746b6bd4291be7e00f002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data\n",
    "d = datasets.load_dataset('bigbench', 'anachronisms')\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = np.array([int(x[0] == 'Yes') for x in d['train']['targets'] + d['validation']['targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "666ef707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Performance:\n",
      "Mean 0.7826086956521738\n",
      "Std. Dev 0.7826086956521738\n"
     ]
    }
   ],
   "source": [
    "# Human Decomp \n",
    "def anachronism(x):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=200, quote='---', n=1)\n",
    "    prompt = '''Given a sentence and the time periods of each entity in it, tell me if it could have happened or not.\n",
    "Sentence: I wrote about shakespeare\n",
    "Entities and dates:\n",
    "I -> 21st century\n",
    "Shakespeare -> 16th century\n",
    "Could the sentence be true based on the dates alone: Yes\n",
    "----\n",
    "Sentence: Shakespeare wrote about me\n",
    "\n",
    "Entities and dates:\n",
    "Shakespeare -> 16th century\n",
    "I -> 21st century\n",
    "\n",
    "Could the sentence be true based on the dates alone: No\n",
    "----\n",
    "Sentence: %s''' % x\n",
    "    return gpt3(prompt)\n",
    "\n",
    "perf_array = []\n",
    "runs = 2\n",
    "for run in range(runs): \n",
    "    answers = []\n",
    "    for x in inputs:\n",
    "        answers.append(anachronism(x))\n",
    "    preds = np.array([int(x[0].endswith('No')) for x in answers])\n",
    "    perf_array.append((preds == labels).mean())\n",
    "print(\"Human Performance:\")\n",
    "print(\"Mean\", np.mean(perf_array))\n",
    "print(\"Std. Dev\", np.mean(perf_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c685df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 0\n",
      "0.63\n",
      "Instruction 1\n",
      "0.61\n",
      "Instruction 2\n",
      "0.5\n",
      "Instruction 3\n",
      "0.58\n",
      "Instruction 4\n",
      "0.63\n",
      "Instruction 5\n",
      "0.62\n",
      "Instruction 6\n",
      "0.62\n",
      "Instruction 7\n",
      "0.58\n",
      "Instruction 8\n",
      "0.66\n",
      "Instruction 9\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "# Automatic instruction runs.\n",
    "\n",
    "instruct_prompt = 'I want to figure out whether a sentence contains anachronisms or not. An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time.'\n",
    "io_pairs = \"\"\"Input: George Washington fought in the American Civil War.\n",
    "Output: No\n",
    "Input: The Mongolian horse rider used his bow to hunt the velociraptor.\n",
    "Output: Yes\n",
    "Input: Beats from the MPC3000 helped inspire many original blues artists.\n",
    "Output: No\n",
    "Input: Attila the Hun acted in the live-action remake of Mulan.\n",
    "Output: Yes\n",
    "Input: Kurt Cobain starred in the 1990 television show \"Twin Peaks\".\n",
    "Output: Yes\"\"\"\n",
    "\n",
    "instructions = propose_instruction(instruct_prompt, io_pairs, 50)\n",
    "\n",
    "def get_anachronism_ape_fn(instruction, batch_size=10):\n",
    "#     decomposition = '1.'+ decomposition\n",
    "#     last_n = int(re.findall(r'(\\d+)\\.', decomposition)[-1])\n",
    "#     decomposition += '\\n%s. Output YES if there is an anachronism, and NO otherwise' % (last_n + 1)\n",
    "    instruction = instruction.strip()\n",
    "    def decomposition_ape_fn(sentences):\n",
    "        gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=1)\n",
    "        out = []\n",
    "        for chunk in chunks(sentences, batch_size):\n",
    "            prompts = ['''An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time. Figure out whether a sentence contains anachronisms or not, using this instruction.\n",
    "Instruction:\n",
    "%s\n",
    "----\n",
    "Sentence: %s\n",
    "Is this an Anachronism? Output YES if there is an anachronism, and NO otherwise.''' % (instruction, x) for x in chunk]\n",
    "            out.extend(gpt3(prompts))\n",
    "        return out\n",
    "    return decomposition_ape_fn\n",
    "\n",
    "labs, subset = get_subset(inputs, labels, n=100)\n",
    "all_preds = []\n",
    "pps = []\n",
    "accs = []\n",
    "for z, instruction in enumerate(instructions):\n",
    "    print('Instruction', z)\n",
    "    fn = get_anachronism_ape_fn(instruction, batch_size=20)\n",
    "    this_preds = fn(subset)\n",
    "    pp = np.array([1 if 'yes' in x.lower() else 0 for x in this_preds])\n",
    "    all_preds.append(this_preds)\n",
    "    pps.append(pp)\n",
    "    accs.append((pp==labs).mean())\n",
    "    print((pp==labs).mean())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2efca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition 0\n",
      "0.55\n",
      "Decomposition 1\n",
      "0.66\n",
      "Decomposition 2\n",
      "0.59\n",
      "Decomposition 3\n",
      "0.57\n",
      "Decomposition 4\n",
      "0.54\n",
      "Decomposition 5\n",
      "0.64\n",
      "Decomposition 6\n",
      "0.6\n",
      "Decomposition 7\n",
      "0.59\n",
      "Decomposition 8\n",
      "0.66\n",
      "Decomposition 9\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "# Automatic decomposition runs\n",
    "\n",
    "decomp_prompt = 'I want to break down the task of figuring out whether a sentence contains anachronisms or not, into individual steps. An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time.'\n",
    "decompositions = propose_decomposition(decomp_prompt, io_pairs, 10)\n",
    "\n",
    "def get_anachronism_fn(decomposition, batch_size=10):\n",
    "    decomposition = '1.'+ decomposition\n",
    "    last_n = int(re.findall(r'(\\d+)\\.', decomposition)[-1])\n",
    "#     decomposition += '\\n%s. Output YES if there is an anachronism, and NO otherwise' % (last_n + 1)\n",
    "    def decomposition_fn(sentences):\n",
    "        gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=1)\n",
    "        out = []\n",
    "        for chunk in chunks(sentences, batch_size):\n",
    "            prompts = ['''Figure out whether a sentence contains anachronisms or not, using the following steps\n",
    "Steps:\n",
    "%s\n",
    "----\n",
    "Sentence: %s\n",
    "Is this an Anachronism? Show me how you arrived at this answer step-wise. Output YES if there is an anachronism, and NO otherwise.''' % (decomposition, x) for x in chunk]\n",
    "            out.extend(gpt3(prompts))\n",
    "        return out\n",
    "    return decomposition_fn\n",
    "\n",
    "\n",
    "labs, subset = get_subset(inputs, labels, n=100)\n",
    "preds = []\n",
    "pps = []\n",
    "accs = []\n",
    "all_preds = []\n",
    "for z, decomposition in enumerate(decompositions):\n",
    "    print('Decomposition', z)\n",
    "    fn = get_anachronism_fn(decomposition, batch_size=20)\n",
    "    this_preds = fn(subset)\n",
    "#     pp = np.array([1 if 'contains an anachronism' in x.lower() else 0 for x in this_preds])\n",
    "    pp = np.array([1 if 'yes' in x.lower() else 0 for x in this_preds])\n",
    "    preds.append(this_preds)\n",
    "    pps.append(pp)\n",
    "    accs.append((pp==labs).mean())\n",
    "    print((pp==labs).mean())\n",
    "    all_preds.append(this_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34858992",
   "metadata": {},
   "source": [
    "#### Dataset from decomposed prompting (K'th letter concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec13d1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Take the letters at position 3 of the words in \"Musa Haiying Schmidt Robinson Afzal\" and concatenate them using a space.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/allenai/DecomP/main/datasets/letter_cat/n5_eg100_pos2_space.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "inputs = [d['question'] for d in data['1']['qa_pairs']]\n",
    "labels = [d['answer']['spans'][0] for d in data['1']['qa_pairs']]\n",
    "len(data['1']['qa_pairs'])\n",
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f54c8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:13,  1.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual decomposition\n",
    "out = []\n",
    "batch_size = 10\n",
    "for chunk in tqdm.tqdm(chunks(inputs, batch_size)):\n",
    "    prompts = [x for x in chunk]\n",
    "#     print(prompts)\n",
    "    out.extend(gpt3(prompts))\n",
    "pp = np.array([1 if p.strip().lower() == l else 0 for p, l in zip(out, labels)])\n",
    "pp.sum()/len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5235b50",
   "metadata": {},
   "source": [
    "#### Dataset from decomposed prompting (List reversal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "820509d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/allenai/DecomP/main/datasets/reverse/test_10_normal_words.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "inputs = [d['question'] for d in data['alg_qa']['qa_pairs']]\n",
    "labels = [d['answer']['spans'][0] for d in data['alg_qa']['qa_pairs']]\n",
    "# len(data['1']['qa_pairs'])\n",
    "len(data['alg_qa']['qa_pairs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09e0afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"banknote, sweet, phone card, identity card, credit card, case, passport, newspaper, painkiller, pen\".', 'Reverse the sequence \"bottle, passport, key, toothbrush, mobile phone, notebook, light bulb, tissue, packet, magazine\".', 'Reverse the sequence \"chewing gum, coin, driving licence, file, headphone, alarm clock, camera, rubbish, case, toothbrush\".', 'Reverse the sequence \"comb, sunscreen, key, postcard, packet, button, stamp, purse, photo, pen\".', 'Reverse the sequence \"rubber, banknote, watch, wallet, phone card, sweet, mirror, alarm clock, comb, tissue\".', 'Reverse the sequence \"magazine, tissue, headphone, stamp, file, banknote, passport, lipstick, diary, watch\".', 'Reverse the sequence \"glasses, rubbish, phone card, diary, wallet, tissue, laptop, toothbrush, battery, chewing gum\".', 'Reverse the sequence \"mirror, magazine, rubber, banknote, dictionary, case, pen, mobile phone, light bulb, tissue\".', 'Reverse the sequence \"comb, notebook, banknote, lipstick, dictionary, wallet, clip, light bulb, sweet, mobile phone\".', 'Reverse the sequence \"pen, painkiller, rubbish, watch, toothbrush, clip, notebook, comb, lipstick, banknote\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:02,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"key, sweet, credit card, watch, battery, chewing gum, player, sunscreen, passport, stamp\".', 'Reverse the sequence \"banknote, postcard, wallet, player, diary, identity card, clip, case, packet, magazine\".', 'Reverse the sequence \"laptop, chewing gum, painkiller, identity card, toothbrush, diary, scissors, bin, photo, sweet\".', 'Reverse the sequence \"sweet, lipstick, laptop, camera, water, alarm clock, driving licence, painkiller, identity card, tissue\".', 'Reverse the sequence \"diary, banknote, lipstick, stamp, phone card, laptop, battery, bottle, wallet, magazine\".', 'Reverse the sequence \"chewing gum, identity card, button, purse, brush, lighter, notebook, credit card, light bulb, photo\".', 'Reverse the sequence \"brush, water, match, passport, sweet, key, driving licence, laptop, sunscreen, umbrella\".', 'Reverse the sequence \"match, stamp, tissue, glasses, rubbish, magazine, coin, phone card, comb, key\".', 'Reverse the sequence \"light bulb, file, banknote, comb, camera, alarm clock, lighter, stamp, bin, button\".', 'Reverse the sequence \"wallet, lighter, coin, toothbrush, key, watch, phone card, scissors, passport, rubber\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"magazine, camera, pen, identity card, water, clip, button, purse, packet, headphone\".', 'Reverse the sequence \"phone card, sweet, player, newspaper, file, pencil, button, chewing gum, toothbrush, magazine\".', 'Reverse the sequence \"alarm clock, pencil, coin, chewing gum, player, scissors, wallet, magazine, cigarette, notebook\".', 'Reverse the sequence \"rubbish, diary, painkiller, toothbrush, pen, pencil, match, stamp, dictionary, lipstick\".', 'Reverse the sequence \"umbrella, sweet, mirror, light bulb, watch, mobile phone, pen, lighter, magazine, phone card\".', 'Reverse the sequence \"button, wallet, alarm clock, phone card, tissue, toothbrush, headphone, case, comb, bin\".', 'Reverse the sequence \"cigarette, laptop, banknote, umbrella, case, lighter, diary, headphone, toothbrush, coin\".', 'Reverse the sequence \"water, postcard, glasses, rubbish, comb, purse, rubber, wallet, magazine, laptop\".', 'Reverse the sequence \"glasses, magazine, light bulb, coin, bottle, water, passport, battery, file, bin\".', 'Reverse the sequence \"lipstick, clip, button, passport, glasses, postcard, brush, cigarette, scissors, laptop\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:06,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"brush, postcard, coin, cigarette, match, lipstick, file, headphone, purse, water\".', 'Reverse the sequence \"mobile phone, battery, comb, magazine, chewing gum, headphone, lighter, rubber, alarm clock, passport\".', 'Reverse the sequence \"brush, notebook, water, credit card, button, dictionary, mirror, identity card, watch, camera\".', 'Reverse the sequence \"comb, camera, pen, postcard, light bulb, photo, pencil, coin, mirror, banknote\".', 'Reverse the sequence \"brush, magazine, rubbish, identity card, bin, player, headphone, phone card, notebook, watch\".', 'Reverse the sequence \"pencil, magazine, battery, passport, brush, watch, packet, notebook, clip, painkiller\".', 'Reverse the sequence \"scissors, file, wallet, rubbish, bottle, tissue, stamp, pen, coin, phone card\".', 'Reverse the sequence \"lighter, umbrella, match, scissors, bottle, key, newspaper, diary, mobile phone, stamp\".', 'Reverse the sequence \"stamp, light bulb, rubber, clip, banknote, player, pen, button, sweet, laptop\".', 'Reverse the sequence \"toothbrush, sweet, newspaper, rubber, magazine, scissors, coin, purse, chewing gum, pencil\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:08,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"alarm clock, scissors, postcard, rubber, comb, diary, bin, painkiller, notebook, sunscreen\".', 'Reverse the sequence \"rubber, bin, postcard, brush, pen, magazine, tissue, rubbish, passport, glasses\".', 'Reverse the sequence \"lighter, photo, postcard, laptop, banknote, coin, button, key, toothbrush, packet\".', 'Reverse the sequence \"pen, brush, painkiller, postcard, key, clip, umbrella, bin, water, purse\".', 'Reverse the sequence \"driving licence, credit card, watch, sweet, wallet, mirror, scissors, sunscreen, clip, alarm clock\".', 'Reverse the sequence \"photo, key, painkiller, dictionary, water, lighter, comb, rubbish, cigarette, coin\".', 'Reverse the sequence \"identity card, camera, rubbish, photo, watch, sunscreen, light bulb, file, coin, sweet\".', 'Reverse the sequence \"comb, sweet, notebook, dictionary, phone card, lipstick, pencil, clip, battery, packet\".', 'Reverse the sequence \"headphone, cigarette, key, purse, phone card, laptop, bin, button, wallet, water\".', 'Reverse the sequence \"lighter, headphone, identity card, magazine, player, rubber, button, pencil, credit card, banknote\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:09,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"rubber, toothbrush, button, case, pencil, passport, diary, bottle, bin, notebook\".', 'Reverse the sequence \"toothbrush, match, brush, file, case, pencil, player, camera, laptop, wallet\".', 'Reverse the sequence \"diary, magazine, stamp, cigarette, player, light bulb, painkiller, phone card, coin, driving licence\".', 'Reverse the sequence \"pen, sunscreen, mirror, stamp, credit card, light bulb, chewing gum, dictionary, banknote, identity card\".', 'Reverse the sequence \"brush, credit card, driving licence, newspaper, rubbish, purse, mobile phone, scissors, bin, light bulb\".', 'Reverse the sequence \"magazine, mirror, stamp, match, player, pen, laptop, clip, photo, watch\".', 'Reverse the sequence \"chewing gum, pencil, button, camera, postcard, passport, dictionary, key, water, battery\".', 'Reverse the sequence \"file, clip, comb, water, key, pencil, postcard, bottle, diary, battery\".', 'Reverse the sequence \"case, mirror, stamp, credit card, dictionary, light bulb, headphone, pencil, passport, bin\".', 'Reverse the sequence \"battery, glasses, magazine, scissors, phone card, dictionary, lighter, light bulb, mirror, wallet\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:11,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"alarm clock, toothbrush, rubbish, file, glasses, mirror, purse, scissors, light bulb, identity card\".', 'Reverse the sequence \"mirror, tissue, purse, driving licence, credit card, lighter, glasses, pencil, wallet, bin\".', 'Reverse the sequence \"sunscreen, wallet, diary, match, dictionary, glasses, comb, brush, mirror, credit card\".', 'Reverse the sequence \"headphone, wallet, file, mirror, identity card, alarm clock, rubbish, laptop, painkiller, tissue\".', 'Reverse the sequence \"coin, key, photo, light bulb, stamp, painkiller, clip, newspaper, scissors, purse\".', 'Reverse the sequence \"alarm clock, painkiller, tissue, rubbish, camera, newspaper, sunscreen, scissors, button, battery\".', 'Reverse the sequence \"umbrella, light bulb, photo, clip, phone card, alarm clock, battery, wallet, banknote, rubbish\".', 'Reverse the sequence \"bottle, identity card, alarm clock, laptop, rubbish, purse, watch, wallet, lighter, photo\".', 'Reverse the sequence \"camera, match, lipstick, sweet, identity card, scissors, alarm clock, clip, diary, water\".', 'Reverse the sequence \"painkiller, button, brush, diary, purse, bottle, umbrella, coin, stamp, rubber\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:13,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"purse, battery, umbrella, magazine, notebook, chewing gum, mobile phone, pencil, file, stamp\".', 'Reverse the sequence \"bottle, clip, diary, camera, mirror, purse, cigarette, banknote, button, alarm clock\".', 'Reverse the sequence \"bin, driving licence, pen, passport, button, glasses, rubber, comb, notebook, tissue\".', 'Reverse the sequence \"magazine, packet, rubbish, clip, lighter, scissors, banknote, credit card, rubber, button\".', 'Reverse the sequence \"light bulb, sweet, bottle, scissors, alarm clock, newspaper, water, toothbrush, sunscreen, coin\".', 'Reverse the sequence \"pen, wallet, purse, watch, photo, scissors, light bulb, match, comb, coin\".', 'Reverse the sequence \"banknote, glasses, dictionary, painkiller, identity card, passport, bottle, light bulb, bin, chewing gum\".', 'Reverse the sequence \"driving licence, tissue, scissors, coin, dictionary, stamp, rubbish, laptop, pen, identity card\".', 'Reverse the sequence \"pencil, case, photo, postcard, lipstick, passport, newspaper, pen, rubber, chewing gum\".', 'Reverse the sequence \"laptop, dictionary, case, lipstick, passport, key, photo, chewing gum, brush, pencil\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:14,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverse the sequence \"watch, clip, file, phone card, painkiller, light bulb, camera, identity card, rubbish, sweet\".', 'Reverse the sequence \"packet, dictionary, driving licence, battery, photo, painkiller, laptop, mobile phone, purse, file\".', 'Reverse the sequence \"key, cigarette, umbrella, purse, identity card, clip, photo, headphone, bin, banknote\".', 'Reverse the sequence \"case, phone card, lighter, headphone, diary, comb, rubber, coin, stamp, toothbrush\".', 'Reverse the sequence \"newspaper, photo, postcard, watch, bin, coin, case, brush, sweet, key\".', 'Reverse the sequence \"purse, camera, banknote, lighter, diary, match, postcard, lipstick, phone card, tissue\".', 'Reverse the sequence \"case, bottle, sunscreen, light bulb, tissue, battery, stamp, glasses, mobile phone, cigarette\".', 'Reverse the sequence \"bottle, packet, water, painkiller, key, driving licence, cigarette, brush, banknote, identity card\".', 'Reverse the sequence \"pencil, laptop, painkiller, mobile phone, dictionary, bottle, sweet, pen, camera, postcard\".', 'Reverse the sequence \"driving licence, glasses, umbrella, match, phone card, notebook, camera, pen, painkiller, dictionary\".']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:16,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual decomposition\n",
    "out = []\n",
    "batch_size = 10\n",
    "for chunk in tqdm.tqdm(chunks(inputs, batch_size)):\n",
    "    prompts = [x for x in chunk]\n",
    "#     print(prompts)\n",
    "    out.extend(gpt3(prompts))\n",
    "pp = np.array([1 if p.strip().lower() == l else 0 for p, l in zip(out, labels)])\n",
    "pp.sum()/len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24ff32",
   "metadata": {},
   "source": [
    "#### Tasks in Self-prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609e970",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Known Unknown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "481840a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/known_unknowns/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52907300d76248069aac4fb1f9eb2339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "d = datasets.load_dataset('bigbench', 'known_unknowns', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e7309",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Strategy QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5222b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/strategyqa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2140a1b2ffd842a4b9d07c00198a08bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "d = datasets.load_dataset('bigbench', 'strategyqa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b698c",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Strategy QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb352fb",
   "metadata": {},
   "source": [
    "#### Tasks in Auto-Cot - MAWPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9672b715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration omarxadel--MaWPS-ar-e0d2ee7eb9fa6fdb\n",
      "WARNING:datasets.builder:Found cached dataset csv (/gscratch/zlab/bparan/projects/cascades/data/omarxadel___csv/omarxadel--MaWPS-ar-e0d2ee7eb9fa6fdb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdea24f74c842a883de210717448f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset('omarxadel/MaWPS-ar', 'test', cache_dir=cache_dir)\n",
    "inputs = [list(d.values())[0] for d in data['validation']]\n",
    "labels = []\n",
    "for d in data['validation']:\n",
    "    try:\n",
    "        ans = eval(list(d.values())[1].split(\"=\")[-1].strip())\n",
    "        if isinstance(ans, int):\n",
    "            labels.append(ans)\n",
    "        elif (ans).is_integer():\n",
    "            labels.append(int(ans))\n",
    "        else:\n",
    "            labels.append(float(\"%.2f\" % ans))\n",
    "        \n",
    "    except:\n",
    "        ans = eval(list(d.values())[1].split(\"=\")[0].strip())\n",
    "        if isinstance(ans, int):\n",
    "            labels.append(ans)\n",
    "        elif (ans).is_integer():\n",
    "            labels.append(int(ans))\n",
    "        else:\n",
    "            labels.append(float(\"%.2f\" % ans))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ece69",
   "metadata": {},
   "source": [
    "#### Tasks in Auto-CoT (GSM8K) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee47c3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset gsm8k (/gscratch/zlab/bparan/projects/cascades/data/gsm8k/main/1.1.0/37bfb08b1d4fcbb01f06b03d9e1ef5f1fcbd4d3af3d08842c50d7305091285ba)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4a5d9f0d184caca424c0d917152c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset('gsm8k', 'main', cache_dir=cache_dir)['test']\n",
    "inputs = [d['question'] for d in data]\n",
    "labels = [d['answer'].split('#### ')[-1] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b454c",
   "metadata": {},
   "source": [
    "#### Tasks on Auto-CoT (AQUA-RAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b5e25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset aqua_rat (/gscratch/zlab/bparan/projects/cascades/data/aqua_rat/raw/0.0.0/fc47b9f437236ab96fc1fcb61096aa193819aedd76437893e2390ab0740a3381)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e6777075404d81981fe5fedca07100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset('aqua_rat', 'raw', cache_dir=cache_dir)['validation']\n",
    "inputs = [d['question'] + \" \".join(d['options']) for d in data]\n",
    "labels = [d['correct'] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd445f15",
   "metadata": {},
   "source": [
    "#### Tasks on Auto-CoT (Commonsense QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ea0ec427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Found cached dataset commonsense_qa (/gscratch/zlab/bparan/projects/cascades/data/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04b0e80a4ab497b899bcde76407b7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset('commonsense_qa', cache_dir=cache_dir)['validation']\n",
    "inputs = [d['question']+ \" \" + \" \".join([k + \") \" + v for k, v in zip(d['choices']['label'], d['choices']['text'])]) for d in data]\n",
    "labels = [d['answerKey'] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e47fee",
   "metadata": {},
   "source": [
    "#### AMA Tasks (From Super-Glue they include boolQ, cb, copa, multirc, record, rte, wsc, WiC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "150349c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset super_glue (/gscratch/zlab/bparan/projects/cascades/data/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612959773ee240b6aaec658ef258da4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BoolQ\n",
    "data = datasets.load_dataset('super_glue', 'boolq', cache_dir=cache_dir)['validation']\n",
    "inputs = [d['passage']+ \" \" + d['question'][0].title() + d['question'][1:]  + \"?\" for d in data]\n",
    "label_dict = {0:'False', 1:'True'}\n",
    "labels = [label_dict[d['label']] for d in data]\n",
    "# Similar transformations to be made for other Superglue tasks: cb, copa, multirc, record, rte, wsc, wic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6681b2",
   "metadata": {},
   "source": [
    "#### AMA Tasks (From Adversarial NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b048d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset anli (/gscratch/zlab/bparan/projects/cascades/data/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70388adbb624042b83e317439e0d841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can also look at dev_r2, dev_r2\n",
    "data = datasets.load_dataset('anli', cache_dir=cache_dir)['dev_r3']\n",
    "inputs = [\"Sentence1: \" + d['premise'] + \"\\nSentence2: \" +d['hypothesis'] for d in data]\n",
    "label_dict = {0:\"entailment\", 1:'neutral', 2:'contradiction'}\n",
    "labels = [label_dict[d['label']] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceca0ed",
   "metadata": {},
   "source": [
    "#### Other tasks from AMA (Classification) : Agnews, DBPedia, Amazon movie reivew and SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4bf6bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Found cached dataset ag_news (/gscratch/zlab/bparan/projects/cascades/data/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0758b6a13674b6b9deaf0514cd78d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## For others use the strings : dbpedia_14, sst2\n",
    "data = datasets.load_dataset('ag_news', cache_dir=cache_dir)['test']\n",
    "inputs = [d['text'] for d in data]\n",
    "label_dict = {0:'World', 1:'Sports', 2:'Business', 3: 'Sci/Tech'}\n",
    "labels = [label_dict[d['label']] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a1898",
   "metadata": {},
   "source": [
    "#### Tasks from reframing natural language instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9406a79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Business',\n",
       " 'Sports',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Business',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Business',\n",
       " 'World',\n",
       " 'World',\n",
       " 'Sci/Tech',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sports',\n",
       " 'Sci/Tech',\n",
       " 'World',\n",
       " ...]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
