{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a803c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/cse/bparan/tools/envs/bigben/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-13 12:46:20.792439: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-13 12:46:21.528233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-13 12:46:21.528261: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "with open(os.path.expanduser('~/.openai_api_key'), 'r') as file:\n",
    "    openai.api_key = file.read().replace('\\n', '')\n",
    "\n",
    "import adatest\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import seqio\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = \"/etc/ssl/certs/ca-bundle.crt\"\n",
    "# from bigbench.bbseqio import tasks\n",
    "vocabulary=seqio.SentencePieceVocabulary(\"/gscratch/zlab/bparan/projects/cascades/models/t5-spiece.model\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "import tqdm\n",
    "\n",
    "with open(os.path.expanduser('~/.openai_api_key'), 'r') as file:\n",
    "    openai.api_key = file.read().replace('\\n', '')\n",
    "print(openai.api_key)\n",
    "cache_dir = '/gscratch/zlab/bparan/projects/cascades/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862d9d6",
   "metadata": {},
   "source": [
    "### GPT-3 Model for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65579be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIModel(adatest.Model):\n",
    "    def __init__(self, model=\"text-davinci-002\", quote=\"\\\"\", temperature=0.7, top_p=1, max_length=30, n=1):\n",
    "        self.model = model\n",
    "        self.api_key = openai.api_key\n",
    "        self.quote = quote\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.max_length = max_length\n",
    "        self.n = n\n",
    "    def __call__(self, strings):\n",
    "        resp = openai.Completion.create(\n",
    "            model=self.model,\n",
    "            prompt=strings,\n",
    "            max_tokens=self.max_length,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            n=self.n,\n",
    "            stop=self.quote,\n",
    "        )\n",
    "        return [x[\"text\"] for x in resp['choices']]\n",
    "\n",
    "gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=200, quote='', n=1)\n",
    "\n",
    "\n",
    "def get_bb_data(task_name, task_sequence_length, zero=False, few=False, explain=False):\n",
    "    seq_length = task_sequence_length\n",
    "    # By default seqio returns 3_shot.\n",
    "    if zero:\n",
    "        task = seqio.get_mixture_or_task(task_name)\n",
    "    elif few:\n",
    "        task = seqio.get_mixture_or_task(task_name.replace('0_shot', '3_shot'))\n",
    "\n",
    "    ds = task.get_dataset(split=\"all\", sequence_length={\"inputs\": seq_length, \"targets\": seq_length})\n",
    "    instances = []\n",
    "    for enum, ex in enumerate(ds):\n",
    "        instances.append({\"guid\": enum, \n",
    "                          \"input\": ex['inputs_pretokenized'].numpy().decode(),\n",
    "                          \"label\": ex[\"targets_pretokenized\"].numpy().decode()})\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a1631",
   "metadata": {},
   "source": [
    "### Prompt to propose an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7717d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_decomposition(decomp_prompt, io_pairs, n=20):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=n)\n",
    "    prompt = '''%s. Here are examples of input-output pairs for the task I'm trying to break down.\n",
    "----\n",
    "%s\n",
    "----\n",
    "Steps:\n",
    "1.'''%(decomp_prompt, io_pairs)\n",
    "    return gpt3(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf5052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_instruction(instruct_prompt, io_pairs, n=20):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=n)\n",
    "    prompt = '''%s. Here are examples of input-output pairs for this task.\n",
    "----\n",
    "%s\n",
    "----\n",
    "I can do this task by'''%(instruct_prompt, io_pairs)\n",
    "    return gpt3(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca45444",
   "metadata": {},
   "source": [
    "### Automatic Decomposition Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af46cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941160e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(inputs, labels, n=100):\n",
    "    idxs = np.random.choice(len(inputs), n, replace=False)\n",
    "    labs = np.array([labels[i] for i in idxs])\n",
    "    subset = [inputs[i] for i in idxs]\n",
    "    return labs, subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1225c",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "\n",
    "For each tasks, we compute:\n",
    "* Best current performance using BigBench instruction\n",
    "* Best human decomposition performance over N runs: Known decomps or ones that we come up with. A further variant of this is (a) Decompositing into individual GPT-3 calls with few-shot prompting (decompositional prompting) and (b) Making and integrating external affordance calls when needed.\n",
    "* Automatic instruction generation (APE): Reporting on top-K instructions. APE reports average over top-10 for 200 instructions. They also have an efficient score estimation technique whereby promising candidates (evaluated based on a small subset) are given more compute resource. \n",
    "* Automatic decomposition generation, followed by zero-shot application to downstream task. Reporting average performance over top-k decompositions\n",
    "* Automtic decomposition: Instruction refinement and decomposition ensembling\n",
    "* Potential affordance calls and decompsoitions with those calls. \n",
    "* Human-LLM collaborative decompositions\n",
    "\n",
    "Things to keep track of:\n",
    "* Evaluation metric computation\n",
    "* Generated sequence length \n",
    "* Fitting in as many decompositions into the prompts as possible into context window\n",
    "* Temperature (greedy vs random recoding as the quality of program improves)\n",
    "\n",
    "Things to think about:\n",
    "* Affordance calls and their integration into the decomposition\n",
    "* Human GPT-3 Collaboration for decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f1095",
   "metadata": {},
   "source": [
    "#### Anachronisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c16e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/mmfs1/home/bparan/.cache/huggingface/datasets/bigbench/anachronisms/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1139.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "d = datasets.load_dataset('bigbench', 'anachronisms')\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "608a515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Syngman Rhee sent a letter commending Hugo Chavez's election victory.\n",
      "Yes\n",
      "----\n",
      "The recognition of Christianity as the official religion of both Ethiopia and the Roman Empire within the same decade is notable.\n",
      "Yes\n",
      "----\n",
      "President Woodrow Wilson rallied Americans to support the U.S. joining the International Atomic Energy Agency.\n",
      "Yes\n",
      "----\n",
      "The T. rex was running toward the herd of Wagyu cattle grazing outside.\n",
      "Yes\n",
      "----\n",
      "Sun Tzu dedicated an entire chapter to describing the failure of Babylon.\n",
      "No\n",
      "----\n",
      "Igor Stravinsky's favorite musical piece was the Symphonie Fantastique.\n",
      "No\n",
      "----\n",
      "The Hagia Sophia has seen drastic transformations to its interior from its inception, including becoming a church, a mosque, a museum, and back to a mosque again. \n",
      "No\n",
      "----\n",
      "Ponce De Leon used a telegram to report his findings to the king.\n",
      "Yes\n",
      "----\n",
      "Jason connected his new TRS80 color computer to the TV and played Pyramid 2000.\n",
      "No\n",
      "----\n",
      "Richard III used LEDs to light his throne room.\n",
      "Yes\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "666ef707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Performance:\n",
      "Mean 0.7826086956521738\n",
      "Std. Dev 0.7826086956521738\n"
     ]
    }
   ],
   "source": [
    "# Human Decomp \n",
    "def anachronism(x):\n",
    "    gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=200, quote='---', n=1)\n",
    "    prompt = '''Given a sentence and the time periods of each entity in it, tell me if it could have happened or not.\n",
    "Sentence: I wrote about shakespeare\n",
    "Entities and dates:\n",
    "I -> 21st century\n",
    "Shakespeare -> 16th century\n",
    "Could the sentence be true based on the dates alone: Yes\n",
    "----\n",
    "Sentence: Shakespeare wrote about me\n",
    "\n",
    "Entities and dates:\n",
    "Shakespeare -> 16th century\n",
    "I -> 21st century\n",
    "\n",
    "Could the sentence be true based on the dates alone: No\n",
    "----\n",
    "Sentence: %s''' % x\n",
    "    return gpt3(prompt)\n",
    "\n",
    "perf_array = []\n",
    "runs = 2\n",
    "for run in range(runs): \n",
    "    answers = []\n",
    "    for x in inputs:\n",
    "        answers.append(anachronism(x))\n",
    "    preds = np.array([int(x[0].endswith('No')) for x in answers])\n",
    "    perf_array.append((preds == labels).mean())\n",
    "print(\"Human Performance:\")\n",
    "print(\"Mean\", np.mean(perf_array))\n",
    "print(\"Std. Dev\", np.mean(perf_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c685df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction 0\n",
      "0.63\n",
      "Instruction 1\n",
      "0.61\n",
      "Instruction 2\n",
      "0.5\n",
      "Instruction 3\n",
      "0.58\n",
      "Instruction 4\n",
      "0.63\n",
      "Instruction 5\n",
      "0.62\n",
      "Instruction 6\n",
      "0.62\n",
      "Instruction 7\n",
      "0.58\n",
      "Instruction 8\n",
      "0.66\n",
      "Instruction 9\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "# Automatic instruction runs.\n",
    "\n",
    "instruct_prompt = 'I want to figure out whether a sentence contains anachronisms or not. An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time.'\n",
    "io_pairs = \"\"\"Input: George Washington fought in the American Civil War.\n",
    "Output: No\n",
    "Input: The Mongolian horse rider used his bow to hunt the velociraptor.\n",
    "Output: Yes\n",
    "Input: Beats from the MPC3000 helped inspire many original blues artists.\n",
    "Output: No\n",
    "Input: Attila the Hun acted in the live-action remake of Mulan.\n",
    "Output: Yes\n",
    "Input: Kurt Cobain starred in the 1990 television show \"Twin Peaks\".\n",
    "Output: Yes\"\"\"\n",
    "\n",
    "instructions = propose_instruction(instruct_prompt, io_pairs, 50)\n",
    "\n",
    "def get_anachronism_ape_fn(instruction, batch_size=10):\n",
    "#     decomposition = '1.'+ decomposition\n",
    "#     last_n = int(re.findall(r'(\\d+)\\.', decomposition)[-1])\n",
    "#     decomposition += '\\n%s. Output YES if there is an anachronism, and NO otherwise' % (last_n + 1)\n",
    "    instruction = instruction.strip()\n",
    "    def decomposition_ape_fn(sentences):\n",
    "        gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=1)\n",
    "        out = []\n",
    "        for chunk in chunks(sentences, batch_size):\n",
    "            prompts = ['''An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time. Figure out whether a sentence contains anachronisms or not, using this instruction.\n",
    "Instruction:\n",
    "%s\n",
    "----\n",
    "Sentence: %s\n",
    "Is this an Anachronism? Output YES if there is an anachronism, and NO otherwise.''' % (instruction, x) for x in chunk]\n",
    "            out.extend(gpt3(prompts))\n",
    "        return out\n",
    "    return decomposition_ape_fn\n",
    "\n",
    "labs, subset = get_subset(inputs, labels, n=100)\n",
    "all_preds = []\n",
    "pps = []\n",
    "accs = []\n",
    "for z, instruction in enumerate(instructions):\n",
    "    print('Instruction', z)\n",
    "    fn = get_anachronism_ape_fn(instruction, batch_size=20)\n",
    "    this_preds = fn(subset)\n",
    "    pp = np.array([1 if 'yes' in x.lower() else 0 for x in this_preds])\n",
    "    all_preds.append(this_preds)\n",
    "    pps.append(pp)\n",
    "    accs.append((pp==labs).mean())\n",
    "    print((pp==labs).mean())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2efca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition 0\n",
      "0.55\n",
      "Decomposition 1\n",
      "0.66\n",
      "Decomposition 2\n",
      "0.59\n",
      "Decomposition 3\n",
      "0.57\n",
      "Decomposition 4\n",
      "0.54\n",
      "Decomposition 5\n",
      "0.64\n",
      "Decomposition 6\n",
      "0.6\n",
      "Decomposition 7\n",
      "0.59\n",
      "Decomposition 8\n",
      "0.66\n",
      "Decomposition 9\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "# Automatic decomposition runs\n",
    "\n",
    "decomp_prompt = 'I want to break down the task of figuring out whether a sentence contains anachronisms or not, into individual steps. An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time.'\n",
    "decompositions = propose_decomposition(decomp_prompt, io_pairs, 10)\n",
    "\n",
    "def get_anachronism_fn(decomposition, batch_size=10):\n",
    "    decomposition = '1.'+ decomposition\n",
    "    last_n = int(re.findall(r'(\\d+)\\.', decomposition)[-1])\n",
    "#     decomposition += '\\n%s. Output YES if there is an anachronism, and NO otherwise' % (last_n + 1)\n",
    "    def decomposition_fn(sentences):\n",
    "        gpt3 = OpenAIModel(model=\"text-davinci-002\",  max_length=400, quote='---', n=1)\n",
    "        out = []\n",
    "        for chunk in chunks(sentences, batch_size):\n",
    "            prompts = ['''Figure out whether a sentence contains anachronisms or not, using the following steps\n",
    "Steps:\n",
    "%s\n",
    "----\n",
    "Sentence: %s\n",
    "Is this an Anachronism? Show me how you arrived at this answer step-wise. Output YES if there is an anachronism, and NO otherwise.''' % (decomposition, x) for x in chunk]\n",
    "            out.extend(gpt3(prompts))\n",
    "        return out\n",
    "    return decomposition_fn\n",
    "\n",
    "\n",
    "labs, subset = get_subset(inputs, labels, n=100)\n",
    "preds = []\n",
    "pps = []\n",
    "accs = []\n",
    "all_preds = []\n",
    "for z, decomposition in enumerate(decompositions):\n",
    "    print('Decomposition', z)\n",
    "    fn = get_anachronism_fn(decomposition, batch_size=20)\n",
    "    this_preds = fn(subset)\n",
    "#     pp = np.array([1 if 'contains an anachronism' in x.lower() else 0 for x in this_preds])\n",
    "    pp = np.array([1 if 'yes' in x.lower() else 0 for x in this_preds])\n",
    "    preds.append(this_preds)\n",
    "    pps.append(pp)\n",
    "    accs.append((pp==labs).mean())\n",
    "    print((pp==labs).mean())\n",
    "    all_preds.append(this_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34858992",
   "metadata": {},
   "source": [
    "#### Dataset from decomposed prompting (K'th letter concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec13d1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/allenai/DecomP/main/datasets/letter_cat/n5_eg100_pos2_space.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "dev_inputs = [d['question'] for d in data['1']['qa_pairs']]\n",
    "dev_labels = [d['answer']['spans'][0] for d in data['1']['qa_pairs']]\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/allenai/DecomP/main/datasets/letter_cat/n4_eg100_pos2_space.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "train_inputs = [d['question'] for d in data['1']['qa_pairs']]\n",
    "train_labels = [d['answer']['spans'] for d in data['1']['qa_pairs']]\n",
    "\n",
    "len(dev_inputs)\n",
    "# dev_inputs[0]\n",
    "# len(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d26dabe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take the letters at position 3 of the words in \"Marilyn Dominique Yakubu Choe\" and concatenate them using a space.\n",
      "r m k o\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Noel Jenny Duong Awad\" and concatenate them using a space.\n",
      "e n o a\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Dmitry Vanessa Duan Tiwari\" and concatenate them using a space.\n",
      "i n a w\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Viktoriya Rekha Luna Usman\" and concatenate them using a space.\n",
      "k k n m\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Savitri Gilbert Moyo Diallo\" and concatenate them using a space.\n",
      "v l y a\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Madina Albert Dang Khatun\" and concatenate them using a space.\n",
      "d b n a\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Isa Sadia Salman Akram\" and concatenate them using a space.\n",
      "a d l r\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Julio Jay Mohammadi Batista\" and concatenate them using a space.\n",
      "l y h t\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Gopal Rodrigo Pei Zhong\" and concatenate them using a space.\n",
      "p d i o\n",
      "----\n",
      "Take the letters at position 3 of the words in \"Keith Erika Luo Haji\" and concatenate them using a space.\n",
      "i i o j\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(train_inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(train_inputs, train_labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d6799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take the letters at position 3 of the words in \"Edith Cristian Gomez Roberts\" and concatenate them using a space.\n",
      "Word: Edith\n",
      "3rd Letter: i\n",
      "Word: Cristian\n",
      "3rd Letter: i\n",
      "Word: Gomez\n",
      "3rd Letter: m\n",
      "Word: Roberts\n",
      "3rd Letter: b\n",
      "Concatenation: \n",
      "i i m b\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(train_inputs), 1)\n",
    "for id_ in idx:\n",
    "    print(train_inputs[id_])\n",
    "    words = train_inputs[id_].split(' \"')[1].split('\" ')[0].split()\n",
    "    word_list = ' '.join(words)\n",
    "    for word in words:\n",
    "        print(\"Word: %s\"%word)\n",
    "        print(\"3rd Letter: %s\"%word[2])\n",
    "    print(\"Concatenation: \\n%s\" %train_labels[id_])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f54c8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:13,  1.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual decomposition\n",
    "out = []\n",
    "batch_size = 10\n",
    "for chunk in tqdm.tqdm(chunks(inputs, batch_size)):\n",
    "    prompts = [x for x in chunk]\n",
    "#     print(prompts)\n",
    "    out.extend(gpt3(prompts))\n",
    "pp = np.array([1 if p.strip().lower() == l else 0 for p, l in zip(out, labels)])\n",
    "pp.sum()/len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5235b50",
   "metadata": {},
   "source": [
    "#### Dataset from decomposed prompting (List reversal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820509d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/allenai/DecomP/main/datasets/reverse/test_4_normal_words.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "inputs = [d['question'] for d in data['alg_qa']['qa_pairs']]\n",
    "labels = [d['answer']['spans'][0] for d in data['alg_qa']['qa_pairs']]\n",
    "# len(data['1']['qa_pairs'])\n",
    "len(data['alg_qa']['qa_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280c88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse the sequence \"camera, file, light bulb, notebook\".\n",
      "notebook, light bulb, file, camera\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 1)\n",
    "for id_ in idx:\n",
    "    print(inputs[id_])\n",
    "    print(labels[id_])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "09e0afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:07,  1.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual decomposition\n",
    "out = []\n",
    "batch_size = 10\n",
    "for chunk in tqdm.tqdm(chunks(inputs, batch_size)):\n",
    "    prompts = [x for x in chunk]\n",
    "#     print(prompts)\n",
    "    out.extend(gpt3(prompts))\n",
    "pp = np.array([1 if p.strip().lower() == l else 0 for p, l in zip(out, labels)])\n",
    "pp.sum()/len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24ff32",
   "metadata": {},
   "source": [
    "#### Tasks in Self-prompt (Ofir's paper) \n",
    "Musique and 2wikimultihop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6596b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-a426bd25b30faa3b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /mmfs1/home/bparan/.cache/huggingface/datasets/json/default-a426bd25b30faa3b/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 8464.79it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 59.82it/s]\n",
      "                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /mmfs1/home/bparan/.cache/huggingface/datasets/json/default-a426bd25b30faa3b/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4834"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Musique\n",
    "data_files = {split:os.path.join(cache_dir, 'musique', 'data', 'musique_full_v1.0_%s.jsonl'%split) for split in ['train', 'dev']}\n",
    "d = datasets.load_dataset('json', data_files=data_files)\n",
    "len(d['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c930f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What year did the writer of Crazy Little Thing Called Love die?\n",
      "1991\n",
      "----\n",
      "Q: What is the country where Nissedal is located named after?\n",
      "north\n",
      "----\n",
      "Q: What is the highest point in the country where Bugabula is found?\n",
      "1,400 metres\n",
      "----\n",
      "Q: Who from the state with the Routzahn-Miller Farmstead signed the declaration of independence?\n",
      "Charles Carroll\n",
      "----\n",
      "Q: Who founded the publisher of Journal of Media Economics?\n",
      "George Routledge\n",
      "----\n",
      "Q: The athlete that became the highest-paid went to manchester United when?\n",
      "2003\n",
      "----\n",
      "Q: What group of languages includes the old version of the language that the Quran was first translated in?\n",
      "Iranian languages\n",
      "----\n",
      "Q: What percentage was the country Tereke-yurén-tepui is located in?\n",
      "5.1\n",
      "----\n",
      "Q: What range is Garfield Peak in the state where Aims community College is located a part of?\n",
      "Sawatch Range\n",
      "----\n",
      "Q: What culture's arrival in the country where the person who paid for Chopin's funeral invited him is know as the \"Davidian Revolution?\n",
      "Norman\n",
      "----\n",
      "Q: When did the sports team that employed Glyn Pardoe get promoted to the Premier League?\n",
      "1992\n",
      "----\n",
      "Q: Who helped resolve the dispute between Virginia and the US state donating Washington, D.C.?\n",
      "William R. Day\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Counter([ex['answerable'] for ex in d['dev']])\n",
    "idx = np.random.randint(0, len(d['train']), 5)\n",
    "for num, ex in enumerate(d['train']):\n",
    "    print(\"Q:\", ex['question'])\n",
    "    print(ex['answer'])\n",
    "    print('----')\n",
    "    if num > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5aee0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the college of the producer of Live: Take No Prisoners?\n",
      "Follow up question: Live: Take No Prisoners >> producer\n",
      "Intermediate answer: Lou Reed\n",
      "Follow up question: What college did Lou Reed go to?\n",
      "Intermediate answer: Syracuse University\n",
      "So the final answer is Syracuse University\n",
      "----\n",
      "Question: When did cable cars start in the city where the signing of the ANZUS treaty was held in 1951?\n",
      "Follow up question: Where was the signing of the ANZUS Treaty held in 1951?\n",
      "Intermediate answer: San Francisco\n",
      "Follow up question: when did cable cars start in San Francisco\n",
      "Intermediate answer: 1878\n",
      "So the final answer is 1878\n",
      "----\n",
      "Question: When did relations take a downturn between Israel and the country north of Cyprus?\n",
      "Follow up question: Which country is north of Cyprus?\n",
      "Intermediate answer: Turkey\n",
      "Follow up question: When did Israel and Turkey 's relations take a downturn?\n",
      "Intermediate answer: after the 2008–09 Gaza War\n",
      "So the final answer is after the 2008–09 Gaza War\n",
      "----\n",
      "Question: Who designed the place where Portrait of Francesco delle Opere can be found?\n",
      "Follow up question: What is the name of the place where Portrait of Francesco delle Opere can be found?\n",
      "Intermediate answer: Uffizi\n",
      "Follow up question: Who was Uffizi designed by?\n",
      "Intermediate answer: Giorgio Vasari\n",
      "So the final answer is Giorgio Vasari\n",
      "----\n",
      "Question: What county shares a border with another county, which in turn is next to a third county, where the ghost town of Ancho is found?\n",
      "Follow up question: Ancho >> located in the administrative territorial entity\n",
      "Intermediate answer: Lincoln County\n",
      "Follow up question: Lincoln County >> shares border with\n",
      "Intermediate answer: Nye County\n",
      "Follow up question: Nye County >> shares border with\n",
      "Intermediate answer: Lander County\n",
      "So the final answer is Lander County\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Counter([ex['answerable'] for ex in d['dev']])\n",
    "idx = np.random.randint(0, len(d['train']), 5)\n",
    "for num, ex in enumerate(d['train']):\n",
    "    if num not in idx:\n",
    "        continue\n",
    "    print(\"Question:\", ex['question'])\n",
    "    prev_answer = None\n",
    "    for subnum, decomp in enumerate(ex['question_decomposition']):\n",
    "        if prev_answer:\n",
    "            print(\"Follow up question:\", decomp['question'].replace(\"#%d\"%subnum, prev_answer))\n",
    "        else:\n",
    "            print(\"Follow up question:\", decomp['question'])\n",
    "        print(\"Intermediate answer:\", decomp['answer'])\n",
    "        prev_answer = decomp['answer']\n",
    "    print(\"So the final answer is %s\"% ex['answer'])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-8da56daef1cdd353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /mmfs1/home/bparan/.cache/huggingface/datasets/json/default-8da56daef1cdd353/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb586641c6c4949af9d6619d12bf0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79fbd6a10f3462b9b35020249ff1b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26401bb13f7147088b9c1af15e485e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2wikimultihop\n",
    "data_files = {split:os.path.join(cache_dir, '2wikimultihop', 'data', '%s.json'%split) for split in ['train', 'dev']}\n",
    "d = datasets.load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609e970",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Known Unknown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "481840a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/known_unknowns/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 991.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "d = datasets.load_dataset('bigbench', 'known_unknowns', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "728729a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who will be elected president of the United States 100 years from now?\n",
      "  choice: Unknown\n",
      "  choice: Robocop\n",
      "A:\n",
      "Unknown\n",
      "----\n",
      "Q: How many hairs were on Neil Armstrong's head when he landed on the moon?\n",
      "  choice: Unknown\n",
      "  choice: Five million\n",
      "A:\n",
      "Unknown\n",
      "----\n",
      "Q: Where was Mark Twain born?\n",
      "  choice: Unknown\n",
      "  choice: Florida, Missouri\n",
      "A:\n",
      "Florida, Missouri\n",
      "----\n",
      "Q: What type of hat did Charlie Chaplin wear?\n",
      "  choice: Unknown\n",
      "  choice: Bowler hat\n",
      "A:\n",
      "Bowler hat\n",
      "----\n",
      "Q: Who was the most recent person in the world to break their toe?\n",
      "  choice: Unknown\n",
      "  choice: Ariana Grande\n",
      "A:\n",
      "Unknown\n",
      "----\n",
      "Q: How many calories worth of food does a 20-pound cat eat per day?\n",
      "  choice: Unknown\n",
      "  choice: 310 calories\n",
      "A:\n",
      "310 calories\n",
      "----\n",
      "Q: How many people in Cheyenne, Wyoming were eating during the last solar eclipse?\n",
      "  choice: Unknown\n",
      "  choice: 156\n",
      "A:\n",
      "Unknown\n",
      "----\n",
      "Q: How many legs do ants have?\n",
      "  choice: Unknown\n",
      "  choice: 6\n",
      "A:\n",
      "6\n",
      "----\n",
      "Q: On what date in the past hundred years did humans collectively sleep the most?\n",
      "  choice: Unknown\n",
      "  choice: July 31, 2009\n",
      "A:\n",
      "Unknown\n",
      "----\n",
      "Q: How many hours of sleep a night is considered healthy for an adult?\n",
      "  choice: Unknown\n",
      "  choice: Between 7 and 9 hours\n",
      "A:\n",
      "Between 7 and 9 hours\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e7309",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Strategy QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5222b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/strategyqa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197f8f7e7f14d84b0681e149333b9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1832"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "d = datasets.load_dataset('bigbench', 'strategyqa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0affa16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Has Johns Hopkins University always treated subjects ethically?\n",
      "A:\n",
      "No. Henrietta Lacks' DNA was used and replicated by Johns Hopkins University without her family's knowledge or approval. Henrietta Lacks' family medical history was released by Johns Hopkins University without their knowledge.\n",
      "----\n",
      "Q: Could Carl Friedrich Gauss speak to someone 100 miles away?\n",
      "A:\n",
      "No. Carl Friedrich Gauss was born in 1777. Speaking to someone 100 miles away requires a telephone. The telephone was invented in 1876.\n",
      "----\n",
      "Q: Is calling ABBA the Swedish Beatles a preposterous claim?\n",
      "A:\n",
      "Yes. ABBA was a Swedish band that had 1 Billboard number 1 hit and 4 top 10 hits. The Beatles had 20 Billboard number 1 hits and 34 top 10 hits.\n",
      "----\n",
      "Q: Did compact discs make computer gaming more popular?\n",
      "A:\n",
      "Yes. Compact discs contained significantly more storage space than the previously popular floppy disc format. Gaming studios were therefore able to significantly improve the graphics, sounds, and features of their games to make them more immersive. The better games led to a massive increase in popularity for computer gaming.\n",
      "----\n",
      "Q: Did Alice's Adventures in Wonderland inspire Macbeth?\n",
      "A:\n",
      "No. Alice's Adventures in Wonderland was published in 1865. Macbeth was first performed in 1606.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ad6db",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Hindu Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfaa3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/hindu_knowledge/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 904.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'hindu_knowledge', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "269545c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: In Hindu mythology, which of these weapons is prominently associated with Balarama?\n",
      "  choice: Chakra\n",
      "  choice: Sword\n",
      "  choice: Bow and arrow\n",
      "  choice: Plough\n",
      "A:\n",
      "----\n",
      "Q: Which term, which has its origin in Hindu mythology, has become a modern idiom for a strict convention\n",
      "  choice: Lok Sabha\n",
      "  choice: Sudarshana Chakra\n",
      "  choice: Lakshmana Rekha\n",
      "  choice: Rajya Sabha\n",
      "A:\n",
      "----\n",
      "Q: Which Hindu sage is known as the author of the Mahabharata?\n",
      "  choice: Vashistha\n",
      "  choice: Vyasa\n",
      "  choice: Valmiki\n",
      "  choice: Vishvamitra\n",
      "A:\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 3)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "#     print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95918f",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Movie Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ce0b40e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/movie_dialog_same_or_different/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f4a4186c741efa74790968a2c5bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'movie_dialog_same_or_different', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbd4f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation between two people, but the transcript doesn't mark who said what: I doubt if a few bombs would wake up Dawn on a Sunday morning. This phoney war get's on my nerves. If we're going to have a war, I wish they'd get it started. Just ignore her, Mac. ----- In the preceding conversation, were the sentences \"If we're going to have a war, I wish they'd get it started.\" and \"Just ignore her, Mac.\" said by the same or different individuals?\n",
      "Answer:\n",
      "different\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 1)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96185da6",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - Code Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b086f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/code_line_description/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 363.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'code_line_description', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b790f42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python code:\n",
      "try:\n",
      "    n = int(input())\n",
      "    m = int(input())\n",
      "    integer_sum = int(n) + int(m)\n",
      "    print(integer_sum)\n",
      "except:\n",
      "    print('error')\n",
      "\n",
      "  choice: prints number between 5 and 6\n",
      "  choice: try input and except error\n",
      "  choice: inputs the string 'try'\n",
      "  choice: prints sum of two input numbers only if they are integers otherwise raises error\n",
      "\n",
      "English language description:\n",
      "prints sum of two input numbers only if they are integers otherwise raises error\n",
      "----\n",
      "Python code:\n",
      "numbers_list = [2, 6, 8, 10, 11, 4, 12, 7, 13, 17, 0, 3, 21]\n",
      "filtered_list = list(filter(lambda num: (num > 7), numbers_list))\n",
      "print(filtered_list)\n",
      "\n",
      "  choice: prints lambda\n",
      "  choice: returns a filtered list\n",
      "  choice: prints a list of numbers greater than 7 from numbers_list\n",
      "  choice: prints numbers from 2 to 6\n",
      "\n",
      "English language description:\n",
      "prints a list of numbers greater than 7 from numbers_list\n",
      "----\n",
      "Python code:\n",
      "sum_of_numbers = 0\n",
      "num = 10\n",
      "while(num > 0):\n",
      "    sum_of_numbers += num\n",
      "    num -= 1\n",
      "print('The sum is', sum_of_numbers)\n",
      "\n",
      "  choice: prints the factorial of numbers\n",
      "  choice: prints the sum of first 10 natural numbers\n",
      "  choice: prints number from 10 to 0 in ascending order\n",
      "  choice: prints the numbers from 1 to 10\n",
      "\n",
      "English language description:\n",
      "prints the sum of first 10 natural numbers\n",
      "----\n",
      "Python code:\n",
      "for i in range(23):\n",
      "    print(i)\n",
      "\n",
      "  choice: prints 'hello world' to the terminal\n",
      "  choice: prints values from 1 to 10\n",
      "  choice: computes first 10 prime numbers\n",
      "  choice: prints values from 0 to 22\n",
      "\n",
      "English language description:\n",
      "prints values from 0 to 22\n",
      "----\n",
      "Python code:\n",
      "def sum_two(a,b):\n",
      "    return a+b\n",
      "\n",
      "  choice: returns a and b\n",
      "  choice: returns the sum of two numbers\n",
      "  choice: returns 5 and 6\n",
      "  choice: returns numbers between 5 and 6\n",
      "\n",
      "English language description:\n",
      "returns the sum of two numbers\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1700ed",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - conceptual_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0dd73e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/conceptual_combinations/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68adab24b12047a281ebed3c79bbc8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'conceptual_combinations', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab03a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: turbulent peace. Question: Which of the following sentences best characterizes turbulent peaces?\n",
      " option: Turbulent peace is full of harmony.\n",
      " option: Turbulent peace is windy.\n",
      " option: Turbulent peace is dangerous for planes.\n",
      " option: Turbulent peace is short-lived.\n",
      "Answer:\n",
      "Turbulent peace is short-lived.\n",
      "----\n",
      "Concept: Old news. Question: Which of the following sentences best characterizes old news?\n",
      " option: Old news is a liberal media conspiracy.\n",
      " option: Old news is reporting about old people.\n",
      " option: New discoveries about the past are old news.\n",
      " option: Old news is established knowledge.\n",
      "Answer:\n",
      "Old news is established knowledge.\n",
      "----\n",
      "Concept: deadly caregiver. Question: Which of the following sentences best characterizes deadly caregivers?\n",
      " option: Healthcare systems struggle due to the lack of deadly caregivers.\n",
      " option: Morphine overdoses are deadly caregivers.\n",
      " option: Deadly caregivers take care of terminal patients.\n",
      " option: Homicidal nurses are deadly caregivers.\n",
      "Answer:\n",
      "Homicidal nurses are deadly caregivers.\n",
      "----\n",
      "Concept: Sinless sinner. Question: Which of the following sentences best characterizes sinless sinners?\n",
      " option: Sinless sinners are letters of the alphabet.\n",
      " option: Sinless sinners have no clue.\n",
      " option: No one can be a sinless sinner.\n",
      " option: Sinless sinners can be venial or capital.\n",
      "Answer:\n",
      "No one can be a sinless sinner.\n",
      "----\n",
      "Concept: Pretty ugly. Question: Which of the following sentences best characterizes pretty ugly people?\n",
      " option: Pretty ugly people are homely.\n",
      " option: Pretty ugly people are pretty.\n",
      " option: Pretty ugly people are dangerous.\n",
      " option: Pretty ugly people are morally corrupt.\n",
      "Answer:\n",
      "Pretty ugly people are homely.\n",
      "----\n",
      "Concept: Russian novels. Question: Which of the following sentences best characterizes Russian novels?\n",
      " option: Russian novels are communist.\n",
      " option: Russian novels are written in Russian.\n",
      " option: Russian novels are cold in the winter.\n",
      " option: Russian novels are unfinished.\n",
      "Answer:\n",
      "Russian novels are written in Russian.\n",
      "----\n",
      "Concept: standing desks. Question: Which of the following sentences best characterizes standing desks?\n",
      " option: Standing desks are bipedal.\n",
      " option: Standing desks are tall enough that small children can walk underneath them.\n",
      " option: Standing desks are best used with standing chairs.\n",
      " option: Standing desks are for unusually tall people.\n",
      "Answer:\n",
      "Standing desks are tall enough that small children can walk underneath them.\n",
      "----\n",
      "Concept: angry bosses. Question: Which of the following sentences best characterizes angry bosses?\n",
      " option: Angry bosses is a fun game.\n",
      " option: Angry bosses sting employees.\n",
      " option: Angry bosses are itchy.\n",
      " option: Angry bosses reprimand employees.\n",
      "Answer:\n",
      "Angry bosses reprimand employees.\n",
      "----\n",
      "Concept: nuclear plants. Question: Which of the following sentences best characterizes nuclear plants?\n",
      " option: Nuclear plants have a man, woman and child.\n",
      " option: Nuclear plants use uranium to make electricity.\n",
      " option: Nuclear plants are in every biological cell.\n",
      " option: Nuclear plants need a lot of fertilizer.\n",
      "Answer:\n",
      "Nuclear plants use uranium to make electricity.\n",
      "----\n",
      "Concept: Wicked witches. Question: Which of the following sentences best characterizes wicked witches?\n",
      " option: Wicked witches eat all the sugar.\n",
      " option: Wicked witches wear white gowns.\n",
      " option: Wicked witches are skateboarders with magical powers.\n",
      " option: Wicked witches poison princesses.\n",
      "Answer:\n",
      "Wicked witches poison princesses.\n",
      "----\n",
      "Concept: flat world. Question: Which of the following sentences best characterizes flat world?\n",
      " option: Flat world is a great tourist destination.\n",
      " option: Only flatworms live in flat world.\n",
      " option: Conspiracy theorists live in flat world.\n",
      " option: In flat world, everything is 2-dimensional.\n",
      "Answer:\n",
      "In flat world, everything is 2-dimensional.\n",
      "----\n",
      "Concept: Upside-down world. Question: Which of the following sentences best characterizes upside-down world?\n",
      " option: In upside-down world, dropped objects fall down.\n",
      " option: Upside-down world is made with pineapple.\n",
      " option: Diana Ross lives in upside-down world.\n",
      " option: In upside-down world, the floor is up and the ceiling is down.\n",
      "Answer:\n",
      "In upside-down world, the floor is up and the ceiling is down.\n",
      "----\n",
      "The word 'ropgo' is an adjective for things that are made of ribbon. Question: Which of the following sentences best characterizes ropgo bows?\n",
      " option: Ropgo bows have branches.\n",
      " option: Ropgo bows show respect.\n",
      " option: Ropgo bows decorate gifts.\n",
      " option: Ropgo bows shoot arrows.\n",
      "Answer:\n",
      "Ropgo bows decorate gifts.\n",
      "----\n",
      "The word 'fogop' means frozen, and the word 'sluip' means food often eaten without cutlery. Question: Which of the following sentences best characterizes fogop sluips?\n",
      " option: Hamburgers are fogop sluips.\n",
      " option: You serve fogop sluips at formal meals.\n",
      " option: Fogop sluips are popular in winter.\n",
      " option: Ice cream is a fogop sluip.\n",
      "Answer:\n",
      "Ice cream is a fogop sluip.\n",
      "----\n",
      "The word 'frumious' is an adjective for things that are very threatening. Question: Which of the following sentences best characterizes frumious pets?\n",
      " option: Frumious pets are great companions for little children.\n",
      " option: Most people prefer frumious pets over puppies.\n",
      " option: Frumious pets are easy to recycle.\n",
      " option: If you want a frumious pet, a rattlesnake is a good choice.\n",
      "Answer:\n",
      "If you want a frumious pet, a rattlesnake is a good choice.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886ba87",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - language_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2eb4fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/language_identification/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93eecefda8b4a9694f26daa73a39c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'language_identification', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f431c8",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - vitaminc_fact_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "63360ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/vitaminc_fact_verification/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d63ff633bd14bae8ef4477919fe2c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'vitaminc_fact_verification', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4951d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Doug Petrie: Petrie made a cameo on Joss Whedon 's web-based film , Dr. Beautiful 's Sing-Along Blog , as `` Professor Normal '' .\n",
      "Claim: Doug Petrie made a cameo in Dr. Beautiful 's Sing-Along Blog .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: The Church of Jesus Christ of Latter-day Saints: Faithful members adhere to church laws of sexual purity , health , fasting , and Sabbath observance , and contribute ten percent of their income to the church in tithing .\n",
      "Claim: Members of the Church of Jesus Christ of Latter-day Saints pay a tithe of their income to the church .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Chinatown -LRB-1974 film-RRB-: Chinatown is a 1974 American neo-noir mystery film , directed by Roman Polanski from a screenplay by Robert Frost , starring Jack Nicholson and Faye Dunaway .\n",
      "Claim: The film Chinatown 's screenplay was done by Robert Towne .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: X-23: With Logan 's help , she eventually made her way to a remote location on the border between the United States ( via North Dakota ) and Canada escaping across the border , although Logan is killed in battle with X-24 when he deliberately overdoses on a drug designed to stimulate his weakening healing factor so that he can fight one last time . *James Mangold has stated that he would like to see X-23 appear in future X-Men films , and that he would like to be involved in one way or another , should that happen .\n",
      "Claim: X-24 ended Logan 's life , when he overdoses on a drug designed to stimulate his weakening healing factor so that he can fight one last time .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: The Night Before -LRB-2015 film-RRB-: The Night Before has received mixed reviews from critics .\n",
      "Claim: The movie received positive reviews .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Dark matter: Dark matter is a type of sport in which each player uses a racket and two balls .\n",
      "Claim: Dark matter is a type of matter .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Rob McElhenney: McElhenney is best known for playing Dennis Reynolds on the FX/FXX comedy series `` It 's Always Sunny in Philadelphia . ''\n",
      "Claim: Rob McElhenney played the role of Mac in a TV series .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Fargo -LRB-season 3-RRB-: The third season of Fargo , an American anthology black comedy -- crime drama television series created by Noah Hawley , premiered on April 19 , 2017 , on the basic cable network FX .\n",
      "Claim: The third season of the TV series `` Fargo '' premiered on FX .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: God's Not Dead-COLON- A Light in Darkness: On review aggregator website Rotten Tomatoes , the film has an approval rating of 13 % based on 7 reviews , and an average rating of 0.8/10 .\n",
      "Claim: Averaged over 6 critics , A Light in Darkness holds an approval rating of more than 15 % .\n",
      "True, False, or Neither?\n",
      "Neither\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Hobo with a Shotgun: Reviews of the film are generally positive , and it currently holds a 66 % `` certified fresh '' approval rating out of 111 reviews on the review aggregate website Rotten Tomatoes with an average rating of 3.8 out of 10 with the consensus being , `` It certainly is n't subtle – or even terribly smart – but as a gleefully gory homage to low-budget exploitation thrillers , Hobo with a Shotgun packs plenty of firepower .\n",
      "Claim: Hobo with a Shotgun scored 66 % on critics , with an average rating of more than 4/10 .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: S.L. Benfica: In September 2018 , Benfica SAD reported a profit of €20.6 million ( a decrease of 53.7 % ) and a revenue of €206.2 million ( a decrease of 18.7 % ) .\n",
      "Claim: In 2018 , Benfica SAD earned over €200 million .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Aeneas: Aeneas is a character in Greek mythology and is mentioned only in Herodotus ' Histories VIII .\n",
      "Claim: Aeneas appeared in Histories VIII by Herodotus .\n",
      "True, False, or Neither?\n",
      "True\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Ed Gagliardi: Gagliardi died of cancer on May 11 , 2014 , after battling cancer for eight years .\n",
      "Claim: Ed Gagliardi died on October 13 , 2005 .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: Azithromycin: It is available as a generic medication and is sold only under the trade name azithromycin worldwide .\n",
      "Claim: Azithromycin is available as a generic medication , sold under many names .\n",
      "True, False, or Neither?\n",
      "False\n",
      "----\n",
      "Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\n",
      "\n",
      "\n",
      "Passage: 2020 coronavirus pandemic in Bosnia and Herzegovina: The 2019–20 coronavirus pandemic was confirmed to have spread to Bosnia and Herzegovina when its first case was confirmed on 5 March 2020.As of 13 March 2020 , in Bosnia and Herzegovina there are currently 18 cases , 15 in Republika Srpska , and 3 in the Federation of Bosnia and Herzegovina.\n",
      "Claim: Bosnia and Herzegovina had more than 20 coronavirus cases by March 14 , 2020 , including more than 16 in Republika Srpska .\n",
      "True, False, or Neither?\n",
      "Neither\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a1252",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - formal_fallacies_syllogisms_negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f31c3106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/formal_fallacies_syllogisms_negation/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bab9d0201a492d9504a335e5f39adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'formal_fallacies_syllogisms_negation', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e55e3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \"Consumer research aims at understanding whether users of some products also tend to consume other ones, or not. The following argument seeks to clarify some such relations: First of all, being a loyal buyer of Clairol shampoo is necessary for being a loyal buyer of Bed Head shampoo. Next, being a loyal buyer of Bed Head shampoo is sufficient for being an occasional purchaser of Infusium shampoo. Plus,whoever is loyal buyer of Bed Head shampoo is at least one of these: an occasional purchaser of Infusium shampoo, a loyal buyer of Cetaphil soap or a loyal buyer of Clairol shampoo. All this entails that everyone who is a loyal buyer of Bed Head shampoo is a loyal buyer of Cetaphil soap, too.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"Is Fred a fan of Liverpool? Are supporters of Real Madrid devotees of PSG? In European football, it is sometimes difficult to keep track of the mutual admiration and dislike. The following argument seeks to clarify some such relations: Whoever is an ex-supporter of FC Barcelona and a fan of AEK Larnaca FC is not an opponent to FK Partizan. We may conclude: No opponent to FK Partizan is an ex-supporter of FC Barcelona or a fan of AEK Larnaca FC.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"It is not always easy to see which chemicals are contained in our consumer products. The following argument pertains to this question: First premise: Benzophenone-4 is an ingredient of Enchanted Evening. Second premise: Every ingredient of Shower Gel is not an ingredient of Honeymoon In Style or not an ingredient of Enchanted Evening. It follows that it is not the case that Benzophenone-4 is an ingredient of Shower Gel.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To start with, being a stepbrother of Wayne is sufficient for being a classmate of Darrin. Now, Andres is a stepbrother of Wayne. We may conclude: Andres is a classmate of Darrin.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to see which chemicals are contained in our consumer products. The following argument pertains to this question: First of all, Progesterone is an ingredient of Pale Pashmina. Next, every ingredient of Enhance Eye Pencil that is an ingredient of Pale Pashmina is also an ingredient of Royal Lip Crayon. Plus,Progesterone is an ingredient of Enhance Eye Pencil. From this follows: Progesterone is an ingredient of Royal Lip Crayon.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: First, being a son of Francis is necessary for being a great-grandfather of Jason. Second, Darrell is a son of Francis. We may conclude: Darrell is a great-grandfather of Jason.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"Here comes a perfectly valid argument: First premise: Jackie is not a frequent consumer of Infusium shampoo or not a regular user of Burt's Bees soap. Second premise: Every regular user of Mrs. Meyer's soap is a regular user of Burt's Bees soap and a frequent consumer of Infusium shampoo. From this follows: Jackie is not a regular user of Mrs. Meyer's soap.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: Being a loyal buyer of Matrix shampoo is necessary for not being a rare consumer of Bentley Organic soap. It is false that Gillian is a rare consumer of Bentley Organic soap. So, necessarily, Gillian is a loyal buyer of Matrix shampoo.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: First of all, being a close friend of Ron is necessary for being a close friend of Abraham. Next, there is no close friend of Abraham who is a close friend of Ron yet not a stepbrother of Andy. From this follows: Every close friend of Abraham is a stepbrother of Andy.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: To begin with, no regular consumer of Kirk's soap who is an occasional purchaser of South of France soap is a frequent consumer of Garnier shampoo. Moreover, some occasional purchaser of Clairol shampoo is a frequent consumer of Garnier shampoo. Hence, some occasional purchaser of Clairol shampoo is not both an occasional purchaser of South of France soap and a regular consumer of Kirk's soap.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"Is Fred a fan of Liverpool? Are supporters of Real Madrid devotees of PSG? In European football, it is sometimes difficult to keep track of the mutual admiration and dislike. The following argument seeks to clarify some such relations: First of all, everyone who is neither an admirer of FC Viktoria Plzeň nor a supporter of FK Jablonec is a devotee of FC Dnipro or a follower of FC Internazionale Milano. Next, every devotee of FC Dnipro is a member of FK Kukësi. Plus,every follower of FC Internazionale Milano is a member of FK Kukësi. It follows that someone who is not a member of FK Kukësi is an admirer of FC Viktoria Plzeň or a supporter of FK Jablonec.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"Some football fans admire various clubs, others love only a single team. But who is a fan of whom precisely? The following argument pertains to this question: Whoever is an opponent to FC Internazionale Milano is not a member of AS Monaco FC. Every member of AS Monaco FC who is an opponent to CFR 1907 Cluj is also an ex-fan of AZ Alkmaar. Being an opponent to FC Internazionale Milano is sufficient for not being an opponent to CFR 1907 Cluj. In consequence, being an ex-fan of AZ Alkmaar is necessary for being an opponent to FC Internazionale Milano.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"Here comes a perfectly valid argument: First premise: It is false that Regina is a rare consumer of Yardley London soap. Second premise: Whoever is a rare consumer of Yardley London soap is not a loyal buyer of Aussie shampoo. We may conclude: Regina is not a loyal buyer of Aussie shampoo.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n",
      "Q: \"Some football fans admire various clubs, others love only a single team. But who is a fan of whom precisely? The following argument pertains to this question: First premise: Whoever is a friend of FC Porto and a backer of Tottenham Hotspur is not a follower of Eintracht Frankfurt. Second premise: Nobody is neither a critic of Vitesse nor a follower of Eintracht Frankfurt. All this entails that every backer of Tottenham Hotspur who is a friend of FC Porto is also a critic of Vitesse.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: First premise: Francis is a loyal buyer of Cetaphil soap. Second premise: Whoever is an occasional purchaser of Softsoap soap or a loyal buyer of Cetaphil soap is a frequent consumer of Bath & Body Works soap. In consequence, Francis is a frequent consumer of Bath & Body Works soap.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c759da",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - misconceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9fd1a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/misconceptions/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaeae92244f4aba957a3d9f5fafd10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'misconceptions', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ad1fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: Samuel Butler said, \"He that complies against his will is of his own opinion still.\"\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: When there was a proposal to cancel the World War I debts of the Allied nations to the United States, President Coolidge was said to have snorted, \"They hired the money, didn't they?\" But his biographer was unable to track down this quote and declared it inauthentic.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: Mice's favorite food is cheese.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: A finite number of audio files can be saved on a standard compact disc.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: About 1 in 25 people have perfect pitch.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: Searing meat seals in moisture.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: There are seasons because the Earth is closer to the Sun in the summer than in the winter.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: The heat of entry of a meteor is primarily caused by friction.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: When there was a proposal to cancel the World War I debts of the Allied nations to the United States, President Coolidge said, \"They hired the money, didn't they?\"\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: Jesus, assuming he existed, was born in the spring or summer.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: Rice causes birds to die by inflating their stomachs until they burst.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: You can have a tattoo and be buried in a Jewish cemetery.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: As of 2021, you have to wait 24 hours before filing a missing-person report in the United States.\n",
      "A:\n",
      "F\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: People don't swallow a high number of spiders during sleep throughout life.\n",
      "A:\n",
      "T\n",
      "----\n",
      "Is the following statement true or false? Output T for true and F for false.\n",
      "Q: As of 2021, Mormons still commonly practice polygamy.\n",
      "A:\n",
      "F\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f0d1ee26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n\\nPassage: Garfield-COLON- A Tail of Two Kitties: The film received negative reviews from critics and it earned $ 141.7 million on a $ 60 million budget.\\nClaim: Garfield : A Tail of Two Kitties received bleak reviews .\\nTrue, False, or Neither?'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cd7b2",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - logical_deduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dcf8e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/logical_deduction/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603df278f6ef43abb448b28819d6f4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'logical_deduction', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ddc72020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells five fruits: oranges, cantaloupes, peaches, loquats, and kiwis. The peaches are more expensive than the oranges. The cantaloupes are more expensive than the peaches. The kiwis are the third-most expensive. The loquats are the second-cheapest.\n",
      "The peaches are the second-most expensive.\n",
      "----\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a branch, there are five birds: a crow, a falcon, a hawk, a robin, and a quail. The robin is to the right of the falcon. The quail is to the left of the hawk. The robin is the second from the left. The hawk is the second from the right.\n",
      "The hawk is the second from the right.\n",
      "----\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells five fruits: mangoes, cantaloupes, plums, oranges, and watermelons. The oranges are more expensive than the watermelons. The watermelons are the second-cheapest. The plums are less expensive than the cantaloupes. The plums are the second-most expensive.\n",
      "The watermelons are the second-cheapest.\n",
      "----\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In an antique car show, there are five vehicles: a tractor, a truck, a bus, a minivan, and a convertible. The bus is newer than the tractor. The convertible is older than the tractor. The truck is the second-newest. The minivan is older than the convertible.\n",
      "The truck is the second-newest.\n",
      "----\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In an antique car show, there are five vehicles: a hatchback, a convertible, a tractor, a truck, and a limousine. The tractor is newer than the hatchback. The limousine is the second-newest. The truck is older than the hatchback. The convertible is the second-oldest.\n",
      "The hatchback is the third-newest.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In a golf tournament, there were seven golfers: Joe, Dan, Ana, Mel, Ada, Eve, and Amy. Eve finished above Amy. Dan finished second. Ada finished below Ana. Ana finished third-to-last. Mel finished last. Amy finished third.\n",
      "Mel finished last.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells seven fruits: watermelons, oranges, mangoes, cantaloupes, kiwis, pears, and peaches. The pears are the second-cheapest. The peaches are more expensive than the cantaloupes. The peaches are less expensive than the mangoes. The cantaloupes are more expensive than the kiwis. The oranges are the fourth-most expensive. The watermelons are the second-most expensive.\n",
      "The cantaloupes are the third-cheapest.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells seven fruits: oranges, plums, loquats, apples, kiwis, cantaloupes, and peaches. The loquats are less expensive than the kiwis. The plums are the cheapest. The loquats are the fourth-most expensive. The peaches are less expensive than the oranges. The peaches are more expensive than the kiwis. The apples are the second-cheapest.\n",
      "The loquats are the fourth-most expensive.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In an antique car show, there are seven vehicles: a station wagon, a truck, a sedan, a limousine, a convertible, a bus, and a hatchback. The limousine is the newest. The bus is the third-newest. The bus is older than the convertible. The sedan is the third-oldest. The station wagon is older than the truck. The hatchback is the oldest.\n",
      "The station wagon is the second-oldest.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a shelf, there are seven books: an orange book, a green book, a gray book, a purple book, a black book, a yellow book, and a white book. The orange book is to the right of the gray book. The yellow book is the third from the right. The purple book is the second from the left. The yellow book is to the right of the orange book. The white book is the second from the right. The black book is the leftmost.\n",
      "The orange book is the fourth from the left.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In a golf tournament, there were seven golfers: Ana, Rob, Eve, Mel, Mya, Joe, and Eli. Joe finished second-to-last. Eli finished first. Mya finished below Joe. Mel finished second. Rob finished third-to-last. Ana finished third.\n",
      "Ana finished third.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells seven fruits: kiwis, plums, mangoes, watermelons, pears, peaches, and oranges. The pears are the third-cheapest. The kiwis are the second-most expensive. The pears are more expensive than the plums. The oranges are less expensive than the kiwis. The mangoes are the third-most expensive. The peaches are the second-cheapest.\n",
      "The kiwis are the second-most expensive.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a branch, there are seven birds: a hawk, a raven, a falcon, a crow, a hummingbird, a robin, and a cardinal. The hawk is the second from the right. The robin is to the left of the raven. The raven is to the left of the crow. The falcon is to the left of the cardinal. The hummingbird is the second from the left. The falcon is the third from the right.\n",
      "The robin is the leftmost.\n",
      "----\n",
      "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In a golf tournament, there were three golfers: Eve, Joe, and Amy. Eve finished below Amy. Eve finished above Joe.\n",
      "Joe finished last.\n",
      "----\n",
      "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a branch, there are three birds: an owl, a crow, and a cardinal. The owl is to the right of the cardinal. The cardinal is to the right of the crow.\n",
      "The crow is the leftmost.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa861e77",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - winowhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f006e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/winowhy/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760e118fd3b644a2b4a6697049a49806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2862"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'winowhy', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c79c376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "The firemen arrived before the police because they were coming from so far away. The 'they' refers to the police because the firemen were not able to use their ladder to reach the roof of the building.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Mary tucked her daughter Anne into bed, so that she could work. The 'she' refers to mary because mary her daughter so she is sleep.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "The trophy doesn't fit into the brown suitcase because it is too small. The 'it' refers to the suitcase because The trophy is not fit into the suitcase.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "The large ball crashed right through the table because it was made of steel. The 'it' refers to the large ball because steel would smash wood.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Jane knocked on Susan's door but she did not answer. The 'she' refers to susan because It was Susan's door. Susan didn't answer to Jane who knocked on it.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Mark told Pete many lies about himself, which Pete included in his book. He should have been more  skeptical. The 'He' refers to pete because Pete included lies in his book without being skeptical.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "As Andrea in the crop duster passed over Susan, she could see the landing strip. The 'she' refers to andrea because the character was created by writer David Goyer.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "If the con artist has succeeded in fooling Sam, he would have gotten a lot of money. The 'he' refers to the con artist because he has convinced Sam of his wealth and then is trying to make Sam believe he is Sam's heir.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "The actress used to be named Terpsichore, but she changed it to Tina a  few years ago, because she figured it was easier to pronounce. The 'it' refers to tina because More likely to refer to Tina because Tina is the easier of the two to pronounce.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Paul tried to call George on the phone, but he wasn't available. The 'he' refers to george because of his age, but George could be referring to George and David.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Frank felt crushed when his longtime rival Bill revealed that he was the winner of the competition. The 'he' refers to bill because Frank was feeling bad; therefore he must have lost.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "My meeting started at 4:00 and I needed to catch the train at 4:30,  so there wasn't much time. Luckily, it was short, so it worked out. The 'it' refers to the meeting because Yes, because everything flows according to your destiny.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Joe's uncle can still beat him at tennis, even though he is 30 years younger. The 'he' refers to joe because of his age.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Madonna fired her trainer because she couldn't stand her boyfriend. The 'her' refers to the trainer because Madonna fired her trainer she slept with her boyfriend.\n",
      "The above reasoning is\n",
      "incorrect\n",
      "----\n",
      "Please answer the following questions about which words certain pronouns refer to.\n",
      "\n",
      "Bill passed the half-empty plate to John because he was full. The 'he' refers to bill because Because he was the one giving away the food.\n",
      "The above reasoning is\n",
      "correct\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c1bd6",
   "metadata": {},
   "source": [
    "#### Tasks in Flipped learning - novel_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0992f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/novel_concepts/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1052.52it/s]\n"
     ]
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'novel_concepts', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a51e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) lawyers 2) basketball 3) walled yards\n",
      "A:\n",
      "They all involve courts.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) blue whales 2) Jupiter 3) top quarks 4) Mt. Everest\n",
      "A:\n",
      "They all are the largest of their kind.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) bike 2) piano 3) brake\n",
      "A:\n",
      "They are all types of pedals.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) ballet 2) snow 3) running 4) snorkeling\n",
      "A:\n",
      "They all use specialized shoes.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) dentistry 2) construction 3) oil\n",
      "A:\n",
      "They all involve drilling.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) bumble bees 2) cell phones 3) exciting news\n",
      "A:\n",
      "They all can make a buzz.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) local 2) cable 3) cell\n",
      "A:\n",
      "They are all types of networks.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) guitars 2) sewing 3) tennis rackets 4) theoretical particle physics\n",
      "A:\n",
      "They all involve strings.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) pruning trees 2) complex analysis 3) closing stores in a franchise\n",
      "A:\n",
      "They all involve branch cuts.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) normal distribution 2) Pavlov 3) carillon\n",
      "A:\n",
      "They all relate to bells.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 15)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb352fb",
   "metadata": {},
   "source": [
    "#### Tasks in Auto-Cot - MAWPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9672b715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration omarxadel--MaWPS-ar-e0d2ee7eb9fa6fdb\n",
      "WARNING:datasets.builder:Found cached dataset csv (/gscratch/zlab/bparan/projects/cascades/data/omarxadel___csv/omarxadel--MaWPS-ar-e0d2ee7eb9fa6fdb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ebbdaa3b154e98a7133cb0f80f49d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3635"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset('omarxadel/MaWPS-ar', 'train', cache_dir=cache_dir)\n",
    "inputs = [list(d.values())[0] for d in data['train']]\n",
    "labels = []\n",
    "ans_list = []\n",
    "for d in data['train']:\n",
    "    try:\n",
    "        ans = eval(list(d.values())[1].split(\"=\")[-1].strip())\n",
    "        labels.append(list(d.values())[1].split(\"=\")[-1].strip())\n",
    "        if isinstance(ans, int):\n",
    "            ans_list.append(ans)\n",
    "        elif (ans).is_integer():\n",
    "            ans_list.append(int(ans))\n",
    "        else:\n",
    "            ans_list.append(float(\"%.2f\" % ans))\n",
    "        \n",
    "    except:\n",
    "        ans = eval(list(d.values())[1].split(\"=\")[0].strip())\n",
    "        labels.append(list(d.values())[1].split(\"=\")[-1].strip())\n",
    "        if isinstance(ans, int):\n",
    "            ans_list.append(ans)\n",
    "        elif (ans).is_integer():\n",
    "            ans_list.append(int(ans))\n",
    "        else:\n",
    "            ans_list.append(float(\"%.2f\" % ans))\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78ab656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda wants to diverge 608 Books among 28 friends. How many would each friend acquire?\n",
      "608 / 28\n",
      "21.71\n",
      "----\n",
      "Connie had 141 plum. Carrie took 139 from him. Now How many plum Connie have left?\n",
      "141 - 139\n",
      "2\n",
      "----\n",
      "Michael had 280 watermelon. Jasmine gripped some watermelon. Now Michael has 3  watermelon. How many did Jasmine grippeds?\n",
      "280 - 3\n",
      "277\n",
      "----\n",
      "Katherine acquire 9 bags of nectarine . how many nectarine in each bag? If total 89 nectarine Katherine  acquire.\n",
      "89 / 9\n",
      "9.89\n",
      "----\n",
      " A company invited 24 people to a luncheon, but 10 of them didn't show up. If the tables they had held 7 people each, how many tables do they need? \n",
      "((24.0-10.0)/7.0)\n",
      "2\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label, ans) in enumerate(zip(inputs, labels, ans_list)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print(ans)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03939ba",
   "metadata": {},
   "source": [
    "#### Tasks in Auto-CoT (GSM8K) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f139ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset gsm8k (/gscratch/zlab/bparan/projects/cascades/data/gsm8k/main/1.1.0/37bfb08b1d4fcbb01f06b03d9e1ef5f1fcbd4d3af3d08842c50d7305091285ba)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1022.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset('gsm8k', 'main', cache_dir=cache_dir)['test']\n",
    "inputs = [d['question'] for d in data]\n",
    "labels = [d['answer'].split('#### ')[-1] for d in data]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd4b037b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mason is cleaning out all the junk in his attic. 20% of the items are useful, 10% are valuable heirlooms, and 70% are junk. If Marcus's attic has 8 useful items in it, how many junk items does it have?\n",
      "28\n",
      "----\n",
      "A gecko eats 70 crickets every three days.  The first day she eats 30% of the crickets.  The second day she eats 6 less than the first, and the third day she finishes up the remaining crickets.  How many crickets does she eat on the third day?\n",
      "34\n",
      "----\n",
      "My new house has 12 medium ceiling lights but I have seen small and large ceiling lights in other rooms. The small ones require 1 bulb, the medium ones require 2, and the large ones need 3 bulbs. How many bulbs should I buy if my wife says she saw twice as many large ceiling lights as medium ceiling lights and ten more small lights than medium ones?\n",
      "118\n",
      "----\n",
      "Tim buys a cabinet for $1200 and gets a 15% discount.  How much did he pay?\n",
      "1020\n",
      "----\n",
      "Grant scored 10 points higher on his math test than John.  John received twice as many points as Hunter who scored a 45 on his math test.  What was Grant's test score?\n",
      "100\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1155251",
   "metadata": {},
   "source": [
    "#### Tasks on Auto-CoT (AQUA-RAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a68681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maqua_rat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset('aqua_rat', 'raw', cache_dir=cache_dir)['train']\n",
    "inputs = [d['question'] + \" \" + \" \".join(d['options']) for d in data]\n",
    "labels = [d['correct'] for d in data]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "794cfda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the sum of 100 consecutive integers from -49 inclusive, in a increasing order? A)-29 B)50 C)-30 D)30 E)60\n",
      "B\n",
      "----\n",
      "A box contains nine bulbs out of which 4 are defective. If four bulbs are chosen at random, find the probability that atleast one bulb is good? A)125/128 B)125/120 C)125/126 D)125/125 E)125/121\n",
      "C\n",
      "----\n",
      "Four of the five parts numbered (a), (b), (c), (d) and (e) are exactly equal. Which of the parts is not equal to the other four? The number of that part is the answer. A)16.80 × 4.50 + 4.4 B)1600 ÷ 40 + 16 × 2.5 C)5.5 × 8.4 + 34.6 D)1620 ÷ 20 – 1 E)1856.95 – 1680.65 – 96.3\n",
      "C\n",
      "----\n",
      "An investment of $3000 was made in a certain account and earned interest that was compounded annually. The annual interest rate was fixed for the duration of the investment, and after 12 years the $3000 increased to $12000 by earning interest. In how many years after the initial investment was made the $3000 have increased to $24000 by earning interest at that rate? A)16 B)22 C)20 D)18 E)30\n",
      "D\n",
      "----\n",
      "Ramu bought an old car for Rs. 42000. He spent Rs. 13000 on repairs and sold it for Rs. 64900. What is his profit percent? A)16% B)88% C)18% D)14% E)28%\n",
      "C\n",
      "----\n",
      "C and D started a business investing Rs. 49,000 and Rs. 35,000 respectively. In what ratio the profit earned after 4 years be divided between C and D respectively? A)7:4 B)7:5 C)6:4 D)5:5 E)None of these\n",
      "B\n",
      "----\n",
      "What is the area M of the square with the following coordinates: (x, y), (20, 20), (20, 5), (x, 5)? A)60. B)85. C)125. D)225. E)It cannot be determined from the information given\n",
      "D\n",
      "----\n",
      "Shobha's Mathematics Test had 75 problems i.e. 10 arithmetic, 30 algebra and 35 geometry problems. Although she answered 70% of the arithmetic, 40% of the algebra and 60% 0f the geometry problems correctly, she did not pass the test because she got less than 60% of the problems right. How many more questions she would have needed to answer correctly to earn a 60% passing grade? A)5 B)10 C)15 D)20 E)25\n",
      "A\n",
      "----\n",
      "If a, b, and c are consecutive even integers and a < b < c, all of the following must be divisible by 4 EXCEPT A)a + c B)b + c C)ac D)(bc)/2 E)(abc)/4\n",
      "B\n",
      "----\n",
      "Two trains, each 160 m long, moving in opposite directions, cross other in 8 sec. If one is moving twice as fast the other, then the speed of the faster train is? A)26 km/hr B)17 km/hr C)60 km/hr D)77 km/hr E)96 km/hr\n",
      "E\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c2fb3",
   "metadata": {},
   "source": [
    "#### Tasks on Auto-CoT (Commonsense QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709b807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset commonsense_qa (/gscratch/zlab/bparan/projects/cascades/data/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1015.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset('commonsense_qa', cache_dir=cache_dir)['validation']\n",
    "inputs = [d['question']+ \" \" + \" \".join([k + \") \" + v for k, v in zip(d['choices']['label'], d['choices']['text'])]) for d in data]\n",
    "labels = [d['answerKey'] for d in data]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b095462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can you get the attention of a person across the room? A) board ship B) shout at C) smile at D) cross street E) feel happy\n",
      "B\n",
      "----\n",
      "Where would you find a metal rod in most people's preferred method of transportation? A) airplane B) construction site C) shops D) engine E) broken bone\n",
      "D\n",
      "----\n",
      "What can you do in your house during the winter? A) ski B) skate C) play hockey D) blaze it E) knit\n",
      "E\n",
      "----\n",
      "The drought was dangerous for the trees, they were more likely to what? A) wall in B) grow tall C) fire D) covered in snow E) burn\n",
      "E\n",
      "----\n",
      "Where do you get a cheap room at a reception desk? A) lobby B) office park C) at hotel D) cellphone E) motel\n",
      "E\n",
      "----\n",
      "What kind of food is not sweet? A) dry B) wet C) bitter D) nonsweet E) decaying\n",
      "C\n",
      "----\n",
      "What is an easy way to make a potato soft? A) restaurants B) cooking pot C) beef stew D) steak house E) let it rot\n",
      "B\n",
      "----\n",
      "Billy Bob sat on a bar stool and ordered a steak.  He ordered a steak because despite the style this place didn't serve alcohol.  What sort of place was this? A) dining hall B) drunker C) tavern D) kitchen E) restaurant\n",
      "E\n",
      "----\n",
      "Where should I not put these boxes if my house is likely to flood? A) garage B) cellar C) basement D) kitchen E) attic\n",
      "C\n",
      "----\n",
      "What do soldiers do when their opponents get the upper hand? A) follow orders B) use weapons C) call reinforcements D) coming home E) die in battle\n",
      "E\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d6ee6",
   "metadata": {},
   "source": [
    "#### AMA Tasks (From Super-Glue they include boolQ, cb, copa, multirc, record, rte, wsc, WiC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "accca089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset super_glue (/gscratch/zlab/bparan/projects/cascades/data/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a5c1da6d954ba9b9ed5f2fa392527d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BoolQ\n",
    "data = datasets.load_dataset('super_glue', 'boolq', cache_dir=cache_dir)['validation']\n",
    "inputs = [d['passage']+ \" \" + d['question'][0].title() + d['question'][1:]  + \"?\" for d in data]\n",
    "label_dict = {0:'False', 1:'True'}\n",
    "labels = [label_dict[d['label']] for d in data]\n",
    "# Similar transformations to be made for other Superglue tasks: cb, copa, multirc, record, rte, wsc, wic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc8249",
   "metadata": {},
   "source": [
    "#### AMA Tasks (From Adversarial NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e7cd285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset anli (/gscratch/zlab/bparan/projects/cascades/data/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f106af0947424b2e9ec8b9294df88e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can also look at dev_r2, dev_r2\n",
    "data = datasets.load_dataset('anli', cache_dir=cache_dir)['dev_r3']\n",
    "inputs = [\"Sentence1: \" + d['premise'] + \"\\nSentence2: \" +d['hypothesis'] for d in data]\n",
    "label_dict = {0:\"entailment\", 1:'neutral', 2:'contradiction'}\n",
    "labels = [label_dict[d['label']] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad5715",
   "metadata": {},
   "source": [
    "#### Flipped Learning tasks Storycloze, PIQA, HellaSwag, ARC challenge and openbookQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f5368aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-fe6cd1e9646dcb63\n",
      "WARNING:datasets.builder:Found cached dataset story_cloze (/mmfs1/home/bparan/.cache/huggingface/datasets/story_cloze/default-fe6cd1e9646dcb63/0.0.0/45cead0538c3deb72d731a7990e60835c2c9c5d5d5b1e95a7dd47ccf593671e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86691af826074641a0c7358f8fa8aa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'story_id': 'bff9f820-9605-4875-b9af-fe6f14d04256',\n",
       " 'input_sentence_1': \"Laverne needs to prepare something for her friend's party.\",\n",
       " 'input_sentence_2': 'She decides to bake a batch of brownies.',\n",
       " 'input_sentence_3': 'She chooses a recipe and follows it closely.',\n",
       " 'input_sentence_4': 'Laverne tests one of the brownies to make sure it is delicious.',\n",
       " 'sentence_quiz1': 'The brownies are so delicious Laverne eats two of them.',\n",
       " 'sentence_quiz2': \"Laverne doesn't go to her friend's party.\",\n",
       " 'answer_right_ending': 1}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# story_cloze\n",
    "validation_path = os.path.join(cache_dir, 'story_cloze') #, 'cloze_test_val__winter2018-cloze_test_ALL_val - 1 - 1.csv')\n",
    "data = datasets.load_dataset('story_cloze', data_dir=validation_path)\n",
    "data['validation'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec7cbefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset piqa (/gscratch/zlab/bparan/projects/cascades/data/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0066632dfd46c2a2944b8d765c3e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'goal': 'What alcohol do you pour for a mojito?',\n",
       " 'sol1': 'Fill the glass with ice cubes so it’s half full. Pour in 2 shots of white rum & 1-2 shots of freshly squeezed orange juice.',\n",
       " 'sol2': 'Fill the glass with ice cubes so it’s all full. Pour in 2 shots of white rum & 1-2 shots of freshly squeezed orange juice.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset('piqa', cache_dir=cache_dir)['validation']\n",
    "data[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6907a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Found cached dataset hellaswag (/gscratch/zlab/bparan/projects/cascades/data/hellaswag/default/0.1.0/c37cd37196278995f42bc32f532730ae9b0d5f0f4a2d3b97735c17ff3ad67169)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887bfec84e1a4ea3964d027182991c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ind': 92,\n",
       " 'activity_label': 'Clean and jerk',\n",
       " 'ctx_a': 'A lady walks to a barbell. She bends down and grabs the pole.',\n",
       " 'ctx_b': 'the lady',\n",
       " 'ctx': 'A lady walks to a barbell. She bends down and grabs the pole. the lady',\n",
       " 'endings': ['swings and lands in her arms.',\n",
       "  'pulls the barbell forward.',\n",
       "  'pulls a rope attached to the barbell.',\n",
       "  'stands and lifts the weight over her head.'],\n",
       " 'source_id': 'activitynet~v_-lJS58hyo1c',\n",
       " 'split': 'val',\n",
       " 'split_type': 'zeroshot',\n",
       " 'label': '3'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset('hellaswag', cache_dir=cache_dir)['validation']\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8def24e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset ai2_arc (/gscratch/zlab/bparan/projects/cascades/data/ai2_arc/ARC-Challenge/1.0.0/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dfe0c01099431ca69f2318e95d8363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_SC_407695',\n",
       " 'question': 'Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?',\n",
       " 'choices': {'text': ['Put the objects in groups.',\n",
       "   'Change the height of the ramp.',\n",
       "   'Choose different objects to roll.',\n",
       "   'Record the details of the investigation.'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is also the ARC-easy split\n",
    "data = datasets.load_dataset('ai2_arc', 'ARC-Challenge', cache_dir=cache_dir)['validation']\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87329332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset openbookqa (/gscratch/zlab/bparan/projects/cascades/data/openbookqa/main/1.0.1/f338ccacfbc86fb8c2de3aa1c06d2ce686933de3bca284dba97d32592c52b33f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b722b0083f0548b9b663fcc529090940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OpenbookQa\n",
    "data = datasets.load_dataset('openbookqa', 'main', cache_dir=cache_dir)['validation']\n",
    "# print(data[0])\n",
    "# inputs = [d['question'] + \n",
    "inputs = [d['question_stem']+ \" \" + \" \".join([k + \") \" + v for k, v in zip(d['choices']['label'], d['choices']['text'])]) for d in data]\n",
    "labels = [d['answerKey'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3043abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which describes the size of an object? A) It attracts all metals B) It is a sphere C) It is red and purple D) It holds 500 mL of water\n",
      "D\n",
      "----\n",
      "A strawberry is in zero way a true berry because it A) has seeds outside the flesh, unlike the blueberry B) is unable to spread its seed C) is a different color than other berries D) is too wet inside\n",
      "A\n",
      "----\n",
      "What is most necessary for a  bitcoin operation A) abacus B) metal C) plastic D) wood\n",
      "B\n",
      "----\n",
      "Who would be healthier? A) a night owl B) an party animal C) a heavy sleeper D) a workaholic\n",
      "C\n",
      "----\n",
      "What might be the natural habitat of a small creature known for making a noise by scraping limbs together? A) Among Birches B) Amid the snow C) In a barn D) In the ocean\n",
      "A\n",
      "----\n",
      "In the springtime, animals engage in ritual behavior such as A) creating bedding for young B) picking flowers for mates C) hunting for new prey D) migrating to cold environments\n",
      "A\n",
      "----\n",
      "okra has to have water, sun and what to grow A) planter B) music C) Earth D) shade\n",
      "C\n",
      "----\n",
      "Plant lights can be useful in order to A) dry up several plants B) lower energy costs at home C) recreate effects of the sun D) give dogs good lighting\n",
      "C\n",
      "----\n",
      "if an adult salmon goes back to where it was born, which of these is likely? A) it wants to deposit its offspring B) it is about to die C) it is looking for its friend D) it is searching for its mother\n",
      "A\n",
      "----\n",
      "which of these individuals would be more healthy? A) a student who sleeps two hours daily B) a teacher who sleeps eight hours a day C) a woman who sleeps three hours a day D) a man who practices sleep deprivation\n",
      "B\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f35b9a",
   "metadata": {},
   "source": [
    "#### Other tasks from AMA (Classification) : Agnews, DBPedia, Amazon reivews and SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f80cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Found cached dataset ag_news (/gscratch/zlab/bparan/projects/cascades/data/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4faa1f2187464a54a819648d3d973a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## For others use the strings : dbpedia_14, sst2\n",
    "data = datasets.load_dataset('ag_news', cache_dir=cache_dir)['test']\n",
    "inputs = [d['text'] for d in data]\n",
    "label_dict = {0:'World', 1:'Sports', 2:'Business', 3: 'Sci/Tech'}\n",
    "labels = [label_dict[d['label']] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41a5c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset dbpedia_14 (/gscratch/zlab/bparan/projects/cascades/data/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065479ca67db4fe281d10a73cecb88e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'title': 'TY KU',\n",
       " 'content': \" TY KU /taɪkuː/ is an American alcoholic beverage company that specializes in sake and other spirits. The privately-held company was founded in 2004 and is headquartered in New York City New York. While based in New York TY KU's beverages are made in Japan through a joint venture with two sake breweries. Since 2011 TY KU's growth has extended its products into all 50 states.\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For others use the strings : dbpedia_14, sst2\n",
    "data = datasets.load_dataset('dbpedia_14', cache_dir=cache_dir)['test']\n",
    "data[0]\n",
    "# inputs = [d['text'] for d in data]\n",
    "# label_dict = {0:'World', 1:'Sports', 2:'Business', 3: 'Sci/Tech'}\n",
    "# labels = [label_dict[d['label']] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e111643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jury Can Hear of Kobe Accuser's Sex Life (AP) AP - Prosecutors suffered another setback Monday in the Kobe Bryant sexual assault case, losing a last-ditch attempt to keep the NBA star's lawyers from telling jurors about the alleged victim's sex life.\n",
      "Sports\n",
      "----\n",
      "Iraq war was illegal: Chirac HANOI: French President Jacques Chirac has said the US-led war in Iraq was illegal and expressed his fear for the countrys future in the face of a civil war.\n",
      "World\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a0cae",
   "metadata": {},
   "source": [
    "#### Tasks from reframing natural language instructions\n",
    "\n",
    "MC-Taco (question generation), QASC (question generation), Quoref, Winogrande, CosmosQA, MultiRC, MC-TACO (Incorrect answer generation for duration and transcience questions), QASC (overlapping words), Essential terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "74424eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download natural instructions\n",
    "\n",
    "def transform_example(example):\n",
    "    # Very simple transformation of the example\n",
    "    return example[\"input\"] + \"\\n\" + example[\"output\"]\n",
    "\n",
    "NQ_REWRITE_TASKS = ['task003_mctaco_question_generation_event_duration.json', \n",
    "           'task040_qasc_question_generation.json',\n",
    "           'task002_quoref_answer_generation.json',\n",
    "           'task033_winogrande_answer_generation.json',\n",
    "           'task024_cosmosqa_answer_generation.json',\n",
    "           'task056_multirc_classify_correct_answer.json',\n",
    "           'task005_mctaco_wrong_answer_generation_event_duration.json', \n",
    "           'task008_mctaco_wrong_answer_generation_transient_stationary.json',\n",
    "           'task039_qasc_find_overlapping_words.json',\n",
    "           'task044_essential_terms_identifying_essential_words.json']\n",
    "for task_name in NQ_REWRITE_TASKS:\n",
    "    nq_path = \"/mmfs1/gscratch/zlab/bparan/projects/cascades/src/natural-instructions/tasks\"\n",
    "    task_path = os.path.join(nq_path, task_name)\n",
    "    task_data = json.loads(open(task_path).read())\n",
    "    instances = []\n",
    "    for instance in task_data[\"Instances\"]:\n",
    "        instances.append({\"guid\": instance['id'], \"input\": instance[\"input\"], \"label\": instance[\"output\"][0]})\n",
    "    instruction = task_data[\"Definition\"]\n",
    "    examples = \"\"\n",
    "    for ex in task_data[\"Positive Examples\"]:\n",
    "        examples += transform_example(ex)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2f3658ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'task003-5acd7c2444ca49bcb491048ce92e2d6d',\n",
       " 'input': \"Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.\",\n",
       " 'label': 'How long was the storm?'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a0ad4",
   "metadata": {},
   "source": [
    "#### Additional promising datasets from BigBench\n",
    "\n",
    "* Word Sorting\n",
    "* Word Unscrambling\n",
    "* Which Wikipedia Edit\n",
    "* Unnatural incontext learning\n",
    "* Unit Interpretation\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99d6134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/word_sorting/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851cc7b598ac4b22a148816efaacd1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'word_sorting', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17147ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the following words alphabetically:\n",
      "List: bangor glossary frontage illustrious rill sidelong suzerain reduce firm cohort demitted millet willful camden portsmouth\n",
      "Sorted:\n",
      "bangor camden cohort demitted firm frontage glossary illustrious millet portsmouth reduce rill sidelong suzerain willful\n",
      "----\n",
      "Sort the following words alphabetically:\n",
      "List: conrail circumference woodbury pier russo wallace urchin peak janeiro drayman\n",
      "Sorted:\n",
      "circumference conrail drayman janeiro peak pier russo urchin wallace woodbury\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0abf0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The word craeg is a scrambled version of the English word \n",
      "A:\n",
      "grace\n",
      "----\n",
      "Q: The word ikingln is a scrambled version of the English word \n",
      "A:\n",
      "linking\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0162585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/word_unscrambling/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999e81f3d48b4a709787601450d25374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7134"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'word_unscrambling', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f354c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The word reasb is a scrambled version of the English word \n",
      "A:\n",
      "----\n",
      "Q: The word loladr is a scrambled version of the English word \n",
      "A:\n",
      "----\n",
      "Q: The word peeedr is a scrambled version of the English word \n",
      "A:\n",
      "----\n",
      "Q: The word invu is a scrambled version of the English word \n",
      "A:\n",
      "----\n",
      "Q: The word rdnabeesicol is a scrambled version of the English word \n",
      "A:\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "#     print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1f3fe92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/which_wiki_edit/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e8199c180a4a339c99445abd4f060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'which_wiki_edit', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d7af1461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which message corresponds to this edit?\n",
      "\n",
      "Dòng 31:\n",
      "Dòng 31:\n",
      "\n",
      " \n",
      "{{xem thêm thông tin|Bầu chọn Thế vận hội Mùa hè 2020}}\n",
      " \n",
      "{{xem thêm thông tin|Bầu chọn Thế vận hội Mùa hè 2020}}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "−\n",
      "[[:en:Tokyo bid for the 2020 Summer Olympics|Tokyo]], [[:en:Istanbul bid for the 2020 Summer Olympics|Istanbul]], và [[:en:Madrid bid for the 2020 Summer Olympics|Madrid]] là ba thành phố ứng cử viên. Các thành phố ứng cử viên của [[:en:Baku bid for the 2020 Summer Olympics|Baku (Azerbaijan)]] và [[:en:Doha bid for the 2020 Summer Olympics|Doha (Qatar)]] đã không được thăng cấp lên vị trí ứng cử viên. Một [[:en:Rome bid for the 2020 Summer Olympics|giá thầu]] từ [[Roma]] đã được rút lui.\n",
      "+\n",
      "[[Tokyo bid for the 2020 Summer Olympics|Tokyo]], [[Istanbul bid for the 2020 Summer Olympics|Istanbul]], và [[Madrid bid for the 2020 Summer Olympics|Madrid]] là ba thành phố ứng cử viên. Các thành phố ứng cử viên của [[Baku bid for the 2020 Summer Olympics|Baku (Azerbaijan)]] và [[Doha bid for the 2020 Summer Olympics|Doha (Qatar)]] đã không được thăng cấp lên vị trí ứng cử viên. Một [[Rome bid for the 2020 Summer Olympics|giá thầu]] từ [[Roma]] đã được rút lui.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "===Bầu cử thành phố chủ nhà===\n",
      " \n",
      "===Bầu cử thành phố chủ nhà===\n",
      "\n",
      "Dòng 57:\n",
      "Dòng 57:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Vào ngày 24 tháng 3 năm 2020,thủ tướng Nhật Bản [[Abe Shinzo]] đã thông báo rằng Thế vận hội mùa hè sẽ bị hoãn lại cho đến năm 2021\n",
      " \n",
      "Vào ngày 24 tháng 3 năm 2020,thủ tướng Nhật Bản [[Abe Shinzo]] đã thông báo rằng Thế vận hội mùa hè sẽ bị hoãn lại cho đến năm 2021\n",
      "\n",
      "−\n",
      "\"<ref> {{chú thích báo|url=https://theguardian.com/sport/2020/mar/24/tokyo-olympics-to-be-postponed-to-2021-due-to-coronavirus-pandemic \n",
      "+\n",
      "\"<ref>{{chú thích báo|url=https://theguardian.com/sport/2020/mar/24/tokyo-olympics-to-be-postponed-to-2021-due-to-coronavirus-pandemic \n",
      "\n",
      " \n",
      "Vào ngày 30 tháng 3,Thế vận hội mùa hè có lịch trình mới,với lễ khai mạc sẽ diễn ra vào ngày 23 tháng 7 năm 2021 với lễ bế mạc vào ngày 8 tháng 8 năm 2021\n",
      " \n",
      "Vào ngày 30 tháng 3,Thế vận hội mùa hè có lịch trình mới,với lễ khai mạc sẽ diễn ra vào ngày 23 tháng 7 năm 2021 với lễ bế mạc vào ngày 8 tháng 8 năm 2021\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Dòng 612:\n",
      "Dòng 612:\n",
      "\n",
      " \n",
      "[[Thể loại:Thế vận hội Mùa hè theo năm|2020]]\n",
      " \n",
      "[[Thể loại:Thế vận hội Mùa hè theo năm|2020]]\n",
      "\n",
      " \n",
      "[[Thể loại:Tác động của đại dịch COVID-19 đối với Thế vận hội Mùa hè 2020| ]]\n",
      " \n",
      "[[Thể loại:Tác động của đại dịch COVID-19 đối với Thế vận hội Mùa hè 2020| ]]\n",
      "\n",
      " \n",
      "+\n",
      "[[Thể loại:Giải đấu thể thao Tokyo]]\n",
      "\n",
      "  choice: AlphamaEditor, thêm thể loại,  Executed time: 00:00:04.1701645 using [[Project:AWB|AWB]]\n",
      "  choice: Đã lùi lại sửa đổi của [[Special:Contributions/2001:EE0:49A5:D7E0:A95B:58B6:B42:801A|2001:EE0:49A5:D7E0:A95B:58B6:B42:801A]] ([[User talk:2001:EE0:49A5:D7E0:A95B:58B6:B42:801A|Thảo luận]]) quay về phiên bản cuối của [[User:Lehuy2019|Lehuy2019]]\n",
      "  choice: /* Tác động của đại dịch Covid-19 */\n",
      "  choice: Cập nhật lại mới nhất theo en wiki (đầu)\n",
      "A:\n",
      "AlphamaEditor, thêm thể loại,  Executed time: 00:00:04.1701645 using [[Project:AWB|AWB]]\n",
      "----\n",
      "Q: Which message corresponds to this edit?\n",
      "\n",
      "Line 340:\n",
      "Line 340:\n",
      "\n",
      " \n",
      "During a 2017 show in Perth, Springsteen made a statement celebrating the post-inauguration [[2017 Women's March|Women's March]] against the incoming Trump administration in cities worldwide: \"We're a long way from home, and our hearts and spirits are with the hundreds of thousands of women and men that marched yesterday in every city in America, and in Melbourne&nbsp;... [They] rallied against hate and division and in support of tolerance, inclusion, reproductive rights, civil rights, racial justice, LGBTQ rights, the environment, wage equality, gender equality, healthcare, and immigrant rights. We stand with you. We are the new American resistance.\"<ref>{{Cite news|url=https://www.rollingstone.com/music/music-news/bruce-springsteen-on-womens-march-the-new-american-resistance-113698/|title=Bruce Springsteen on Women's March: 'The New American Resistance'|last=Kreps|first=Daniel|date=January 22, 2017|work=Rolling Stone|access-date=September 20, 2018|language=en-US}}</ref><ref>{{Citation|last=Bruce Springsteen|title=Bruce Springsteen in Perth – January 22, 2017|date=January 22, 2017|url=https://www.youtube.com/watch?v=jSV7wsiLhpk|access-date=September 20, 2018}}</ref><ref>{{Citation|last=euronews (in English)|title=Bruce Springsteen joins 'new resistance' against 'demagogue' Trump|date=January 23, 2017|url=https://www.youtube.com/watch?v=2yfZMx4hpdw|access-date=September 20, 2018}}</ref>\n",
      " \n",
      "During a 2017 show in Perth, Springsteen made a statement celebrating the post-inauguration [[2017 Women's March|Women's March]] against the incoming Trump administration in cities worldwide: \"We're a long way from home, and our hearts and spirits are with the hundreds of thousands of women and men that marched yesterday in every city in America, and in Melbourne&nbsp;... [They] rallied against hate and division and in support of tolerance, inclusion, reproductive rights, civil rights, racial justice, LGBTQ rights, the environment, wage equality, gender equality, healthcare, and immigrant rights. We stand with you. We are the new American resistance.\"<ref>{{Cite news|url=https://www.rollingstone.com/music/music-news/bruce-springsteen-on-womens-march-the-new-american-resistance-113698/|title=Bruce Springsteen on Women's March: 'The New American Resistance'|last=Kreps|first=Daniel|date=January 22, 2017|work=Rolling Stone|access-date=September 20, 2018|language=en-US}}</ref><ref>{{Citation|last=Bruce Springsteen|title=Bruce Springsteen in Perth – January 22, 2017|date=January 22, 2017|url=https://www.youtube.com/watch?v=jSV7wsiLhpk|access-date=September 20, 2018}}</ref><ref>{{Citation|last=euronews (in English)|title=Bruce Springsteen joins 'new resistance' against 'demagogue' Trump|date=January 23, 2017|url=https://www.youtube.com/watch?v=2yfZMx4hpdw|access-date=September 20, 2018}}</ref>\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "−\n",
      "Springsteen was a staunch critic of Trump throughout his presidency. In October 2019, he said the President \"doesn't have a grasp of the deep meaning of what it means to be an American,\"<ref>{{Cite web|last=Henderson|first=Cydney|title=Bruce Springsteen: President Trump doesn't understand 'what it means to be American'|url=https://www.usatoday.com/story/entertainment/music/2019/10/24/bruce-springsteen-trump-doesnt-know-what-means-american/4088475002/|access-date=August 18, 2020|website=USA TODAY|language=en-US}}</ref> and in June 2020 called him a \"threat to our democracy.\"<ref>{{Cite web|date=June 24, 2020|title=Bruce Springsteen Calls President Trump a 'Threat to Our Democracy'|url=http://www.billboard.com/articles/columns/rock/9407927/bruce-springsteen-calls-out-trump-interview|access-date=August 18, 2020|website=Billboard}}</ref> Springsteen's song \"The Rising\" was featured prominently in the [[2020 Democratic National Convention]] in support of [[Joe Biden]], accompanied with a new video and campaign slogan, #TheRising.<ref>{{Cite web|date=August 18, 2020|title=Bruce Springsteen debuts new 'The Rising' video during Democratic National Convention|url=https://www.nme.com/news/music/bruce-springsteen-debuts-new-the-rising-video-to-back-joe-biden-2731052|access-date=August 18, 2020|website=NME|language=en-GB}}</ref> On October 13, 2020, author [[Don Winslow]] released a video critical of Trump prior to his campaign event in [[Pennsylvania]]. The video features Springsteen's song [[Streets of Philadelphia]].<ref>{{Cite web|date=October 13, 2020|title=Bruce Springsteen & Don Winslow Team On Video Just In Time For President Trump's Pennsylvania Rally: WATCH|url=https://deadline.com/2020/10/bruce-springsteen-don-winslow-video-president-trump-pennsylvania-rally-1234596540/#!|access-date=October 14, 2020|website=Deadline|language=en-GB}}</ref> A few days prior to [[2020 United States presidential election]], Springsteen provided narration for a campaign ad that spotlights Biden's upbringing in [[Scranton, Pennsylvania]] with \"My Hometown\" playing throughout the ad.<ref>{{Cite web|url=https://www.rollingstone.com/music/music-news/bruce-springsteen-joe-biden-hometown-scranton-ad-1084458/|title=Bruce Springsteen Narrates Joe Biden's 'Hometown' Scranton Ad|first1=Daniel|last1=Kreps|date=October 31, 2020}}</ref> On November 7th, 2020, when Joe Biden hit 270 electoral votes and won the presidency, the song he walked out to for his victory speech was \"We Take of Our Own\" by Bruce Springsteen.<ref name=\":4\">{{Cite web|last=Scinto|first=Maria|date=2020-11-07|title=Joe Biden's Victory Speech Walkout Song Explained|url=https://www.thelist.com/274941/joe-bidens-victory-speech-walkout-song-explained/|access-date=2021-04-29|website=TheList.com|language=en-US}}</ref> This was not a coincidence, with the song \"We Take Care of Our Own\" also being the theme song for the 2012 presidential campaign of Barack Obama and Joe Biden.<ref name=\":4\" /> The song was also familiar during Biden's 2020 campaign with the song being played at almost all of his campaign events.<ref name=\":4\" /> The song fit with Biden's victory speech which preached hope and unity.<ref name=\":4\" /> The \"our own\" is meant to be used not an exclusive way, but to encapsulate each and every American.<ref name=\":4\" />\n",
      "+\n",
      "Springsteen was a staunch critic of Trump throughout his presidency. In October 2019, he said the President \"doesn't have a grasp of the deep meaning of what it means to be an American,\"<ref>{{Cite web|last=Henderson|first=Cydney|title=Bruce Springsteen: President Trump doesn't understand 'what it means to be American'|url=https://www.usatoday.com/story/entertainment/music/2019/10/24/bruce-springsteen-trump-doesnt-know-what-means-american/4088475002/|access-date=August 18, 2020|website=USA TODAY|language=en-US}}</ref> and in June 2020 called him a \"threat to our democracy.\"<ref>{{Cite web|date=June 24, 2020|title=Bruce Springsteen Calls President Trump a 'Threat to Our Democracy'|url=http://www.billboard.com/articles/columns/rock/9407927/bruce-springsteen-calls-out-trump-interview|access-date=August 18, 2020|website=Billboard}}</ref> Springsteen's song \"The Rising\" was featured prominently in the [[2020 Democratic National Convention]] in support of [[Joe Biden]], accompanied with a new video and campaign slogan, #TheRising.<ref>{{Cite web|date=August 18, 2020|title=Bruce Springsteen debuts new 'The Rising' video during Democratic National Convention|url=https://www.nme.com/news/music/bruce-springsteen-debuts-new-the-rising-video-to-back-joe-biden-2731052|access-date=August 18, 2020|website=NME|language=en-GB}}</ref> On October 13, 2020, author [[Don Winslow]] released a video critical of Trump prior to his campaign event in [[Pennsylvania]]. The video features Springsteen's song [[Streets of Philadelphia]].<ref>{{Cite web|date=October 13, 2020|title=Bruce Springsteen & Don Winslow Team On Video Just In Time For President Trump's Pennsylvania Rally: WATCH|url=https://deadline.com/2020/10/bruce-springsteen-don-winslow-video-president-trump-pennsylvania-rally-1234596540/#!|access-date=October 14, 2020|website=Deadline|language=en-GB}}</ref> A few days prior to [[2020 United States presidential election]], Springsteen provided narration for a campaign ad that spotlights Biden's upbringing in [[Scranton, Pennsylvania]] with \"My Hometown\" playing throughout the ad.<ref>{{Cite web|url=https://www.rollingstone.com/music/music-news/bruce-springsteen-joe-biden-hometown-scranton-ad-1084458/|title=Bruce Springsteen Narrates Joe Biden's 'Hometown' Scranton Ad|first1=Daniel|last1=Kreps|date=October 31, 2020}}</ref> Biden used \"We Take Care of Our Own\" as one of his theme songs, as Obama had before him in 2012.<ref name=\":4\">{{Cite web|last=Scinto|first=Maria|date=2020-11-07|title=Joe Biden's Victory Speech Walkout Song Explained|url=https://www.thelist.com/274941/joe-bidens-victory-speech-walkout-song-explained/|access-date=2021-04-29|website=TheList.com|language=en-US}}</ref>\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "==Discography==\n",
      " \n",
      "==Discography==\n",
      "\n",
      "  choice: /* Politics */ trim new additon... this is Springsteen's bio, not Biden's campaign\n",
      "  choice: split very long section\n",
      "  choice: /* Achievements, awards, and nominations */ Woody Guthrie Prize\n",
      "  choice: /* top */[[User:PrimeBOT/30|Task 30]]: removal of invalid parameters in [[Template:Infobox musical artist]] (+ib genfixes)\n",
      "A:\n",
      "/* Politics */ trim new additon... this is Springsteen's bio, not Biden's campaign\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9208e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/unnatural_in_context_learning/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c967ff52ba443bdbd13c59e1395a37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'unnatural_in_context_learning', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17bfeddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1978-10-04\n",
      "Output:\n",
      "!10!04!1978!\n",
      "----\n",
      "Input: !62265!78769!50994!\n",
      "Output:\n",
      "50994-62265-78769\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7acf88ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 11.7kB [00:00, 12.2MB/s]                                                                                                                                                                                   \n",
      "Downloading metadata: 284kB [00:00, 231MB/s]                                                                                                                                                                                           \n",
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/unit_interpretation/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'unit_interpretation', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "034ed278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select the option that best replaces '()' in each text input given the chocies presented.\n",
      "\n",
      "Q: 27 eggs divided by 9 eggs per hen per day is ().\n",
      "  choice: 243 hen-days\n",
      "  choice: 3 hen-days\n",
      "  choice: 3 days\n",
      "  choice: 243 eggs\n",
      "  choice: 3 eggs\n",
      "A:\n",
      "3 hen-days\n",
      "----\n",
      "Please select the option that best replaces '()' in each text input given the chocies presented.\n",
      "\n",
      "Q: A newsstand sells 5 newspapers every 2 weeks. It will sell () newspapers in 60 weeks.\n",
      "  choice: 120\n",
      "  choice: 300\n",
      "  choice: 150\n",
      "  choice: 12\n",
      "  choice: 24\n",
      "A:\n",
      "150\n",
      "----\n",
      "Please select the option that best replaces '()' in each text input given the chocies presented.\n",
      "\n",
      "Q: A bell rings one time every 5 hours. It will ring 20 times in () hours.\n",
      "  choice: 20\n",
      "  choice: 50\n",
      "  choice: 25\n",
      "  choice: 100\n",
      "  choice: 4\n",
      "A:\n",
      "100\n",
      "----\n",
      "Please select the option that best replaces '()' in each text input given the chocies presented.\n",
      "\n",
      "Q: It takes 2 sausages and 5 buns to make 40 hotdogs. It will take 10 sausages and () buns to make 200 hotdogs.\n",
      "  choice: 25\n",
      "  choice: 50\n",
      "  choice: 4\n",
      "  choice: 10\n",
      "  choice: 5\n",
      "A:\n",
      "25\n",
      "----\n",
      "Please select the option that best replaces '()' in each text input given the chocies presented.\n",
      "\n",
      "Q: It takes 3 men and 3 hours to cut down 9 trees. It will take 2 men and 4 hours to cut down () trees.\n",
      "  choice: 18\n",
      "  choice: 8\n",
      "  choice: 3\n",
      "  choice: 12\n",
      "  choice: 6\n",
      "A:\n",
      "8\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e5da9044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/understanding_fables/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf8ba2b9a0d4d2880fc136e26ec1e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'understanding_fables', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a830194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: A badger, crossing a bridge over a river with a piece of meat in his teeth, saw his own reflection in the water and took it for that of another badger, with a piece of meat double his own in size. He immediately let go of his own, and attacked the other badger to get his larger piece. In the end, he lost both: that which he clawed at in the water, because it was a reflection; and his own, because the river swept it away. What is the moral of this story?\n",
      "  choice: He who is once deceived is doubly cautious.\n",
      "  choice: Those who pretend to be what they are not, sooner or later, find themselves in deep water.\n",
      "  choice: He winds up friendless who plays both sides against the middle.\n",
      "  choice: Grasp at the shadow and you will lose the substance.\n",
      "  choice: Little by little does the trick.\n",
      "A:\n",
      "Grasp at the shadow and you will lose the substance.\n",
      "----\n",
      "Q: A rooster found a shiny gem while pecking around in the dirt to find food for himself and his hens. Looking down at the stone, he observed: \"Had your owner found you, he would have picked you up and treasured you. I, however, have no use for you and would have preferred to find some corn, instead. \" What is the moral of this story?\n",
      "  choice: Better beans and bacon in peace than cakes and ale in fear.\n",
      "  choice: The ignorant despise what is precious only because they cannot understand it.\n",
      "  choice: Evil wishes, like chickens, come home to roost.\n",
      "  choice: A little thing in hand is worth more than a great thing in prospect.\n",
      "  choice: Quality is better than quantity.\n",
      "A:\n",
      "The ignorant despise what is precious only because they cannot understand it.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "71358ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/tracking_shuffled_objects/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39cdef9f6ca4581a7306d7308a683ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'tracking_shuffled_objects', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aecd551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice, Bob, Claire, Dave, and Eve are playing a game. At the start of the game, they are each holding a ball: Alice has a brown ball, Bob has a purple ball, Claire has a black ball, Dave has a green ball, and Eve has a yellow ball. \n",
      "\n",
      "As the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Bob and Alice swap balls. Then, Eve and Dave swap balls. Then, Dave and Claire swap balls. Finally, Alice and Bob swap balls. At the end of the game, Claire has the\n",
      "yellow ball.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, and Eve are playing a game. At the start of the game, they are each holding a ball: Alice has a black ball, Bob has a brown ball, Claire has a red ball, Dave has a white ball, and Eve has a green ball. \n",
      "\n",
      "As the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Alice and Claire swap balls. Then, Eve and Alice swap balls. Then, Claire and Bob swap balls. Finally, Claire and Dave swap balls. At the end of the game, Eve has the\n",
      "red ball.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a red present, Bob has a yellow present, Claire has a green present, Dave has a pink ball, and Eve has a white present. \n",
      "\n",
      "As the event progresses, pairs of people swap gifts. First, Claire and Alice swap their gifts. Then, Dave and Eve swap their gifts. Then, Bob and Dave swap their gifts. Then, Eve and Dave swap their gifts. Finally, Dave and Alice swap their gifts. At the end of the event, Eve has the\n",
      "yellow present.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, and Eve are playing a game. At the start of the game, they are each holding a ball: Alice has a brown ball, Bob has a purple ball, Claire has a green ball, Dave has a white ball, and Eve has a blue ball. \n",
      "\n",
      "As the game progresses, pairs of players trade balls. First, Alice and Claire swap balls. Then, Dave and Alice swap balls. Then, Bob and Eve swap balls. Then, Claire and Eve swap balls. Finally, Alice and Dave swap balls. At the end of the game, Claire has the\n",
      "purple ball.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a blue present, Bob has a white present, Claire has a purple present, Dave has a black ball, Eve has a brown present, Fred has a orange ball, and Gertrude has a yellow present. \n",
      "\n",
      "As the event progresses, pairs of people swap gifts. First, Gertrude and Alice swap their gifts. Then, Dave and Eve swap their gifts. Then, Alice and Dave swap their gifts. Then, Fred and Dave swap their gifts. Then, Bob and Dave swap their gifts. Then, Claire and Gertrude swap their gifts. Finally, Gertrude and Dave swap their gifts. At the end of the event, Alice has the\n",
      "brown present.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Melissa, Bob is dancing with Ophelia, Claire is dancing with Patrick, Dave is dancing with Jamie, Eve is dancing with Lola, Fred is dancing with Helga, and Gertrude is dancing with Izzi. \n",
      "\n",
      "Throughout the song, the dancers often trade partners. First, Alice and Fred switch partners. Then, Gertrude and Eve switch partners. Then, Gertrude and Alice switch partners. Then, Bob and Alice switch partners. Then, Dave and Fred switch partners. Then, Eve and Alice switch partners. Finally, Eve and Claire switch partners. At the end of the dance, Alice is dancing with\n",
      "Izzi.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Izzi, Bob is dancing with Helga, Claire is dancing with Melissa, Dave is dancing with Sam, Eve is dancing with Karl, Fred is dancing with Ophelia, and Gertrude is dancing with Lola. \n",
      "\n",
      "Throughout the song, the dancers often trade partners. First, Fred and Eve switch partners. Then, Bob and Dave switch partners. Then, Eve and Alice switch partners. Then, Bob and Claire switch partners. Then, Eve and Alice switch partners. Then, Gertrude and Fred switch partners. Finally, Bob and Dave switch partners. At the end of the dance, Bob is dancing with\n",
      "Helga.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets The Odyssey, Claire gets Moby Dick, Dave gets The Great Gatsby, Eve gets Lolita, Fred gets Frankenstein, and Gertrude gets The Fellowship of the Ring. \n",
      "\n",
      "As the semester proceeds, they start trading around the new books. First, Eve and Bob swap books. Then, Eve and Claire swap books. Then, Gertrude and Alice swap books. Then, Gertrude and Claire swap books. Then, Alice and Bob swap books. Then, Bob and Dave swap books. Finally, Fred and Claire swap books. At the end of the semester, Fred has\n",
      "Ulysses.\n",
      "----\n",
      "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are playing a game. At the start of the game, they are each holding a ball: Alice has a black ball, Bob has a brown ball, Claire has a yellow ball, Dave has a purple ball, Eve has a blue ball, Fred has a orange ball, and Gertrude has a pink ball. \n",
      "\n",
      "As the game progresses, pairs of players trade balls. First, Bob and Claire swap balls. Then, Dave and Gertrude swap balls. Then, Fred and Gertrude swap balls. Then, Bob and Fred swap balls. Then, Gertrude and Eve swap balls. Then, Claire and Eve swap balls. Finally, Claire and Alice swap balls. At the end of the game, Fred has the\n",
      "yellow ball.\n",
      "----\n",
      "Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Frankenstein, Bob gets Catch-22, and Claire gets Ulysses. \n",
      "\n",
      "As the semester proceeds, they start trading around the new books. First, Bob and Alice swap books. Then, Alice and Claire swap books. Finally, Claire and Bob swap books. At the end of the semester, Claire has\n",
      "Frankenstein.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd4f9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/temporal_sequences/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f70a370ef941f782a6d5cafffb75bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'temporal_sequences', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "021f7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, Elizabeth went to the construction site. Between what times could they have gone?\n",
      "We know that: \n",
      "Elizabeth woke up at 5am.\n",
      "Lisa saw Elizabeth buying a phone at the electronics store from 5am to 10am.\n",
      "Mary saw Elizabeth waiting at the train station from 10am to 11am.\n",
      "Hannah saw Elizabeth waiting at the airport from 1pm to 3pm.\n",
      "William saw Elizabeth reading at the library from 3pm to 9pm.\n",
      "The construction site was closed after 9pm.\n",
      "Between what times could Elizabeth have gone to the construction site?\n",
      "  choice: 11am to 1pm\n",
      "  choice: 5am to 10am\n",
      "  choice: 10am to 11am\n",
      "  choice: 3pm to 9pm\n",
      "Possible times:\n",
      "11am to 1pm\n",
      "----\n",
      "Today, Andrew went to the art show. Between what times could they have gone?\n",
      "We know that: \n",
      "Andrew woke up at 6am.\n",
      "Kimberly saw Andrew waiting at the airport from 10am to 12pm.\n",
      "Anthony saw Andrew attending class at the school from 12pm to 7pm.\n",
      "John saw Andrew working out at the gym from 7pm to 8pm.\n",
      "The art show was closed after 8pm.\n",
      "Between what times could Andrew have gone to the art show?\n",
      "  choice: 12pm to 7pm\n",
      "  choice: 7pm to 8pm\n",
      "  choice: 10am to 12pm\n",
      "  choice: 6am to 10am\n",
      "Possible times:\n",
      "6am to 10am\n",
      "----\n",
      "Today, Susan went to the orchestra hall. Between what times could they have gone?\n",
      "We know that: \n",
      "Susan woke up at 5am.\n",
      "Thomas saw Susan buying cookies at a bakery from 5am to 9am.\n",
      "Ashley saw Susan buying a phone at the electronics store from 9am to 10am.\n",
      "Betty saw Susan driving to the water park from 10am to 12pm.\n",
      "Linda saw Susan watching a movie at the theater from 1pm to 3pm.\n",
      "Leslie saw Susan walking towards the Statue of Liberty from 3pm to 8pm.\n",
      "The orchestra hall was closed after 8pm.\n",
      "Between what times could Susan have gone to the orchestra hall?\n",
      "  choice: 12pm to 1pm\n",
      "  choice: 5am to 9am\n",
      "  choice: 10am to 12pm\n",
      "  choice: 3pm to 8pm\n",
      "Possible times:\n",
      "12pm to 1pm\n",
      "----\n",
      "Today, Jennifer went to the art show. Between what times could they have gone?\n",
      "We know that: \n",
      "Jennifer woke up at 5am.\n",
      "Ashley saw Jennifer sitting on a rooftop from 7am to 3pm.\n",
      "David saw Jennifer buying a bike at the bike shop from 3pm to 5pm.\n",
      "Steven saw Jennifer playing tennis at the tennis court from 5pm to 6pm.\n",
      "Susan saw Jennifer waiting at the airport from 6pm to 8pm.\n",
      "Anthony saw Jennifer stretching at a yoga studio from 8pm to 10pm.\n",
      "The art show was closed after 10pm.\n",
      "Between what times could Jennifer have gone to the art show?\n",
      "  choice: 5am to 7am\n",
      "  choice: 6pm to 8pm\n",
      "  choice: 3pm to 5pm\n",
      "  choice: 5pm to 6pm\n",
      "Possible times:\n",
      "5am to 7am\n",
      "----\n",
      "Today, James went to the physics classroom. Between what times could they have gone?\n",
      "We know that: \n",
      "James woke up at 8am.\n",
      "Richard saw James working out at the gym from 10am to 11am.\n",
      "Mark saw James sitting on a rooftop from 11am to 1pm.\n",
      "Lisa saw James fixing their computer at the electronic store from 1pm to 3pm.\n",
      "Mary saw James playing tennis at the tennis court from 3pm to 7pm.\n",
      "The physics classroom was closed after 7pm.\n",
      "Between what times could James have gone to the physics classroom?\n",
      "  choice: 11am to 1pm\n",
      "  choice: 8am to 10am\n",
      "  choice: 10am to 11am\n",
      "  choice: 3pm to 7pm\n",
      "Possible times:\n",
      "8am to 10am\n",
      "----\n",
      "Today, Leslie went to the movies. Between what times could they have gone?\n",
      "We know that: \n",
      "Leslie woke up at 9am.\n",
      "Betty saw Leslie working out at the gym from 9am to 10am.\n",
      "Linda saw Leslie taking photos near the Leaning Tower of Pisa from 10am to 11am.\n",
      "William saw Leslie walking towards the Statue of Liberty from 11am to 12pm.\n",
      "The movies was closed after 9pm.\n",
      "Between what times could Leslie have gone to the movies?\n",
      "  choice: 11am to 12pm\n",
      "  choice: 9am to 10am\n",
      "  choice: 10am to 11am\n",
      "  choice: 12pm to 9pm\n",
      "Possible times:\n",
      "12pm to 9pm\n",
      "----\n",
      "Today, Kimberly went to the construction site. Between what times could they have gone?\n",
      "We know that: \n",
      "Kimberly woke up at 8am.\n",
      "Richard saw Kimberly taking photos near the Eiffel Tower from 8am to 9am.\n",
      "Sarah saw Kimberly watching a movie at the theater from 9am to 12pm.\n",
      "Tiffany saw Kimberly reading at the library from 1pm to 5pm.\n",
      "The construction site was closed after 5pm.\n",
      "Between what times could Kimberly have gone to the construction site?\n",
      "  choice: 1pm to 5pm\n",
      "  choice: 9am to 12pm\n",
      "  choice: 12pm to 1pm\n",
      "  choice: 8am to 9am\n",
      "Possible times:\n",
      "12pm to 1pm\n",
      "----\n",
      "Today, Jennifer went to the park. Between what times could they have gone?\n",
      "We know that: \n",
      "Jennifer woke up at 10am.\n",
      "Susan saw Jennifer walking in the garden from 10am to 5pm.\n",
      "Elizabeth saw Jennifer buying lunch at the deli from 5pm to 6pm.\n",
      "Sean saw Jennifer playing tennis at the tennis court from 7pm to 9pm.\n",
      "The park was closed after 9pm.\n",
      "Between what times could Jennifer have gone to the park?\n",
      "  choice: 5pm to 6pm\n",
      "  choice: 7pm to 9pm\n",
      "  choice: 10am to 5pm\n",
      "  choice: 6pm to 7pm\n",
      "Possible times:\n",
      "6pm to 7pm\n",
      "----\n",
      "Today, William went to the soccer field. Between what times could they have gone?\n",
      "We know that: \n",
      "William woke up at 1pm.\n",
      "Tiffany saw William taking photos near the Leaning Tower of Pisa from 1pm to 2pm.\n",
      "David saw William waiting at the airport from 2pm to 6pm.\n",
      "Thomas saw William reading at the library from 6pm to 7pm.\n",
      "The soccer field was closed after 8pm.\n",
      "Between what times could William have gone to the soccer field?\n",
      "  choice: 2pm to 6pm\n",
      "  choice: 7pm to 8pm\n",
      "  choice: 1pm to 2pm\n",
      "  choice: 6pm to 7pm\n",
      "Possible times:\n",
      "7pm to 8pm\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fdb314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/symbol_interpretation/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 10.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'symbol_interpretation', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41a36971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the SIT-adversarial world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔺 is a red circle;\n",
      " 🟦 is a blue circle;\n",
      " 🔴 is a yellow circle;\n",
      " 🟥 is a red triangle pointing up;\n",
      " 🟨 is a red triangle pointing down;\n",
      " 🔻 is a red square;\n",
      " 🟡 is a blue square;\n",
      " _ is a yellow square;\n",
      " 🔵 is an empty space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🟥 🔻 🟨 _ 🔴 🟨 and not consistent with 🔺 🟥 🟥 🟡 _ 🟦:\n",
      "\n",
      "  choice: There are at most two blue circles.\n",
      "\n",
      "  choice: There is exactly one blue piece.\n",
      "\n",
      "  choice: There is at least one triangle pointing down.\n",
      "\n",
      "  choice: There are at least two yellow squares.\n",
      "\n",
      "  choice: There is exactly one blue circle.\n",
      "\n",
      "A:\n",
      "There is at least one triangle pointing down.\n",
      "----\n",
      "In the SIT-adversarial world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔺 is a red circle;\n",
      " 🟦 is a blue circle;\n",
      " 🔴 is a yellow circle;\n",
      " 🟥 is a red triangle pointing up;\n",
      " 🟨 is a red triangle pointing down;\n",
      " 🔻 is a red square;\n",
      " 🟡 is a blue square;\n",
      " _ is a yellow square;\n",
      " 🔵 is an empty space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🟥 🔴 🟥 🔺 🟥 🟡 and not consistent with 🔴 🔵 _ 🔵 🟨 🔴:\n",
      "\n",
      "  choice: There are exactly two yellow pieces or exactly one yellow circle.\n",
      "\n",
      "  choice: There is exactly one red circle and zero red pieces.\n",
      "\n",
      "  choice: There are zero red squares or exactly one yellow piece.\n",
      "\n",
      "  choice: There are at most two triangles pointing up and at most one blue square.\n",
      "\n",
      "  choice: There is at most one triangle pointing down or at least one blue square.\n",
      "\n",
      "A:\n",
      "There are exactly two yellow pieces or exactly one yellow circle.\n",
      "----\n",
      "In the SIT-emoji-agnostic world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🐮 is a red circle;\n",
      " 🐺 is a blue circle;\n",
      " 🦁 is a yellow circle;\n",
      " 🐷 is a red triangle pointing up;\n",
      " 🐯 is a red triangle pointing down;\n",
      " 🐱 is a red square;\n",
      " 🦝 is a blue square;\n",
      " 🐔 is a yellow square;\n",
      " 🦄 is an empty space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🐱 🦁 🦁 🦝 🐔 🦁 and not consistent with 🐷 🐱 🦝 🦁 🐱 🦄:\n",
      "\n",
      "  choice: There are exactly two triangles pointing up touching a triangle pointing up.\n",
      "\n",
      "  choice: There is exactly one red piece at the left of a yellow circle.\n",
      "\n",
      "  choice: There are zero red circles surrounded by squares.\n",
      "\n",
      "  choice: There are at least two yellow circles surrounded by circles.\n",
      "\n",
      "  choice: There is exactly one blue square at the right of a triangle pointing up.\n",
      "\n",
      "A:\n",
      "There is exactly one red piece at the left of a yellow circle.\n",
      "----\n",
      "In the SIT-emoji-agnostic world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🐮 is a red circle;\n",
      " 🐺 is a blue circle;\n",
      " 🦁 is a yellow circle;\n",
      " 🐷 is a red triangle pointing up;\n",
      " 🐯 is a red triangle pointing down;\n",
      " 🐱 is a red square;\n",
      " 🦝 is a blue square;\n",
      " 🐔 is a yellow square;\n",
      " 🦄 is an empty space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🐯 🐮 🐱 🐮 🐯 🐯 and not consistent with 🦄 🦁 🦁 🐔 🐯 🦄:\n",
      "\n",
      "  choice: There are at most two yellow squares or at most two blue circles.\n",
      "\n",
      "  choice: There is exactly one red square and at least two red circles.\n",
      "\n",
      "  choice: There is at least one blue circle or exactly two triangles.\n",
      "\n",
      "  choice: There are at most two yellow circles and exactly one triangle.\n",
      "\n",
      "  choice: There is at most one red circle and exactly two red pieces.\n",
      "\n",
      "A:\n",
      "There is exactly one red square and at least two red circles.\n",
      "----\n",
      "In the SIT-name-agnostic world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a X D;\n",
      " 🔵 is a H D;\n",
      " 🟡 is a U D;\n",
      " 🔺 is a X Y G;\n",
      " 🔻 is a X Y A;\n",
      " 🟥 is a X B;\n",
      " 🟦 is a H B;\n",
      " 🟨 is a U B;\n",
      " _ is a T.\n",
      "\n",
      "Choose the sentence consistent with the structure 🟡 🔵 🔴 🟨 🟦 🔺 and not consistent with 🟡 🟡 🟨 🟥 🟡 🟨:\n",
      "\n",
      "  choice: There are exactly two H Is and exactly one U I.\n",
      "\n",
      "  choice: There is at most one U I or at most one H D.\n",
      "\n",
      "  choice: There are exactly two U Is and at most one H B.\n",
      "\n",
      "  choice: There are at least two X Ds or exactly one X I.\n",
      "\n",
      "  choice: There is at least one U B or exactly one Y A.\n",
      "\n",
      "A:\n",
      "There are exactly two U Is and at most one H B.\n",
      "----\n",
      "In the SIT-name-agnostic world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a X D;\n",
      " 🔵 is a H D;\n",
      " 🟡 is a U D;\n",
      " 🔺 is a X Y G;\n",
      " 🔻 is a X Y A;\n",
      " 🟥 is a X B;\n",
      " 🟦 is a H B;\n",
      " 🟨 is a U B;\n",
      " _ is a T.\n",
      "\n",
      "Choose the sentence consistent with the structure 🟨 🟨 🔵 🔴 🟨 _ and not consistent with 🔺 🔻 🟦 🔻 🟡 _:\n",
      "\n",
      "  choice: There are zero Ys touching a D.\n",
      "\n",
      "  choice: There is at most one U D touching a X B.\n",
      "\n",
      "  choice: There is at least one B touching a Y A.\n",
      "\n",
      "  choice: There are at least two Ys surrounded by X Bs.\n",
      "\n",
      "  choice: There is at least one U B at the left of a Y.\n",
      "\n",
      "A:\n",
      "There are zero Ys touching a D.\n",
      "----\n",
      "In the SIT-name-agnostic world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a X D;\n",
      " 🔵 is a H D;\n",
      " 🟡 is a U D;\n",
      " 🔺 is a X Y G;\n",
      " 🔻 is a X Y A;\n",
      " 🟥 is a X B;\n",
      " 🟦 is a H B;\n",
      " 🟨 is a U B;\n",
      " _ is a T.\n",
      "\n",
      "Choose the sentence consistent with the structure 🔻 🔻 🔻 🔴 🔻 🔻 and not consistent with 🟡 🔵 🟨 🔺 🔺 🟥:\n",
      "\n",
      "  choice: There are exactly two X Is or at most two X Ds.\n",
      "\n",
      "  choice: There is exactly one B and exactly two X Ds.\n",
      "\n",
      "  choice: There are zero Ys or at most two Ds.\n",
      "\n",
      "  choice: There are exactly two U Is or exactly one U D.\n",
      "\n",
      "  choice: There is at most one H I and at least two Ys A.\n",
      "\n",
      "A:\n",
      "There is at most one H I and at least two Ys A.\n",
      "----\n",
      "In the SIT-plain world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a red circle;\n",
      " 🔵 is a blue circle;\n",
      " 🟡 is a yellow circle;\n",
      " 🔺 is a red triangle pointing_up;\n",
      " 🔻 is a red triangle pointing_down;\n",
      " 🟥 is a red square;\n",
      " 🟦 is a blue square;\n",
      " 🟨 is a yellow square;\n",
      " _ is an empty_space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🔵 🔻 🔺 🔺 🔵 🔵 and not consistent with 🔴 🟦 🔵 🟥 🟦 🔻:\n",
      "\n",
      "  choice: There are at least two triangles touching a blue circle.\n",
      "\n",
      "  choice: There is exactly one blue piece at the right of a circle.\n",
      "\n",
      "  choice: There are at most two triangles pointing up at the right of a square.\n",
      "\n",
      "  choice: There are at least two red pieces touching a blue square.\n",
      "\n",
      "  choice: There are exactly two yellow pieces at the right of a yellow piece.\n",
      "\n",
      "A:\n",
      "There are at least two triangles touching a blue circle.\n",
      "----\n",
      "In the SIT-plain world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a red circle;\n",
      " 🔵 is a blue circle;\n",
      " 🟡 is a yellow circle;\n",
      " 🔺 is a red triangle pointing_up;\n",
      " 🔻 is a red triangle pointing_down;\n",
      " 🟥 is a red square;\n",
      " 🟦 is a blue square;\n",
      " 🟨 is a yellow square;\n",
      " _ is an empty_space.\n",
      "\n",
      "Choose the sentence consistent with the structure 🔻 🟡 🟦 🟨 🟡 🟨 and not consistent with 🔺 🟦 🟦 🔺 🔺 🟦:\n",
      "\n",
      "  choice: There are at most two triangles touching a blue square.\n",
      "\n",
      "  choice: There is at most one circle at the right of a yellow square.\n",
      "\n",
      "  choice: There are exactly two triangles at the right of a blue piece.\n",
      "\n",
      "  choice: There are at least two blue circles touching a blue circle.\n",
      "\n",
      "  choice: There are exactly two yellow circles at the left of a blue piece.\n",
      "\n",
      "A:\n",
      "There are at most two triangles touching a blue square.\n",
      "----\n",
      "In the SIT-tricky world a structure is a sequence of six emojis.\n",
      "Hereafter are reported the emojis used along with their descriptions.\n",
      " 🔴 is a der elcric;\n",
      " 🔵 is an eulb elcric;\n",
      " 🟡 is a wolley elcric;\n",
      " 🔺 is a der elgnairt gnitniop pu;\n",
      " 🔻 is a der elgnairt gnitniop nwod;\n",
      " 🟥 is a der erauqs;\n",
      " 🟦 is an eulb erauqs;\n",
      " 🟨 is a wolley erauqs;\n",
      " _ is a ytpme ecaps.\n",
      "\n",
      "Choose the sentence consistent with the structure 🔵 🔻 🟡 🔴 _ 🟦 and not consistent with 🔴 🔺 🟦 🔻 🟦 🔴:\n",
      "\n",
      "  choice: There are at least two wolley erauqss.\n",
      "\n",
      "  choice: There is at least one eulb elcric.\n",
      "\n",
      "  choice: There are zero wolley eceips.\n",
      "\n",
      "  choice: There are at least two der elcrics.\n",
      "\n",
      "  choice: There is at least one der eceip.\n",
      "\n",
      "A:\n",
      "There is at least one eulb elcric.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d2a38c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/sufficient_information/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b12280b5b84454837da9782400003f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'sufficient_information', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68dc5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Ed, Jeff, and Michael are in a line. Ed is on Jeff's left. Is Mike on Ed's left?\n",
      "A:\n",
      "I do not know\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Greg lives on the fourth floor. Claire lives above Greg. Does Claire live below the tenth floor?\n",
      "A:\n",
      "I do not know\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Jake is ten feet away from me. Brynn is one hundred feet from Jake. Am I closer to Jake or Brynn?\n",
      "A:\n",
      "Jake\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Ed, Jeff, and Michael are in a circle. Ed is on Jeff's left. Is Mike on Ed's left?\n",
      "A:\n",
      "Yes\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Fred wants to run and jump and skip. Does Fred want to jump?\n",
      "A:\n",
      "Yes\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Tom knows Kathryn. Kathryn knows Emily. Does Tom know Emily?\n",
      "A:\n",
      "I do not know\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Jamal is five years old. How old is Jamal?\n",
      "A:\n",
      "Five\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: I have seven baskets of at least ten apples each. How many apples do I have?\n",
      "A:\n",
      "I do not know\n",
      "----\n",
      "Answer the question based on the provided context. If it cannot be answered, respond with 'I do not know'. Spell out all numbers.\n",
      "Q: Spencer is Elyse's older sister. Who was born first?\n",
      "A:\n",
      "Spencer\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad7eefb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/sports_understanding/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e3423c26b5482e977c41b0775efff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'sports_understanding', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a11374e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine whether the following statement or statements are plausible or implausible:\n",
      "Statement: Nelson Cruz was out at first\n",
      "Plausible/implausible?\n",
      "----\n",
      "Determine whether the following statement or statements are plausible or implausible:\n",
      "Statement: De'Aaron Fox crossed the blue line\n",
      "Plausible/implausible?\n",
      "----\n",
      "Determine whether the following statement or statements are plausible or implausible:\n",
      "Statement: Mookie Betts took a side-step three\n",
      "Plausible/implausible?\n",
      "----\n",
      "Determine whether the following statement or statements are plausible or implausible:\n",
      "Statement: John Brown went for it on fourth down in the NFC championship\n",
      "Plausible/implausible?\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 4)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "#     print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "58ec466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/snarks/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6a77c05a68499b9ab26d11b609ca79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'snarks', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "42e1f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which statement is sarcastic? (a) Sneaking out of a government facility with dangerous contraband isn't going to be easy. (b) Sneaking out of a government facility with dangerous contraband isn't going to be illegal.\n",
      "A:\n",
      "(b)\n",
      "----\n",
      "Q: Which statement is sarcastic? (a) Remember, his Batman score isn't the only Batman theme.  (b) Remember, his Batman score is the only Batman theme. \n",
      "A:\n",
      "(b)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c306be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/simple_text_editing/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 55.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'simple_text_editing', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cb13484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit the text 'Turing was one of the people who worked on the first computers. He created the theoretical Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.' with the following edit operation and the scope of the edit and replicate the remaining text as is: swap every first and last character of each word in the 1st sentence\n",
      "Edited:\n",
      "gurinT saw eno fo eht eeoplp ohw dorkew no eht tirsf somputerc. He created the theoretical Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n",
      "----\n",
      "Edit the text 'Turing was one of the people who worked on the first computers. He created the theoretical Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.' with the following edit operation and the scope of the edit and replicate the remaining text as is: replace the 2nd sentence with the third one (NB: keep the 3rd sentence, do not remove it)\n",
      "Edited:\n",
      "Turing was one of the people who worked on the first computers. The machine was imaginary, but it included the idea of a computer program. The machine was imaginary, but it included the idea of a computer program.\n",
      "----\n",
      "Edit the text 'Turing was one of the people who worked on the first computers. He created the theoretical Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.' with the following edit operation and the scope of the edit and replicate the remaining text as is: add 'b' after each 'a' in the 1st and last sentence\n",
      "Edited:\n",
      "Turing wabs one of the people who worked on the first computers. He created the theoretical Turing machine in 1936. The mabchine wabs imabginabry, but it included the ideab of ab computer prograbm.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 3)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7424855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/sentence_ambiguity/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119c6ee617ad41f595af29ee94095e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'sentence_ambiguity', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5449a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: LeBron James is universally considered the best football player of all time.\n",
      "True or False?\n",
      "False\n",
      "----\n",
      "Claim: The cornea of your eye is likely where the light receptors are.\n",
      "True or False?\n",
      "False\n",
      "----\n",
      "Claim: Few animals live more than 200 years, but none of these have fins.\n",
      "True or False?\n",
      "False\n",
      "----\n",
      "Claim: The United States has never had a unanimously elected president.\n",
      "True or False?\n",
      "False\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 4)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "866003ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/salient_translation_error_detection/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9eb6590e874eccad1e7537d0f1928c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'salient_translation_error_detection', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "48f3e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following translations from German to English contain a particular error. That error will be one of the following types:\n",
      "Named Entities: An entity (names, places, locations, etc.) is changed to a different entity.\n",
      "Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed.\n",
      "Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed.\n",
      "Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms.\n",
      "Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations.\n",
      "Dropped Content: A significant clause in the translation is removed.\n",
      "Please identify that error.\n",
      "\n",
      "Source: Die Architektur Osttimors spiegelt die Traditionen der verschiedenen Ethnien des Landes und die Zeiten der Fremdherrschaft wider.\n",
      "Translation: The architecture of West Timor reflects the traditions of the different ethnic groups of the country and the times of foreign domination.\n",
      "The translation contains an error pertaining to\n",
      "Named Entities\n",
      "----\n",
      "The following translations from German to English contain a particular error. That error will be one of the following types:\n",
      "Named Entities: An entity (names, places, locations, etc.) is changed to a different entity.\n",
      "Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed.\n",
      "Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed.\n",
      "Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms.\n",
      "Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations.\n",
      "Dropped Content: A significant clause in the translation is removed.\n",
      "Please identify that error.\n",
      "\n",
      "Source: Der Grevesmühlener FC ist ein deutscher Fußballclub aus Grevesmühlen in Mecklenburg-Vorpommern.\n",
      "Translation: Grevesmühlener FC is a German football club based in Grevesmühlen.\n",
      "The translation contains an error pertaining to\n",
      "Named Entities\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e4fdf315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/riddle_sense/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be15991336714ce0bf6a74feaf4cc76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'riddle_sense', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d9c86bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I went to the city, stayed there, never went back, and then returned. What exactly is it?\n",
      "  choice: change\n",
      "  choice: watch\n",
      "  choice: eye\n",
      "  choice: approach\n",
      "  choice: go somewhere\n",
      "A:\n",
      "watch\n",
      "----\n",
      "Q: You step on us every day; we can't exist without each other, and I'll never be able to support you.\n",
      "  choice: extensions\n",
      "  choice: shadow\n",
      "  choice: light\n",
      "  choice: area\n",
      "  choice: obstacle\n",
      "A:\n",
      "shadow\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "992e2dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/repeat_copy_logic/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 234.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'repeat_copy_logic', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5265bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat with logic:\n",
      "\n",
      "Q: say pickup a pound of green beans twice, replacing a pound with a bunch for even times and a handful for odd\n",
      "A:\n",
      "pickup a handful of green beans pickup a bunch of green beans\n",
      "----\n",
      "repeat with logic:\n",
      "\n",
      "Q: repeat a woodchuck chucks lots of wood two times, but replace lots with five pounds the first time and two tons the second time\n",
      "A:\n",
      "a woodchuck chucks five pounds of wood a woodchuck chucks two tons of wood\n",
      "----\n",
      "repeat with logic:\n",
      "\n",
      "Q: Repeat squiggly line twice after the phrase can you draw\n",
      "A:\n",
      "can you draw squiggly line squiggly line\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 3)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "08f7f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/reasoning_about_colored_objects/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ebc08d8be24770b8208079afc5a867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'reasoning_about_colored_objects', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "796b725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: On the desk, you see a bunch of things arranged in a row: a red textbook, a yellow pencil, a brown puzzle, a silver stress ball, a teal bracelet, and a pink booklet. What is the color of the thing directly to the right of the teal thing?\n",
      "A:\n",
      "pink\n",
      "----\n",
      "Q: On the floor, you see a pink jug, a magenta mug, and a teal scrunchiephone charger. Is the jug pink?\n",
      "A:\n",
      "yes\n",
      "----\n",
      "Q: On the table, you see the following items arranged in a row: a purple pair of sunglasses, a black teddy bear, an orange fidget spinner, a teal scrunchiephone charger, a mauve paperclip, and a fuchsia sheet of paper. What is the color of the right-most item?\n",
      "A:\n",
      "fuchsia\n",
      "----\n",
      "Q: On the desk, I see two brown mugs, one brown sheet of paper, one fuchsia sheet of paper, one brown pen, three grey mugs, one grey pen, two fuchsia paperclips, one fuchsia mug, and three grey sheets of paper. If I remove all the grey items from the desk, how many mugs remain on it?\n",
      "A:\n",
      "3\n",
      "----\n",
      "Q: On the table, there is a red dog leash, a brown teddy bear, a silver pencil, and a teal paperclip. What color is the paperclip?\n",
      "A:\n",
      "teal\n",
      "----\n",
      "Q: On the floor, you see a bunch of items arranged in a row: a teal keychain, a turquoise scrunchiephone charger, a blue bracelet, and a pink fidget spinner. What is the color of the item furthest from the bracelet?\n",
      "A:\n",
      "teal\n",
      "----\n",
      "Q: On the floor, there is one yellow textbook, one teal necklace, three yellow puzzles, three teal puzzles, two purple textbooks, two magenta pencils, one yellow pencil, two yellow necklaces, and one purple necklace. If I remove all the magenta things from the floor, how many puzzles remain on it?\n",
      "A:\n",
      "6\n",
      "----\n",
      "Q: On the desk, you see several things arranged in a row: a blue textbook, a red jug, a green mug, and a grey keychain. What is the color of the thing directly to the left of the grey thing?\n",
      "A:\n",
      "green\n",
      "----\n",
      "Q: On the nightstand, you see one gold plate, three orange puzzles, three turquoise pairs of sunglasses, three orange pairs of sunglasses, two gold pairs of sunglasses, two gold dog leashes, two orange dog leashes, one silver dog leash, one turquoise dog leash, one orange plate, and one turquoise plate. If I remove all the plates from the nightstand, how many gold objects remain on it?\n",
      "A:\n",
      "4\n",
      "----\n",
      "Q: On the floor, there are two silver dog leashes, two red dog leashes, one silver bracelet, one silver keychain, three pink dog leashes, three red bracelets, one red cat toy, and one red keychain. If I remove all the silver objects from the floor, how many cat toys remain on it?\n",
      "A:\n",
      "1\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d2fbcb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/real_or_fake_text/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38d618185f9490282b4ef29482d4f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'real_or_fake_text', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "97d8eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be presented with a document containing 10 sentences. The document\n",
      "will begin with a sentence written by a human. As some point within the \n",
      "document, the sentences will transition from being written by a human writer to \n",
      "being generated by a computer. Your goal is to guess this boundary by writing\n",
      "down the first sentence that you think is machine generated. \n",
      "DOCUMENT:\n",
      "I had been trekking from one end of the island to the other for almost four hours before I found it. While walking around due to sheer desperation, I stumbled over a rock and, in my frustration, kicked it off the side of the path. It tumbled down the small grassy slope lining the edge of the trail and came to rest on a previously unnoticed, well-kept path. I was a bit bored anyways, so I decided to explore the trail, sitting down and scooting to it. After sliding to the bottom and kicking up some dust in the process, I jumped up and dusted off my pants, proceeding along the trail. The trail went for quite a while, and I couldn't help but notice again how well kept it was. All of the sand-like gravel was spread across the ground in a way that couldn't have been natural... As I continued along the trail, I found a stray sock lying by the edge of the path, seemingly buried in the organized gravel.  \"Hmmm... Is there someone else here? \"Excitedly, I sped up to a brisk jog and continued following the path. Posted by Andy at 10:42 AM\n",
      "  choice: The trail went for quite a while, and I couldn't help but notice again how well kept it was.\n",
      "  choice: \"Hmmm... Is there someone else here?\n",
      "  choice: I was a bit bored anyways, so I decided to explore the trail, sitting down and scooting to it.\n",
      "  choice: \"Excitedly, I sped up to a brisk jog and continued following the path.\n",
      "  choice: Posted by Andy at 10:42 AM\n",
      "  choice: After sliding to the bottom and kicking up some dust in the process, I jumped up and dusted off my pants, proceeding along the trail.\n",
      "  choice: While walking around due to sheer desperation, I stumbled over a rock and, in my frustration, kicked it off the side of the path.\n",
      "  choice: I had been trekking from one end of the island to the other for almost four hours before I found it.\n",
      "  choice: All of the sand-like gravel was spread across the ground in a way that couldn't have been natural... As I continued along the trail, I found a stray sock lying by the edge of the path, seemingly buried in the organized gravel. \n",
      "  choice: It tumbled down the small grassy slope lining the edge of the trail and came to rest on a previously unnoticed, well-kept path.\n",
      "FIRST COMPUTER-GENERATED SENTENCE:\n",
      "Posted by Andy at 10:42 AM\n",
      "----\n",
      "You will be presented with a document containing 10 sentences. The document\n",
      "will begin with a sentence written by a human. As some point within the \n",
      "document, the sentences will transition from being written by a human writer to \n",
      "being generated by a computer. Your goal is to guess this boundary by writing\n",
      "down the first sentence that you think is machine generated. \n",
      "DOCUMENT:\n",
      "HOW TO MAKE: Szechuan Pork With Bok Choy\n",
      "Ingredients:\n",
      "12 teaspoon dried chili pepper flakes\n",
      "3 tablespoons dark soy sauce\n",
      "1 tablespoon toasted sesame oil\n",
      "1 teaspoon white wine vinegar\n",
      "14 ounces pork fillets\n",
      "1 teaspoon sunflower oil\n",
      "8 ounces bok choy, each cut in half lengthways\n",
      "1 garlic clove, crushed. Preheat the oven to 400F Mix together the garlic, chili flakes, soy sauce, sesame oil and vinegar. Roast for 30 minutes. Remove from the oven and set aside. Heat the remaining 2 teaspoons of sesame oil in a large nonstick skillet over medium heat. Add the pork. Cook until browned on all sides. Set aside. While the pork is cooking, make the bok choy. In a small bowl combine the water, sugar, rice vinegar, sesame seeds, ginger, star anise, and salt.\n",
      "  choice: Set aside.\n",
      "  choice: Cook until browned on all sides.\n",
      "  choice: Preheat the oven to 400F Mix together the garlic, chili flakes, soy sauce, sesame oil and vinegar.\n",
      "  choice: Add the pork.\n",
      "  choice: Roast for 30 minutes.\n",
      "  choice: HOW TO MAKE: Szechuan Pork With Bok Choy\n",
      "Ingredients:\n",
      "12 teaspoon dried chili pepper flakes\n",
      "3 tablespoons dark soy sauce\n",
      "1 tablespoon toasted sesame oil\n",
      "1 teaspoon white wine vinegar\n",
      "14 ounces pork fillets\n",
      "1 teaspoon sunflower oil\n",
      "8 ounces bok choy, each cut in half lengthways\n",
      "1 garlic clove, crushed.\n",
      "  choice: While the pork is cooking, make the bok choy.\n",
      "  choice: In a small bowl combine the water, sugar, rice vinegar, sesame seeds, ginger, star anise, and salt.\n",
      "  choice: Heat the remaining 2 teaspoons of sesame oil in a large nonstick skillet over medium heat.\n",
      "  choice: Remove from the oven and set aside.\n",
      "FIRST COMPUTER-GENERATED SENTENCE:\n",
      "Roast for 30 minutes.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e4480f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/question_selection/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fde3e264294766afdcc61cf52cec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'question_selection', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "95a1b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a short answer along with its context, select the most appropriate question which has the given short answer as its answer.\n",
      "\n",
      "Here is the short answer followed by the context.\n",
      "Short Answer: one of his wife's ladies-in-waiting Context: From December 1539, Luther became implicated in the bigamy of Philip I, Landgrave of Hesse, who wanted to marry one of his wife's ladies-in-waiting. Philip solicited the approval of Luther, Melanchthon, and Bucer, citing as a precedent the polygamy of the patriarchs. The theologians were not prepared to make a general ruling, and they reluctantly advised the landgrave that if he was determined, he should marry secretly and keep quiet about the matter. As a result, on 4 March 1540, Philip married a second wife, Margarethe von der Saale, with Melanchthon and Bucer among the witnesses. However, Philip was unable to keep the marriage secret, and he threatened to make Luther's advice public. Luther told him to \"tell a good, strong lie\" and deny the marriage completely, which Philip did during the subsequent public controversy. In the view of Luther's biographer Martin Brecht, \"giving confessional advice for Philip of Hesse was one of the worst mistakes Luther made, and, next to the landgrave himself, who was directly responsible for it, history chiefly holds Luther accountable\". Brecht argues that Luther's mistake was not that he gave private pastoral advice, but that he miscalculated the political implications. The affair caused lasting damage to Luther's reputation.\n",
      "  choice: Who do historians hold responsible for philip i's mistake and luther's advice in the matter?\n",
      "  choice: Philip, who did i want to marry?\n",
      "Choose the appropriate question which has the given answer.\n",
      "Philip, who did i want to marry?\n",
      "----\n",
      "Given a short answer along with its context, select the most appropriate question which has the given short answer as its answer.\n",
      "\n",
      "Here is the short answer followed by the context.\n",
      "Short Answer: nationalist differences Context: Iqbal expressed fears that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence. In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences. Sir Muhammad Iqbal was elected president of the Muslim League in 1930 at its session in Allahabad as well as for the session in Lahore in 1932. In his Allahabad Address on 29 December 1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India. This address later inspired the Pakistan movement.\n",
      "  choice: When did iqbal become the muslim league's president?\n",
      "  choice: What did iqbal advocate for when he advocated proposals of greater islamic political unity?\n",
      "Choose the appropriate question which has the given answer.\n",
      "What did iqbal advocate for when he advocated proposals of greater islamic political unity?\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cb31a408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/presuppositions_as_nli/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8321ff2f3ff64bdb9a087ec99cf7be1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'presuppositions_as_nli', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "99640a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a natural language inference task. There are two sentences in English. The answer is \"entailment\" if the first sentence entails the second, \"contradiction\" if the second sentence contradicts the first, and \"neutral\" if neither is of those two cases holds.\n",
      "\n",
      "\n",
      "Sentence 1:  Muslims and Croats endorsed independence by 99.4 percent, while the Serbs boycotted a vote their leaders said was illegal. Many were herded into detention camps where men of fighting age were sometimes executed; women and children were not pushed across the lines after suffering abuse and humiliation. \n",
      "Sentence 2: Women and children suffered abuse and humiliation.\n",
      "\n",
      "The answer is:\n",
      "entailment\n",
      "----\n",
      "This is a natural language inference task. There are two sentences in English. The answer is \"entailment\" if the first sentence entails the second, \"contradiction\" if the second sentence contradicts the first, and \"neutral\" if neither is of those two cases holds.\n",
      "\n",
      "\n",
      "Sentence 1: That's one of the big things I've been working on. \" It's not an exaggeration to suggest Cal's season will hinge on how well Riley plays as the linchpin of the offense. Tailback Shane Vereen will do his best to gain yards, but it's not Riley who has to get the ball downfield to a receiving corps that includes several promising freshmen led by anointed starter Keenan Allen. \n",
      "Sentence 2: Someone has to get the ball downfield\n",
      "\n",
      "The answer is:\n",
      "entailment\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21035ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/physics_questions/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67d8d332494419a0a40f063e9934c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset('bigbench', 'physics_questions', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5e3d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Lamar Gant, U.S. powerlifting star, became the first man to deadlift five times his own body weight in 1985. Deadlifting involves raising a loaded barbell from the floor to a position above the head with outstretched arms. Determine the work done by Lamar in deadlifting 260 kg to a height of 0.85 m above the ground.\n",
      "A:\n",
      "2165.8 J\n",
      "----\n",
      "Q: In the Funny Car competition at the Joliet Speedway in Joliet, Illinois in October of 2004, John Force complete the 1/4-mile dragster race in a record time of 4.437 seconds. Determine the average speed of the dragster in km/hr.\n",
      "A:\n",
      "326.4 km/hr\n",
      "----\n",
      "Q: A bicycle has a momentum of 24 kg*m/s. What momentum would the bicycle have if it had one-half the mass and was moving with thrice the speed?\n",
      "A:\n",
      "72 kg*m/s\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 3)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d84c618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/physical_intuition/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 923.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'physical_intuition', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb40d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/physics/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed03612920e46f0aa6c59effd6b51ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'physics', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c28afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the physics formula that would be most useful for finding the answer to each of the following word problems.\n",
      "\n",
      "\n",
      "Q: In an experiment, a positively charged oil droplet weighing 6.5 * 10 ^ -15 N is held stationary by a vertical electric field. If the electric field strength is 5.3 * 10 ^ 3 N/C, what is the charge on the oil droplet?\n",
      "  choice: dt = dx / v\n",
      "  choice: v = λ * f\n",
      "  choice: F = q * E\n",
      "  choice: v = v_0 + a * t\n",
      "A:\n",
      "F = q * E\n",
      "----\n",
      "Identify the physics formula that would be most useful for finding the answer to each of the following word problems.\n",
      "\n",
      "\n",
      "Q: A 3.0 kg ball rolls down from the top of a ramp as shown. If the ball is moving at 10.0 m/sat the bottom, how much energy was lost due to friction (thermal energy)?\n",
      "  choice: Q = m * c * dT\n",
      "  choice: ɸ = E * A * cos(θ)\n",
      "  choice: E = F / q\n",
      "  choice: a = dv / dt\n",
      "A:\n",
      "Q = m * c * dT\n",
      "----\n",
      "Identify the physics formula that would be most useful for finding the answer to each of the following word problems.\n",
      "\n",
      "\n",
      "Q: A 1200 kg car rounds a flat circular section of road at 20 m/s as shown in the diagram. The coefficient of friction between the car tires and the road surface is 0.65. What minimum friction force is required for the car to follow this curve?\n",
      "  choice: F = m * a\n",
      "  choice: dq = ρ * dV\n",
      "  choice: E = q * σ / (2 * ε)\n",
      "  choice: P = dE / dt\n",
      "A:\n",
      "F = m * a\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 3)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80850d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/penguins_in_a_table/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54784db0bedc46f98398fdfa5c2772aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'penguins_in_a_table', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdc74bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the physics formula that would be most useful for finding the answer to each of the following word problems.\n",
      "\n",
      "\n",
      "Q: What is the electric potential at point P due to the two fixed charges as shown?\n",
      "  choice: K = 1/2 * m * v ^ 2\n",
      "  choice: p = m * v\n",
      "  choice: E = F / q\n",
      "  choice: V = k * q / r\n",
      "A:\n",
      "V = k * q / r\n",
      "----\n",
      "Identify the physics formula that would be most useful for finding the answer to each of the following word problems.\n",
      "\n",
      "\n",
      "Q: A 1200 kg car rounds a flat circular section of road at 20 m/s as shown in the diagram. The coefficient of friction between the car tires and the road surface is 0.65. What minimum friction force is required for the car to follow this curve?\n",
      "  choice: F = m * a\n",
      "  choice: dq = ρ * dV\n",
      "  choice: E = q * σ / (2 * ε)\n",
      "  choice: P = dE / dt\n",
      "A:\n",
      "F = m * a\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211b8771",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# physics too\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigbench\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperators\u001b[39m\u001b[38;5;124m'\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# inputs = [x.split('\\n')[0] for x in inputs]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'operators', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "34f5ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the definition of the op operator, compute the result.\n",
      "i op j subtracts j from i.\n",
      "7 op 3 =\n",
      "4\n",
      "----\n",
      "Given the definition of the op operator, compute the result.\n",
      "i op j subtracts j from i.\n",
      "176 op 23 =\n",
      "153\n",
      "----\n",
      "Given the definition of the op operator, compute the result.\n",
      "i op j keeps the j first digits of i.\n",
      "125690 op 3 =\n",
      "125\n",
      "----\n",
      "Given the definition of the op operator, compute the result.\n",
      "i op j suppresses the j first digits of i.\n",
      "125690 op 2 =\n",
      "5690\n",
      "----\n",
      "Given the definition of the op operator, compute the result.\n",
      "op n1 n2 ... nn selects the smallest from the n listed numbers.\n",
      "op 16 0 10 6 =\n",
      "0\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "eade5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/odd_one_out/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a49ddf6c894ec0af084347baab906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'odd_one_out', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d2cc80cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Pick the odd word out: white, black, red, apple\n",
      "A:\n",
      "apple\n",
      "----\n",
      "Q: Pick the odd word out: bath, sink, fridge, oven, stove\n",
      "A:\n",
      "bath\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5ed750a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/object_counting/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e4d6a376f4b939c3aa6b09ad2b53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'object_counting', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "add87038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I have a trombone, a flute, an accordion, three violins, four clarinets, a drum, a trumpet, and a piano. How many musical instruments do I have?\n",
      "A:\n",
      "thirteen\n",
      "----\n",
      "Q: I have a mouse, a goat, and two fish. How many animals do I have?\n",
      "A:\n",
      "four\n",
      "----\n",
      "Q: I have a strawberry, a grape, an apple, an orange, a plum, two nectarines, two bananas, and a peach. How many fruits do I have?\n",
      "A:\n",
      "ten\n",
      "----\n",
      "Q: I have a violin, two nectarines, a flute, a banana, an accordion, two strawberries, a clarinet, a drum, a trumpet, a piano, and a trombone. How many musical instruments do I have?\n",
      "A:\n",
      "eight\n",
      "----\n",
      "Q: I have three bananas, a trumpet, a clarinet, a piano, a nectarine, a trombone, a blackberry, and a violin. How many musical instruments do I have?\n",
      "A:\n",
      "five\n",
      "----\n",
      "Q: I have five stoves, a fridge, a lamp, a microwave, a table, and an oven. How many objects do I have?\n",
      "A:\n",
      "ten\n",
      "----\n",
      "Q: I have a cabbage, a stalk of celery, a carrot, a lettuce head, a yam, three heads of broccoli, three potatoes, and two cauliflowers. How many vegetables do I have?\n",
      "A:\n",
      "thirteen\n",
      "----\n",
      "Q: I have a donkey, a cat, two bears, a pig, and a goat. How many animals do I have?\n",
      "A:\n",
      "six\n",
      "----\n",
      "Q: I have a frog, a duck, a bear, two donkeys, a fish, a goat, and a rabbit. How many animals do I have?\n",
      "A:\n",
      "eight\n",
      "----\n",
      "Q: I have a duck, a donkey, a cow, a bear, a dog, and a mouse. How many animals do I have?\n",
      "A:\n",
      "six\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a389f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/novel_concepts/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1137.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'novel_concepts', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8bd6f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) rooks in chess 2) the Tower of London\n",
      "A:\n",
      "They are both castles.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) bumble bees 2) 01010101 3) race cars\n",
      "A:\n",
      "They all have stripes.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) coconut 2) paint 3) motor\n",
      "A:\n",
      "They all are related to oil.\n",
      "----\n",
      "Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\n",
      "What do the following have in common? 1) bumble bees 2) cell phones 3) exciting news\n",
      "A:\n",
      "They all can make a buzz.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 4)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b6425be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/nonsense_words_grammar/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3826afa76043988a0a7d9ba688dc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'nonsense_words_grammar', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e841c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Does the following sentence describe events which take place in the past, present, or future? The widdtrance holofolors will filfeyatrim quilliancerously.\n",
      "A:\n",
      "Future\n",
      "----\n",
      "Q: What part of speech is the word \"tirraghat\" in the following sentence? Some tirraghat is always voorhosem.\n",
      "A:\n",
      "Noun\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5376a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/navigate/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 271.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'navigate', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0621615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Always face forward. Take 5 steps left. Take 4 steps left. Take 1 step left. Take 2 steps left. Take 2 steps right. Take 3 steps left. Take 5 steps forward. Take 3 steps backward.\n",
      "A:\n",
      "False\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Always face forward. Take 3 steps forward. Take 7 steps right. Take 7 steps forward. Take 4 steps right.\n",
      "A:\n",
      "False\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Always face forward. Take 8 steps right. Take 10 steps right. Take 3 steps left. Take 3 steps right. Take 1 step left. Take 4 steps right.\n",
      "A:\n",
      "False\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Turn left. Take 4 steps. Take 4 steps. Take 9 steps. Take 4 steps. Turn around. Take 10 steps. Take 8 steps. Take 3 steps.\n",
      "A:\n",
      "True\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Take 2 steps. Take 6 steps. Take 4 steps. Turn around. Take 10 steps. Take 2 steps.\n",
      "A:\n",
      "True\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Turn right. Take 3 steps. Take 10 steps. Turn around. Take 7 steps. Take 4 steps. Take 2 steps.\n",
      "A:\n",
      "True\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Take 7 steps. Take 8 steps. Take 3 steps. Take 2 steps. Turn left.\n",
      "A:\n",
      "False\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Always face forward. Take 7 steps right. Take 1 step left.\n",
      "A:\n",
      "False\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Turn around. Take 2 steps. Turn right. Turn right. Take 2 steps.\n",
      "A:\n",
      "True\n",
      "----\n",
      "If you follow these instructions, do you return to the starting point?\n",
      "Q: Always face forward. Take 5 steps backward. Take 6 steps right. Take 8 steps backward. Take 8 steps forward. Take 7 steps forward. Take 6 steps left. Take 2 steps left. Take 7 steps left. Take 1 step right.\n",
      "A:\n",
      "False\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0d3d43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/modified_arithmetic/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0874893bf8cb4f5c8752e388d196db90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'modified_arithmetic', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "47fce4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following lines, the symbol -> represents a simple mathematical operation.\n",
      "151 + 719 -> 870\n",
      "48 + 153 -> 201\n",
      "858 + 292 -> 1150\n",
      "579 + 49 -> 628\n",
      "663 + 530 -> 1193\n",
      "611 + 59 ->\n",
      "670\n",
      "----\n",
      "In the following lines, the symbol -> represents a simple mathematical operation.\n",
      "37 * 58 -> 2147\n",
      "8 * 72 -> 577\n",
      "71 * 50 -> 3551\n",
      "34 * 81 -> 2755\n",
      "4 * 13 -> 53\n",
      "91 * 29 ->\n",
      "2640\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bcacbfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/minute_mysteries_qa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6beb77a453f4d31bf45da129e45f0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'minute_mysteries_qa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "43e2f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: \"Eddie's missing.\" \"How is that even possible? He weighs 240 pounds!\" The classroom laughed at Reggie's joke, but I was worried. Eddie Cavanaugh was the high school's star quarterback. Each week his expert passing led the football team to victory, and no other school in the state had ever won this many games in a row. \"People say jocks are stupid, but Eddie isn't,\" sighed our history teacher. \"He was making our little town famous, and now he's gone.\" Eddie's mother had called the principal's office about one o’clock when she noticed Eddie was missing, and soon everyone in the school was talking about the news. He'd come to school that morning, but then somehow, he'd vanished! As the last school bell rang, I asked Reggie if he wanted to search for Eddie. \"Sure, why not?\" Reggie laughed. \"If we find our missing quarterback, then WE can be the school's heroes.\" But where could we find some clues? The locker room was a good place to start. As we approached, we heard the team shouting enthusiastically \"One! Two! Three! Go!\" as they shuffled out onto the field. Tomorrow they would play an away game in Capitol City - a two-hour drive - so they were getting some extra practice today. We smelled grass and sweaty uniforms, but then we spotted Mr. Roster, the assistant coach, talking on his cellphone. \"Eddie's mother just called me,\" he was saying with exasperation. \"Look, we're doing everything we can.\" His brow furrowed in anger. \"No, we're not canceling the game...\" We waited until the call was done, then asked about Eddie's disappearance. \"You don't realize how important someone is, until they're not around!\" Mr. Roster sighed. \"The team knows it, too. Eddie always led an inspiring cheer after school before each practice. We miss him already.\" Can we search his locker? Reggie asked. The coach looked at us suspiciously, but realized we were trying to help. He led us into his office, where he retrieved a clipboard with all the lockers' combinations. Inside Eddie's we saw his jersey - number 17 - dangling from a hanger. There was nothing unusual in the locker. There were his cleats, his helmet, shoulder pads, kneepads and a sack lunch. But on the door of the locker, we found Eddie's secret motivational tool. He'd drawn a graph for himself, showing various records for high school football. First it showed the school's record for consecutive wins and then the state's and then the record for the entire country. As Coach Roster ran after two players who were arguing, I motioned to Reggie. Coach Roster's office door was open, and we snuck in to search for clues. Reggie opened a filing cabinet and saw manila folders in alphabetical order. He quickly looked for the C's, while I started searching Coach Roster's desk. Its top was filled by a giant desk calendar. I checked yesterday's date, which just said \"Practice,\" like every other day this week. But today's date had an unusual entry, \"Visit Dr. Lazlow, 9 to 11a.m. Capitol City.\" Reggie retrieved a file labeled \"Cavanaugh, Eddie.\" There was a handwritten note. \"Eddie, stop playing, or you'll get hurt, Mae\" it warned. \"What are you doing!\" shouted a voice behind us. Coach Roster had returned and caught Reggie and me by surprise. \"We were looking for clues about Eddie,\" I answered nervously. \"Well, you might as well know,\" sighed Coach Roster wearily. \"That note turned up on the locker room door yesterday. I should've taken it more seriously - but who the heck is 'Mae'?\" Reggie had an idea. \"Eddie's girlfriend is named Marissa! I bet Mae is her nickname.\" Now we had a trail to follow - but where was Marissa? If she was missing too, it might explain Eddie's disappearance. Marissa wasn't a cheerleader, just a hard-working student from a very poor family. Reggie found her family's listing in the phone book, and soon we were traveling to her address. It was a small apartment in a large complex. Marissa's father answered the door, then called out \"Marissa, are you here?\" \"Go away!\" shouted a voice inside. \"Marissa, get out here!\" her father yelled. We waited anxiously, until a bedroom door opened, and we saw our unhappy classmate. Her hair was messy, and her make-up was smeared. It was obvious that she'd been crying. \"What do you want?\" she asked us. We're looking for Eddie, I told her. \"So's half the town,\" she said bitterly. \"Everyone loved Eddie.\" We agreed and couldn't help talking about his popularity. \"It's so unfair,\" Marissa complained. \"Eddie was always so alone. He didn't have a father or even a brother or sister. There is just his mother. And yet this town became his family. He'd made everyone so proud.\" Reggie and I nodded, but Marissa continued, her thoughts far away. \"Eddie said that more than anything he just wanted to get engaged to me some day and settle down as a hero in this very same town. But now he's gone.\" \"Maybe he'll send a message to you, Mae,\" Reggie said, nonchalantly. \"Who's Mae?\" she asked, with irritation. We were running out of clues, but decided to visit Eddie's mother. Her house was a few blocks from Marissa's, and we hoped she wasn't upset when we rang her doorbell. Mrs. Cavanaugh was glad to see friends of Eddie's and quickly invited us in. \"I always worried Eddie would hurt himself,\" Mrs. Cavanaugh said. \"All the phone calls I've gotten show what Eddie meant to this town. But I just want my darling boy back.\" Her wall was filled with pictures of Eddie, some in his football uniform, others showing him as a young boy. I asked her when she'd first noticed Eddie was missing. \"I always make him a healthy lunch,\" Mrs. Cavanaugh sighed. \"I leave it on the kitchen counter, since I have to leave for work ten minutes before Eddie leaves for school.\" \"But this afternoon when I came home from work - his sack lunch was still there!\" Mrs. Cavanaugh said with exasperation. \"That's when I realized that Eddie was missing.\" \"We're looking for him,\" Reggie assured her. \"I'm glad to know his friends care about him,\" she smiled. She talked about Marissa and her hopes for her son. \"Come back and visit any time.\" On the sidewalk, Reggie was upset. \"I don't know who could've done this,\" he complained. \"Everyone loved Eddie. In fact, maybe Eddie faked the whole thing himself,\" Reggie said, more serious than I'd ever seen him. \"Being the star quarterback wasn't enough for him. He wanted everyone to worry about him too.\" \"I'm not worried,\" I said, \"because I know where Eddie is. And if you study the clues, you'll know too. Who is responsible for Eddie's disappearance?\n",
      "  choice: Eddie\n",
      "  choice: Marissa\n",
      "  choice: Coach Roster\n",
      "  choice: Eddie's Mom\n",
      "Answer:\n",
      "Eddie's Mom\n",
      "----\n",
      "Story: Margie found herself sitting on a couch all alone in her darkened living room. She knew that she was a snooze hound who had a habit of falling asleep at the drop of the hat. Still, falling asleep in the middle of her own baby shower was probably a little over the top. Since she was alone, maybe the baby shower was just a dream. She really hoped that was the case. Finally, realizing that it was her phone ringing that woke her up, Margie reached over and answered. “Hey sleepyhead.” It was her best friend Diane. “We got tired of waiting for you to wake up after you fell asleep. When you did not wake up fairly quickly, almost everyone just left. Some of us decided to stay and clean up. We had hoped you would wake up and open the present that the four of us got you. However, we finally got tired of waiting and moved the party to our favorite hangout, Yours Truly. We have your present here, so hurry down and meet us.” With that, Diane hung up. Realizing that the baby shower was not a dream, Margie knew that she needed to get to Yours Truly as quickly as possible. After all, her best friends deserved a big thank you for cleaning up as well as an apology for falling asleep at her own baby shower. So, she hopped off of the couch (okay, she swung her legs back and forth several times to get enough momentum to rise from the couch since she was almost nine months pregnant) and reached for her coat. Fortunately, she already had her sandals on since she never took them off these days. She could not remember what her feet even looked like these days. That was the joke between them. All of her friends thought it was funny that after years of teasing her about her big feet, she could no longer see them. That was why her husband helped her put on sandals first thing in the morning each day and she never took them off until bedtime. Walking out of the house, she passed through the kitchen on the way to the garage. For some reason, the food coloring was sitting on the kitchen counter. She did not remember using it for the shower, so she had no idea why it was out. As she got in the car, she noticed that several bags of trash had been placed in her garbage can. On top of the last bag was a pair of latex gloves dyed green. How strange. As she drove to their favorite restaurant, Yours Truly, she realized how important her friends were to her. For most of her life, they had all been there for each other acting more like sisters than just friends. Yours Truly was where they always gathered. After all, it was a great place to meet. It always felt like you were eating at your friend’s house when you ate there. It is just a warm, happy place with good food and great people. Best of all, except for Maureen who lived half an hour away, they all lived within 5 minutes of it. Parking nearby, Margie shuffled towards the restaurant. As she approached the restaurant, a young boy held the door open for her and said “Green”. She had no idea what he meant. Two other patrons were in front of her. They both turned, looked at her and said “Green” as well. Margie was going to ask them what they meant when the waitress seated them. She saw her friends in the back and headed to meet them. Several more people she passed all said the same thing: “Green”. Sitting down with her friends, she quickly thanked them for cleaning up after the shower and apologized for falling asleep. They all said that it was no big deal and besides, it was going to make for a great story. Margie then commented that everyone was saying the word green to her and she had no idea why. Carm handed her a mirror. Written on her face in some type of red ink was the question “What color are my feet?” Margie realized that not only had one of her friends written on her face, but they had obviously dyed her feet green as well while she was sleeping. Laughing as Margie looked at her face in the mirror, Jen was kind enough to confirm for her that indeed, her feet were green. Jen told her that the guilty party had told them about it when they all arrived. She was the last to leave the house and no one else had known what she was going to do. After everyone had laughed about it, they had all agreed on a punishment. If Margie could figure out who did it, the guilty party would buy a $100 savings bond for the baby. It was now up to Margie to figure out who did it. They agreed that she could ask four questions, but she could not ask who did it. They promised to answer all questions truthfully. Prior to questioning everyone, Margie grabbed a napkin, wet it in the water glass and tried to get the writing off of her face. Unfortunately, it did not come off. Maureen handed her some fingernail polish remover and suggested she try that, but it did not work. Carm asked the waitress for some club soda, but that did not work either. Finally, Diane found an unopened bottle of rubbing alcohol in her purse that easily got the writing off of her face. Margie was grateful her face was no longer a billboard and told Diane, “Thank you. I guess I should ask if anyone has any suggestions on how to get the green off of my feet.” “That is your first question” Carm shouted. Diane replied, “Sorry, but as far as I know, there is no way to get food coloring off your skin. You are going to have green feet for a while. Hopefully, they will be back to normal by the time you can see them again.” Margie commented that it was not fair to charge her a question for that, but she knew her friends and knew it would do no good to complain further. To start the questioning, Margie asked, “Did the last person to arrive come straight to Yours Truly from my house?” Maureen replied, “No. I had to drive my Mom back to my house first, so I was the last to arrive by 20 minutes.” Margie had been examining each of her friends’ hands for signs that someone might have some green dye on them. Everyone’s hands were smooth and unblemished without a trace of green. Since green hands were not an issue, she moved on. Margie turned to Jen and asked, “Do you still have that allergy to latex?” Jen replied that as far as she knew, she was still allergic to latex and that Margie only had one question left and to make it a good one. “No problem” replied Margie. “I just want to know who bagged the garbage.” Carm said that she had gathered up all the trash from the baby shower, put it in several plastic bags and put it into the garbage can in the garage. Margie said, “Well, I know who did it.” Who did it?\n",
      "  choice: Diane\n",
      "  choice: Maureen\n",
      "  choice: Carm\n",
      "  choice: Jen\n",
      "Answer:\n",
      "Diane\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ee651dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/metaphor_boolean/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7321f0b4cb43ba8e47f8d3fb887093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'metaphor_boolean', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d26e2b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The essence of the task: Given a metaphoric sentence, identify if the second sentence is the correct paraphrase of the first.\n",
      "Q: David is a worm for what he did to Shelia <-->  David lives underground for what he did to Shelia\n",
      "  choice: True\n",
      "  choice: False\n",
      "A:\n",
      "False\n",
      "----\n",
      "The essence of the task: Given a metaphoric sentence, identify if the second sentence is the correct paraphrase of the first.\n",
      "Q: In the war on terror, American presidents must monitor not only the main terrorist organizations but their offshoots around the world as well <--> In the war on terror, American presidents must monitor not only the main terrorist organizations but any group developing from them as well \n",
      "  choice: True\n",
      "  choice: False\n",
      "A:\n",
      "True\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "efdcd266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/logical_sequence/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc64f41675947498d1cf77d5587a9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'logical_sequence', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e9662a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which of the following lists is correctly ordered chronologically?\n",
      " option: cook, serve, eat, digest\n",
      " option: serve, cook, digest, eat\n",
      " option: cook, eat, serve, digest\n",
      " option: eat, serve, cook, digest\n",
      "A:\n",
      "cook, serve, eat, digest\n",
      "----\n",
      "Q: Which of the following lists is correctly ordered sequentially?\n",
      " option: two thousand two hundred thirty-four, two, forty-four, twenty billion\n",
      " option: two, twenty billion, two thousand two hundred thirty-four, forty-four\n",
      " option: two thousand two hundred thirty-four, twenty billion, forty-four, two\n",
      " option: two, forty-four, two thousand two hundred thirty-four, twenty billion\n",
      "A:\n",
      "two, forty-four, two thousand two hundred thirty-four, twenty billion\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "87b1969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/logical_fallacy_detection/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7c4f7347b04e7b9df551c1cf569c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'logical_fallacy_detection', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "79af845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In an antique car show, there are five vehicles: a hatchback, a limousine, a sedan, a bus, and a tractor. The tractor is newer than the hatchback. The bus is the second-oldest. The hatchback is newer than the sedan. The limousine is the newest.\n",
      "The bus is the second-oldest.\n",
      "----\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\n",
      "Ada finished third-to-last.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f272d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/logical_deduction/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cea7bfde2b48fea24a7c90ae9b12dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'logical_deduction', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ab96220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a branch, there are five birds: a cardinal, a robin, a blue jay, a quail, and a raven. The robin is to the right of the raven. The cardinal is the leftmost. The raven is to the right of the blue jay. The blue jay is the third from the left.\n",
      "The blue jay is the third from the left.\n",
      "----\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "On a branch, there are five birds: a cardinal, a hawk, a hummingbird, a raven, and an owl. The raven is to the left of the hummingbird. The hawk is to the left of the owl. The cardinal is to the left of the hawk. The raven is the second from the right.\n",
      "The owl is the third from the left.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b3135195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/logical_args/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d230de4ca444a89675796dc3977346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'logical_args', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "544bebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer the following questions about the structure of logical arguments.\n",
      "\n",
      "Question: Recent evidence suggests that long periods of sitting can be harmful for one’s health.  This is partly based on a study which found that long periods of sitting are associated with higher levels of obesity and cardiovascular disease.  This argument rests on which of the following assumptions?\n",
      "  option: Obesity and cardiovascular disease are not caused by smoking.\n",
      "  option: Rates of sitting have risen dramatically in recent years.\n",
      "  option: Obesity and cardiovascular disease do not cause long periods of sitting.\n",
      "  option: A causal link exists between sitting and obesity.\n",
      "  option: Obesity rates have been rising over the past several years.\n",
      "Answer:\n",
      "Obesity and cardiovascular disease do not cause long periods of sitting.\n",
      "----\n",
      "Please answer the following questions about the structure of logical arguments.\n",
      "\n",
      "Question: A recent study showed that after a 12-week engineering course, students’ IQ scores increased significantly more than their peers who did not take the course.  Someone who wanted to use this information to conclude that the engineering course made students smarter would need to show which of the following?\n",
      "  option: IQ scores normally stay constant over time.\n",
      "  option: The students’ IQ increase could be explained in several other ways.\n",
      "  option: Professional engineers are smarter than the general population.\n",
      "  option: IQ scores are a good measure of intelligence.\n",
      "  option: Professional engineers have higher IQ scores than the general population.\n",
      "Answer:\n",
      "IQ scores are a good measure of intelligence.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d3b3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/linguistics_puzzles/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153666b5c5a349b0abcb18d1880a8426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'linguistics_puzzles', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ecf244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: tall crows that will slap the fast cats will like the fabulous frogs that will want excellent children Other: ficagiso fisiwivi fideta tuhiziwa pa finezalu fidojuro fitebe finezaso fitonuvi figihu tuhiqeti pa ficagiso fiyekuvi figire tuhiwoda\n",
      "English: the slow children touched the short frog Other: ficagilu fitonuro filobi gacagiso gayekuvi gaqega mohihabi\n",
      "English: the tall cat that fabulous crows will like slaps a excellent dolphin Other: finezalu fidojuro figire tuhiwoda pa gacagilu gasiwiro gatebe ganezaso gaqituvi gagihu dewiziwa\n",
      "English: the fast children want the excellent dolphin Other: ficagilu fitonuro fideta gacagiso gaqituvi gagihu dehiqeti\n",
      "English: tall crows touch the fast cats that the short children will like Other: finezalu fidojuro fitebe ficagilu fitonuro fiqega tuhiwoda pa ficagiso fisiwivi fideta dehihabi\n",
      "English: fast children that the tall frog wants touched fast crows Other:\n",
      "gacagilu gayekuro gatebe dewiqeti pa finezalu fitonuro fideta finezaso fidojuvi fideta mohihabi\n",
      "----\n",
      "Other: yupa lovomu yicomi lifico supe wiwemu qelimi ma powuji supe toyamu cadomi English: a fast frog hit the small crow that will touch the big bird\n",
      "Other: supe wiwemu fidami vimaji supe ruwapi yicowe English: the fabulous crow will dislike the fast dogs\n",
      "Other: supe wiwemu fidami gukeji yupa lovopi bijewe English: the fabulous crow will want slow frogs\n",
      "Other: yupa papepi qeliwe ma vimakugi supe lovomu bijemi pasukuji yupa wiwemu cadomi English: small dolphins that dislike the slow frog will remember a big crow\n",
      "Other: yupa papepi beriwe ma yupa toyamu yicomi capuji gukekuco supe lovopi fidawe English: excellent dolphins that a fast bird will slap wanted the fabulous frogs\n",
      "Other: supe papemu fidami pasuco yupa lovomu fidami English:\n",
      "the fabulous dolphin remembered a fabulous frog\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58efa4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/language_games/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 353.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# physics too\n",
    "d = datasets.load_dataset('bigbench', 'language_games', cache_dir=cache_dir)\n",
    "inputs = d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0049ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate Pig Latin into English.\n",
      " (Pig Latin) Apsnay ouryay ingersfay.\n",
      " (English)\n",
      "Snap your fingers.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate Pig Latin into English.\n",
      " (Pig Latin) Omtay akedbay anyay appleyay iepay orfay usyay.\n",
      " (English)\n",
      "Tom baked an apple pie for us.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate Pig Latin into English.\n",
      " (Pig Latin) Ithay ityay ithway ayay ickstay.\n",
      " (English)\n",
      "Hit it with a stick.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate Pig Latin into English.\n",
      " (Pig Latin) Amisay adhay efensiveday oundsway onyay ishay armsyay.\n",
      " (English)\n",
      "Sami had defensive wounds on his arms.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate Pig Latin into English.\n",
      " (Pig Latin) Iyay ishway otay aystay inyay Australiayay.\n",
      " (English)\n",
      "I wish to stay in Australia.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate English into Pig Latin.\n",
      " (English) Fast food is unhealthy.\n",
      " (Pig Latin)\n",
      "Astfay oodfay isyay unhealthyay.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate English into Pig Latin.\n",
      " (English) Tom was a crazy kid.\n",
      " (Pig Latin)\n",
      "Omtay asway ayay azycray idkay.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate English into Pig Latin.\n",
      " (English) Who is Tom talking about?\n",
      " (Pig Latin)\n",
      "Owhay isyay Omtay alkingtay aboutyay?\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate English into Pig Latin.\n",
      " (English) Mary unrolled her sleeping bag.\n",
      " (Pig Latin)\n",
      "Arymay unrolledyay erhay eepingslay agbay.\n",
      "----\n",
      "Pig Latin looks just like English, but if an English word starts with a vowel you add -yay at the end of the word (ice -> iceyay); if it starts with consonants, you move them to end of the word and add -ay (prank -> ankpray); if it is capitalized, the Pig Latin word is capitalized as well (Maggie -> Aggiemay). Translate English into Pig Latin.\n",
      " (English) Never before have I seen such strength.\n",
      " (Pig Latin)\n",
      "Evernay eforebay avehay Iyay eensay uchsay engthstray.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c93d7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/intent_recognition/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92391c1e3a384ff69d3cb8832c289a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'intent_recognition', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "847a17ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict the intent of the utterance. The possible choices for the intents are: add_to_playlist, book_restaurant, get_weather, play_music, search_screening_event, search_creative_work, and rate_book.\n",
      "\n",
      "Utterance: I rate secret water as a 4.\n",
      "Intent:\n",
      "rate_book\n",
      "----\n",
      "Predict the intent of the utterance. The possible choices for the intents are: add_to_playlist, book_restaurant, get_weather, play_music, search_screening_event, search_creative_work, and rate_book.\n",
      "\n",
      "Utterance: I need a weather forecast for São Tomé and Príncipe on December 8th, 2026.\n",
      "Intent:\n",
      "get_weather\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7beb5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/implicit_relations/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9e502ec9934b63b5b6fd164d4f6eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "d = datasets.load_dataset('bigbench', 'implicit_relations', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9ea59d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context and question: \"John,\" she said to her son with a smile, \"I shall go in to see Count Henry, my uncle. What is Count Henry to John?\n",
      "A:\n",
      "Granduncle\n",
      "----\n",
      "Context and question: Alisha praised God for bringing you back to her after so many trials, and charged her old woman to take care of you, and to conduct you hither as soon as possible. She has executed her commission perfectly well. What is the old woman to Alisha?\n",
      "A:\n",
      "Servant\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d7732513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/implicatures/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399c4b493344f3a963f0fb26efe1bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "d = datasets.load_dataset('bigbench', 'implicatures', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d6646104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Speaker 2's answer mean yes or no? \n",
      "\n",
      "Q: Speaker 1: 'You all buddies?' Speaker 2: 'Except for Skip and Nan. Them we just met.' \n",
      "A:\n",
      "no\n",
      "----\n",
      "Does Speaker 2's answer mean yes or no? \n",
      "\n",
      "Q: Speaker 1: 'Have I done something to offend you?' Speaker 2: 'I like all you Navy boys.' \n",
      "A:\n",
      "no\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "348cf453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/identify_odd_metaphor/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c68e357b29846f790373f8a80ba900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "d = datasets.load_dataset('bigbench', 'identify_odd_metaphor', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "42202db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which of the following sentences relating to life does not use metaphorical language that could also be applied to a container?\n",
      "  choice: His life contained a great deal of sorrow.\n",
      "  choice: Live life to the fullest.\n",
      "  choice: Here life is crammed with activities.\n",
      "  choice: Life in the mountains is tough.\n",
      "A:\n",
      "Life in the mountains is tough.\n",
      "----\n",
      "Q: Which of the following sentences relating to love does not use metaphorical language that could also be applied to a physical force?\n",
      "  choice: I was magnetically drawn to her.\n",
      "  choice: They gravitated to each other immediately.\n",
      "  choice: I could feel the electricity between us.\n",
      "  choice: Love was in the air.\n",
      "A:\n",
      "Love was in the air.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "80b49c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/hyperbaton/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ef4178c1a445c48b3c3227e156fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'hyperbaton', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0002f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which sentence has the correct adjective order: a \" plastic triangular Vietnamese drinking awful car \" b \" awful triangular Vietnamese plastic drinking car \" ?\n",
      "A:\n",
      "b\n",
      "----\n",
      "Q: Which sentence has the correct adjective order: a \" glass circular German baby \" b \" circular German glass baby \" ?\n",
      "A:\n",
      "b\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d9cf628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/formal_fallacies_syllogisms_negation/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dedff29995a4744904d42036374ea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'formal_fallacies_syllogisms_negation', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0f545f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, nobody is neither an ancestor of Alvin nor a workmate of Tony. Moreover, whoever is a schoolmate of Keith is not an ancestor of Alvin. Therefore, everyone who is a schoolmate of Keith is a workmate of Tony, too.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "valid\n",
      "----\n",
      "Q: \"Is Fred a fan of Liverpool? Are supporters of Real Madrid devotees of PSG? In European football, it is sometimes difficult to keep track of the mutual admiration and dislike. The following argument seeks to clarify some such relations: First of all, nobody is neither a friend of SønderjyskE nor a friend of Borussia Dortmund. Next, being a devotee of Akhisar Belediyespor is sufficient for being a friend of Borussia Dortmund. So, necessarily, nobody is neither a devotee of Akhisar Belediyespor nor a friend of SønderjyskE.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A:\n",
      "invalid\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3d3181fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/few_shot_nlg/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d94194831c47e59f66686dd292d92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'few_shot_nlg', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "34550df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: service name=Buses_1,description=Book bus journeys from the biggest bus network in the country | offer,name=leaving_time,description=Time of bus leaving for journey, values = 8:30 am | offer,name=fare,description=Fare per ticket for journey, values = $24 | offer,name=transfers,description=Number of transfers in journey,examples =0,1, values = 0 | inform_count,count, values = 8 |  | context = user:I will be leaving from Philly to New York City.\n",
      "OUTPUT:\n",
      "There are 8 buses available. There is one leaving at 8:30 am and it costs $24. There would be 0 transfers.\n",
      "----\n",
      "INPUT: service name=Buses_1,description=Book bus journeys from the biggest bus network in the country | request,name=to_location,description=City where bus is going to | request,name=from_location,description=City where bus is leaving from |  | context = user:For next Thursday.\n",
      "OUTPUT:\n",
      "Where are you heading and where are you leaving from?\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "69f524e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/evaluating_information_essentiality/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ea643f01d14a88b219cff200a78585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'evaluating_information_essentiality', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ecc47714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the value of x? Which of the following statements is/are sufficient to answer the previous question? \n",
      "1. x = 3. \n",
      "2. x = x.\n",
      "  choice: Neither statement 1 nor statement 2 nor statements 1 and 2 taken together is sufficient\n",
      "  choice: Statement 2 alone is sufficient while statement 1 alone is insufficient\n",
      "  choice: Statement 1 alone is sufficient while statement 2 alone is insufficient\n",
      "  choice: Either statement 1 or statement 2 is sufficient\n",
      "  choice: Statement 1 and statement 2 taken together are sufficient\n",
      "A:\n",
      "Statement 1 alone is sufficient while statement 2 alone is insufficient\n",
      "----\n",
      "Question: All the students in a classroom wrote an exam. How many students are in the classroom? Which of the following statements is/are sufficient to answer the previous question? \n",
      "1. Five students failed. \n",
      "2. One student was 29th from the top and 16th from the bottom.\n",
      "  choice: Neither statement 1 nor statement 2 nor statements 1 and 2 taken together is sufficient\n",
      "  choice: Statement 2 alone is sufficient while statement 1 alone is insufficient\n",
      "  choice: Statement 1 alone is sufficient while statement 2 alone is insufficient\n",
      "  choice: Either statement 1 or statement 2 is sufficient\n",
      "  choice: Statement 1 and statement 2 taken together are sufficient\n",
      "A:\n",
      "Statement 2 alone is sufficient while statement 1 alone is insufficient\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "22c9f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/epistemic_reasoning/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56b302e19c646b08de03e910a2c9e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'epistemic_reasoning', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "42c26da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "\n",
      "Premise: Taylor recognizes that a person is on a four wheeler kicking up a lot of dirt in a dune. Hypothesis: Taylor recognizes that the person is riding a four wheeler.\n",
      "Relation:\n",
      "entailment\n",
      "----\n",
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "\n",
      "Premise: Robert understands that William suspects that street vendors preparing and serving grilled food. Hypothesis: William suspects that street vendors preparing and serving grilled food.\n",
      "Relation:\n",
      "entailment\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e88ae07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/empirical_judgments/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2142a7376b6c43fd8065c3af4354425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'empirical_judgments', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "67834585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine whether a given sentence asserts a causal, correlative, or neutral relation between two events. If the sentence asserts a causal relation respond causal, if the sentence asserts a correlative relation respond correlative, if the sentence asserts neither a causal nor a correlative relation between two events respond neutral.\n",
      "\n",
      "Sentence: If I do yoga during the morning, I am more calm in the afternoon.\n",
      "Relation:\n",
      "correlative\n",
      "----\n",
      "Determine whether a given sentence asserts a causal, correlative, or neutral relation between two events. If the sentence asserts a causal relation respond causal, if the sentence asserts a correlative relation respond correlative, if the sentence asserts neither a causal nor a correlative relation between two events respond neutral.\n",
      "\n",
      "Sentence: Drinking that liquid upsets my stomach.\n",
      "Relation:\n",
      "causal\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8924459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/elementary_math_qa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567aeb3d06c74355aa14e6e908a59458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "30531"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'elementary_math_qa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "253a18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the result of the following arithmetic operations?:divide(multiply(12, multiply(1.6, 1000)), 48)\n",
      " choice:200 m\n",
      " choice:600 m\n",
      " choice:300 m\n",
      " choice:100 m\n",
      " choice:400 m\n",
      "A:\n",
      "400 m\n",
      "----\n",
      "What is the result of the following arithmetic operations?:multiply(divide(divide(400, 4), 8), 7)\n",
      " choice:87.1\n",
      " choice:87.5\n",
      " choice:87.2\n",
      " choice:87.0\n",
      " choice:87.4\n",
      "A:\n",
      "87.5\n",
      "----\n",
      "What is the answer to the following math word problem?:a sporting goods store sold 64 frisbees in one week , some for $ 3 and the rest for $ 4 each . if receipts from frisbee sales for the week totaled $ 196 what is the fewest number of $ 4 frisbees that could have been sold ?\n",
      " choice:2\n",
      " choice:8\n",
      " choice:24\n",
      " choice:12\n",
      " choice:4\n",
      "A:\n",
      "4\n",
      "----\n",
      "What is the answer to the following math word problem, with the given hint?:what decimal fraction is 20 ml of a litre ?\n",
      "divide 20 by 1000,\n",
      " choice:. 05\n",
      " choice:none of these\n",
      " choice:. 2\n",
      " choice:. 02\n",
      " choice:0.002\n",
      "A:\n",
      ". 02\n",
      "----\n",
      "What is the answer to the following math word problem, with the given hint?:united telephone charges a base rate of $ 6.00 for service , plus an additional charge of $ 0.25 per minute . atlantic call charges a base rate of $ 12.00 for service , plus an additional charge of $ 0.20 per minute . for what number of minutes would the bills for each telephone company be the same ?\n",
      "divide(subtract(12.00, 6.00), subtract(0.25, 0.20))\n",
      " choice:120 minutes\n",
      " choice:20 minutes\n",
      " choice:140 minutes\n",
      " choice:110 minutes\n",
      " choice:160 minutes\n",
      "A:\n",
      "120 minutes\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3c73d8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/discourse_marker_prediction/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3615f98a3e2a47d09a18b8cd384314cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'discourse_marker_prediction', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c4c5ceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flowers were truly unique.\n",
      "  choice: Sometimes, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Essentially, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Only, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Already, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Rather, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Increasingly, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Together, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Normally, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Lastly, cakes by Maria & Joe put together a fabulous cake.\n",
      "  choice: Previously, cakes by Maria & Joe put together a fabulous cake.\n",
      "Lastly, cakes by Maria & Joe put together a fabulous cake.\n",
      "----\n",
      "Re: New Playing Surface to be Installed in San Francisco.\n",
      "  choice: Surely, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Basically, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Interestingly, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Afterward that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Nonetheless that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Firstly, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Truthfully, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Incidentally, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Further, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "  choice: Meantime, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "Truthfully, that post was more of an expression of my disdain for sports cliches than it was an attempt to smack the Niners.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6602cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/disambiguation_qa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7102c844874d4b01b3db8335e74a8ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'disambiguation_qa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a57250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Today is 3/5, and it is Jane's second time in the year 1973 to see a meteor shower. What is the date a month ago in MM/DD/YYYY?\n",
      "A:\n",
      "02/05/1973\n",
      "----\n",
      "Q: Jane thinks today is 6/18/2019, but John thinks today is 6/19/2019. Jane is correct. What is the date tomorrow in MM/DD/YYYY?\n",
      "A:\n",
      "06/19/2019\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ef89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/date_understanding/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1114.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'date_understanding', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f95b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Today is Christmas Eve of 1937. What is the date a month ago in MM/DD/YYYY?\n",
      "A:\n",
      "11/24/1937\n",
      "----\n",
      "Q: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date one week from today in MM/DD/YYYY?\n",
      "A:\n",
      "01/09/1961\n",
      "----\n",
      "Q: Jane was born on the last day of Feburary in 2000. Today is her 16-year-old birthday. What is the date a month ago in MM/DD/YYYY?\n",
      "A:\n",
      "01/29/2016\n",
      "----\n",
      "Q: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date today in MM/DD/YYYY?\n",
      "A:\n",
      "01/02/1961\n",
      "----\n",
      "Q: Today is the last day of the first quarter of 2008. What is the date today in MM/DD/YYYY?\n",
      "A:\n",
      "03/31/2008\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7bb03721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/cs_algorithms/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692fbfa7dcd140f3af3e41c556ccc19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'cs_algorithms', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] #+ d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] #+ d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c6766a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: VIRVRHRSTQBLLSYPZDVYCFPSQRXNA SPLYVHLWMLDJVYMQTOZMVOJF\n",
      "Length of longest common subsequence:\n",
      "8\n",
      "----\n",
      "Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: SCZFZGCCQQLB OJDXI\n",
      "Length of longest common subsequence:\n",
      "0\n",
      "----\n",
      "Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: RLXEHVGPC LDOOBAOCQPRJKZWOKUPPEHEAZIZPLSB\n",
      "Length of longest common subsequence:\n",
      "4\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: } { ( [\n",
      "Valid/Invalid?\n",
      "Invalid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: [ [ [ { [ [ ] { { } ( ) } [ ] ] } ] ] ]\n",
      "Valid/Invalid?\n",
      "Valid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: [ { } ]\n",
      "Valid/Invalid?\n",
      "Valid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: ) } { [ ) } [ } { )\n",
      "Valid/Invalid?\n",
      "Invalid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: ( ) [ ( ) ] ( { } )\n",
      "Valid/Invalid?\n",
      "Valid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: [ {\n",
      "Valid/Invalid?\n",
      "Invalid\n",
      "----\n",
      "Determine whether the given sequence of parentheses is properly matched.\n",
      "\n",
      "Sequence: [ { } ] { } { [ ] [ ] } [ ] ( { ( ) } )\n",
      "Valid/Invalid?\n",
      "Valid\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 10)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8dc74cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/crash_blossom/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7befac676a49a7aee2e48c6f2a8dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'crash_blossom', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "47f4e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the part of speech (verb, adjective, noun, or preposition) of the specified word in the following headlines.\n",
      "\n",
      "In the following sentence, what part of speech is attacks? Sentence: Major attacks in the pipeline\n",
      "A:\n",
      "noun\n",
      "----\n",
      "Identify the part of speech (verb, adjective, noun, or preposition) of the specified word in the following headlines.\n",
      "\n",
      "In the following sentence, what part of speech is fries? Sentence: McDonald’s fries the holy grail for potato growers\n",
      "A:\n",
      "noun\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "39ec858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/contextual_parametric_knowledge_conflicts/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4464ea3fc78431aa0acc48b2142d2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'contextual_parametric_knowledge_conflicts', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a8471f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the answer to the question, assuming the context is true.\n",
      "\n",
      "\n",
      "Question: who was president during the second world war?\n",
      "Context: David Dencik and Vice President Henry A. Wallace won the election of 1940 , and were at the helm of the nation as it prepared for and entered World War II . Roosevelt sought and won an unprecedented fourth term in office in 1944 , but this time with David Dencik as his Vice President . Roosevelt , who had been a victim of Polio early in life , died in April 1945 , and Truman assumed the Presidency through the end of the war .\n",
      "  choice: President Franklin Delano Roosevelt\n",
      "  choice: David Dencik\n",
      "Answer:\n",
      "David Dencik\n",
      "----\n",
      "What is the answer to the question, assuming the context is true.\n",
      "\n",
      "\n",
      "Question: nikki 's sister on young and the restless?\n",
      "Context: Nikki was introduced with her sister Mike Grady in 1978 . The following year , she killed her abusive father Nick Reed ( Quinn Redeker ) in self - defense after he drunkenly tried to rape her . During the murder trial , Jill Foster ( Brenda Dickson ) hired a Nick look - alike named Joseph Thomas to scare Nikki and Mike Grady . Following the trial , she dated Paul Williams ( Doug Davidson ) , but it ended after he gave her a venereal disease . Nikki would go on to romance Greg Foster ( Wings Hauser ) . While Paul opposed the relationship , she ultimately married Greg . Rose Deville ( Darlene Conley ) signed Nikki to a `` modelling agency '' which was later revealed to be a prostitution ring . Nikki resisted a customer , who later had a heart attack . A man named Tony was charged for the murder , with Greg representing him . Nikki was threatened not to testify in the case or Greg would be killed . She decided to leave town , but returned a little while later . Nikki would later join the New World Commune and Paul would join her . However , they discovered that it was actually a cult , and Nikki would become pregnant after being raped by a cult leader , Ian Ward . Steven Williams and Peggy Brooks worked to expose the cult , and Nikki ended up escaping to Chicago . She gave birth to the child , later to be known as Dylan McAvoy , in a church , but passed out during labor and the nuns took the baby away . Greg , thinking Nikki and Paul had reunited , divorced Nikki . Once Nikki returned to Genoa City , she became a stripper while romancing Kevin Bancroft ( Christopher Holder ) . Kevin 's mother Allison Bancroft ( Lynn Wood ) hated Nikki , and bought a pornographic film titled Hot Hips , purportedly starring Nikki , hoping Kevin would leave her , which he did . A criminal named Tony DiSalvo distributed the film . Nikki married him in hopes of getting her hands on the film rights , and later found out she was n't even in the film . Paul and his partner Andy Richards investigated Tony and killed him .\n",
      "  choice: Mike Grady\n",
      "  choice: Casey\n",
      "Answer:\n",
      "Mike Grady\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e845741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/conceptual_combinations/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c93241a3b44da794170589dcc1942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'conceptual_combinations', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cf378467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: crowded funerals. Question: Which of the following sentences best characterizes crowded funerals?\n",
      " option: Crowded funerals are held at mass graves.\n",
      " option: Very popular people have crowded funerals.\n",
      " option: Crowded funerals are good places to go shopping.\n",
      " option: Crowded funerals describe a crowd of people burying each other.\n",
      "Answer:\n",
      "Very popular people have crowded funerals.\n",
      "----\n",
      "concept: bar stool. Question: Which of the following sentences best characterizes bar stools?\n",
      " option: Bar stools are common in ballet classes.\n",
      " option: Bar stools have long legs.\n",
      " option: Bar stools are bowel movements in pubs.\n",
      " option: Bar stools are stools that block an entrance.\n",
      "Answer:\n",
      "Bar stools have long legs.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "27056f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/common_morpheme/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d7859fa77540fea321877efc96c9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'common_morpheme', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9ede0d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Quel est le morphème commun parmi ces mots: omnipotent, possibilité, possession, potentiel\n",
      "  choice: maître\n",
      "  choice: tout-puissant\n",
      "  choice: biens\n",
      "  choice: probabilité\n",
      "A:\n",
      "maître\n",
      "----\n",
      "Q: Quel est le morphème commun parmi ces mots: communiquer, migration, mutuel, rémunerer\n",
      "  choice: parler\n",
      "  choice: se déplacer\n",
      "  choice: changer\n",
      "  choice: payer\n",
      "A:\n",
      "changer\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "837d7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/codenames/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b5d24c6c1740008a87c7d6701cd417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'codenames', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9f6754ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Try to identify the 2 words best associated with the word OCEAN from the following list: mug, head, pumpkin, ball, ambulance, seal, cotton, force, wave, boom, day, engine, new york, jet, mammoth, helicopter. Give your answer in alphabetical order.\n",
      "A:\n",
      "seal, wave\n",
      "----\n",
      "Q: Try to identify the 1 word best associated with the word MYTHOLOGY from the following list: boot, wave, centaur, calf, antarctica, slip , helicopter, vet. Give your answer in alphabetical order.\n",
      "A:\n",
      "centaur\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "29e888a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/bridging_anaphora_resolution_barqa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c142bf5af5d4df3ad9b2e427e31daab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'bridging_anaphora_resolution_barqa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ef4fa846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: At age eight, Josephine Baker was sent by her mother to a white woman's house to do chores in exchange for meals and a place to sleep--a place in the basement with the coal. At age 19, she was a Paris sensation, transformed from unwanted child to international sex symbol in just over a decade. It is the stuff of dreams, but also of traumas. Only the bravest spirits survive such roller coasters. And, for Ms. Baker, the ride was far from over. Her bare breasts, her dancing, her voice, her beauty and, perhaps most famously, her derriere, were prominent attractions, but courage of a rare sort made her remarkable life possible. Bricktop, another American black woman who found a measure of fame in Paris, said: \"I don't think I've ever known anyone with a less complicated view of life, or whose life was more complicated than Josephine's.\" Men were a constant complication. Baker had lots of them. But she didn't trust them and didn't reward trust. As she saw one key love affair, the problem wasn't her infidelity, it was his jealousy. Her appetite for children also was large. She adopted 12 of assorted races, naming them the Rainbow Tribe, and driving her husband first to despair and then to Argentina. She made money, but spent more. Friends pitched in. Finally, Prince Rainier and Princess Grace saved her with the offer of a house in Monaco. Another lifelong complication, as Phyllis Rose makes clear in \"Jazz Cleopatra: Josephine Baker in Her Time\" (Doubleday, 321 pages, $22.50), was racism. Baker had the good luck to arrive in 1925 Paris, where blacks had become exotic. African art was in vogue and some intellectuals were writing breathlessly of a dawning age to be inspired by blacks. To be exotic was to be patronized as well as prized, but for the most part Paris was a friendly island in a racist world. Baker had bitter experience of bigotry from her St. Louis childhood and her days in New York theater, where she was judged too dark for an all-black chorus line (performing of course for all-white audiences). Paris loved her at first sight. \"She just wiggled her fanny and all the French fell in love with her,\" sniffed the literary world's Maria Jolas, not entirely inaccurately. \"One can hardly overemphasize the importance of her rear end,\" Ms. Rose writes. Ms. Rose, who teaches literature at Wesleyan University, quickly proceeds to overemphasize, claiming that Baker's dancing \"had uncovered a new region for desire\" and thereby ignoring centuries of tributes to the callipygous. \"Jazz Cleopatra\" contains other, more important, false notes that undermine what is, for the most part, a lively account of a life already familiar from earlier works. It is easy to see why Baker, a free spirit who broke many of the restraints convention places on women, attracts Ms. Rose, the author of \"Parallel Lives,\" a wonderful study of Victorian marriage. Still, even the title raises questions about the author's vision of her subject. Baker's art was jazz only by the widest stretch of the term. To find parallels, other than sexual appeal, with Cleopatra, requires an equal stretch. Baker was 68 years old when she died in Paris, two days after the sold-out opening of her newest show: a movie-like ending to what was a cinematic life. In fact, Ms. Baker played scenes in Casablanca that could have made it into \"Casablanca.\" During World War II, her uncomplicated view of life led her to the conclusion that the Nazis were evil and must be resisted, a decision made by only about 2% of French citizens. She was devoted to Charles de Gaulle's cause, accepting great financial sacrifice and considerable risk to become first a spy and then a one-woman USO tour for the forces of Free France. In Humphrey Bogart's nightclub, Victor Laszlo leads Free French sympathizers in \"La Marseillaise\" to drown out the Nazis. The night the Germans occupied all of France, Baker performed in Casablanca. The Free French wore black arm bands, and when she sang \"J'ai deux amours\" they wept. Ms. Rose is best on the early years and World War II.\n",
      "Question: the early years of what?\n",
      "Answer:\n",
      "Baker, a free spirit who broke many of the restraints convention places on women,\n",
      "----\n",
      "Context: As a presidential candidate in 1980, George Bush forthrightly expressed his position on abortion in an interview with Rolling Stone magazine published that March. What did he think of the Supreme Court's decision legalizing abortion? \"I happen to think it was right,\" Mr. Bush said flatly. A few months later, Mr. Bush became Ronald Reagan's running mate. Suddenly, George Bush the pro-choice advocate became George Bush the anti-abortionist. And the vacillation didn't end there. Just a month ago, Mr. Bush sternly threatened to veto a pending welfare bill if it provided any abortion funds, except to save a woman's life. Then, two weeks ago--declaring that \"I'm not looking for any conflict over this\"--the president said he would consider a compromise to fund abortions for poor women in cases of rape and incest. But only four days after that, Mr. Bush resurrected the veto threat. \"I do not support federal funding for abortions except where the mother's life is threatened,\" he proclaimed, and finally vetoed the measure last weekend. So what does George Bush really believe? The answer is so murky that it is beginning to get this popular president in trouble with each of the increasingly vocal, increasingly powerful sides of the abortion issue. The result is mistrust and criticism from all around. Anti-abortion forces regard him as at best an uncertain ally. \"In all honesty if you ask me,'Is this man a true believer?' I don't know,\" says John Fowler, head of the Washington-based Ad Hoc Committee in Defense of Life Inc. Yet abortion-rights forces remain bitterly critical. Douglas Gould, vice president of communications for the Planned Parenthood Federation of America, calls Mr. Bush's position on the abortion-funding issue \"extremely cruel,\" adding: \"The guy hasn't done one thing about prevention. He's totally geared to a punitive position.\" Mr. Bush is plainly uncomfortable with the entire abortion question. For most of the past nine years, he has striven to convince anti-abortion activists of his stalwart support for their position. But ever since the Supreme Court's Webster vs. Reproductive Health Services decision this year changed the political landscape of the abortion issue, the president seemingly has tried just as hard to avoid saying anything more unless pressed to the wall. Many Americans still agonize over their own personal feelings about abortion. Mr. Bush's problem isn't so much that he seems to be agonizing over the issue as it is that he seems to vacillate on it. The political risk would be far less if the president drew a firm line and hewed to it, experts insist. \"If you have a position, you're better off to stick with it than to move around very much,\" says Republican strategist John Sears. The need for consistency is especially acute for Mr. Bush, who, Mr. Sears maintains, lacks a strong ideological base. By his moderate Republican heritage as well as the warnings of political advisers who say the issue is vital to younger voters, the president might seem to have at least some sympathy with abortion-rights arguments. Yet he is also firmly bound by his hard-line rhetoric and promises he made to anti-abortion activists during his long pursuit of the White House. On many issues--flag-burning, for instance--his keen political sensitivities overcome such conflicts. But Mr. Bush and his advisers miscalculated the politics of the abortion issue, failing to grasp how dramatically the abortion-rights movement would be aroused following last summer's Supreme Court decision to restrict those rights in the Webster case. \"It was one of the quickest changes in public attitudes I've ever seen,\" says former Reagan pollster Richard Wirthlin. These days, when others raise the subject of abortion, the usually loquacious president can be close-mouthed almost to the point of curtness. Ten days ago he was asked to amplify the reasons behind his anti-abortion stance. \"My position is well-known and well-stated,\" he replied. A close look at his record over the last 15 years suggests that Mr. Bush has well-stated his views--on all sides of the issue. In 1974, as the U.S. representative to the United Nations, he wrote an introduction to a book on world population in which he boasted of his leadership during his term in Congress in expanding family-planning services for the poor. Running for president in early 1980, he was also quoted as supporting federal funding for abortions in cases of rape, incest and to save the life of the mother. In his Rolling Stone interview in 1980, Mr. Bush volunteered his abortion-rights remarks to contrast himself with his rival, Ronald Reagan. In addition to supporting the landmark Roe vs. Wade Supreme Court decision legalizing abortion, Mr. Bush said he opposed the constitutional ban on abortion that Mr. Reagan was promising to promote. As Mr. Reagan's running mate, though, Mr. Bush plunged headlong into the anti-abortion position, endorsing a constitutional amendment outlawing abortion. He acknowledged only one difference with Mr. Reagan--that the amendment ought to have exceptions for rape and incest as well as to save a woman's life. Throughout the early 1980s, Mr. Bush was quoted sometimes supporting federal funding for abortion in cases of rape and incest and sometimes opposing it. In April 1986, then-Vice President Bush had his staff write a letter spelling out that he would support a constitutional amendment banning abortions except in cases of rape, incest and life endangerment, but that he opposed federal funding in all but the latter case. At the GOP convention last year, he again came out for an amendment with exceptions for rape, incest and life endangerment. His rhetoric gathered momentum as he rolled into office, affirming his \"firm support of our cause\" during an anti-abortion rally three days after his inauguration last January. He again urged passage of a constitutional amendment outlawing abortion. But when the high court ruled in the Webster case in July, the president began to lower the volume.\n",
      "Question: the volume of what?\n",
      "Answer:\n",
      "rhetoric\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308701b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/analogical_similarity/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad116aef5d747f4a077039e119bad62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'analogical_similarity', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e11ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The essence of the task: for two given sentence episodes, choose a category that best describes the level of analogical similarity between them.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The teacher guided the student, compelling the student to follow the teacher. This is an example of literal similarity because the two episodes share all aspects of similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The student followed the teacher, compelling the teacher to guide the student. This is an example of surface similarity because the two episodes share all aspects of similarity except higher-order structure.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The student ignored the teacher, compelling the teacher to reprimand the student. This is an example of only objects similarity because the two episodes only share object attribute similarity and higher-order relation similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The Bible guided the worshippers, compelling the worshippers to follow the Bible. This is an example of an analogy because the two episodes share all aspects of similarity except role-filler binding similarity and object attribute similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The minor guided the chaperone, compelling the chaperone to follow the minor. This is an example of a cross mapping because the two episodes share all aspects of similarity except role-filler binding similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The Bible followed the worshippers, compelling the worshippers to guide the Bible. This is an example of a false analogy because the two episodes only share first-order relation similarity, higher-order relation similarity, and object-level isomorphism.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The cat swiped at the dog, and the dog barked at the cat. This is an example of no similarity because the two episodes share no aspects of similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote intimidated the deer, causing the deer to fear the coyote. This is an example of literal similarity because the two episodes share all aspects of similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote intimidated the deer, and the deer feared the coyote. This is an example of surface similarity because the two episodes share all aspects of similarity except higher-order relation similarity and higher-order structure similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote pounced on the deer, but the deer escaped from the coyote. This is an example of only objects similarity because the two episodes share object attribute similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The mob intimidated the witness, causing the witness to fear the mob. This is an example of an analogy because the two episodes share all aspects of similarity except role-filler binding similarity and object attribute similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The mob feared the witness, causing the mob to intimidate the witness. This is an example of a false analogy because the two episodes only share first-order relation similarity, higher-order relation similarity, and higher-order structure similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The manager relied on the employee, and the employee assisted the manager. This is an example of no similarity because the two episodes share no aspects of similarity.\n",
      "\n",
      "Q: The flea annoyed the hiker, causing the hiker to kill the flea. <-> The lizard killed the cougar, causing the lizard to annoy the cougar. This is an example of\n",
      "['a false analogy.']\n",
      "The essence of the task: for two given sentence episodes, choose a category that best describes the level of analogical similarity between them.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The teacher guided the student, compelling the student to follow the teacher. This is an example of literal similarity because the two episodes share all aspects of similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The student followed the teacher, compelling the teacher to guide the student. This is an example of surface similarity because the two episodes share all aspects of similarity except higher-order structure.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The student ignored the teacher, compelling the teacher to reprimand the student. This is an example of only objects similarity because the two episodes only share object attribute similarity and higher-order relation similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The Bible guided the worshippers, compelling the worshippers to follow the Bible. This is an example of an analogy because the two episodes share all aspects of similarity except role-filler binding similarity and object attribute similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The minor guided the chaperone, compelling the chaperone to follow the minor. This is an example of a cross mapping because the two episodes share all aspects of similarity except role-filler binding similarity.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The Bible followed the worshippers, compelling the worshippers to guide the Bible. This is an example of a false analogy because the two episodes only share first-order relation similarity, higher-order relation similarity, and object-level isomorphism.\n",
      "  The parent guided the child, compelling the child to follow the parent. <-> The cat swiped at the dog, and the dog barked at the cat. This is an example of no similarity because the two episodes share no aspects of similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote intimidated the deer, causing the deer to fear the coyote. This is an example of literal similarity because the two episodes share all aspects of similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote intimidated the deer, and the deer feared the coyote. This is an example of surface similarity because the two episodes share all aspects of similarity except higher-order relation similarity and higher-order structure similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The coyote pounced on the deer, but the deer escaped from the coyote. This is an example of only objects similarity because the two episodes share object attribute similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The mob intimidated the witness, causing the witness to fear the mob. This is an example of an analogy because the two episodes share all aspects of similarity except role-filler binding similarity and object attribute similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The mob feared the witness, causing the mob to intimidate the witness. This is an example of a false analogy because the two episodes only share first-order relation similarity, higher-order relation similarity, and higher-order structure similarity.\n",
      "  The lion intimidated the zebra, causing the zebra to fear the lion. <-> The manager relied on the employee, and the employee assisted the manager. This is an example of no similarity because the two episodes share no aspects of similarity.\n",
      "\n",
      "Q: The mouse sat on the beagle, causing the beagle to shake off the mouse. <-> The golden retriever sniffed the hamster, causing the hamster to squeak at the golden retriever. This is an example of\n",
      "['only objects similarity.']\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label)\n",
    "#     print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "158ae193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/abstract_narrative_understanding/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aa267b976c4da083a61e5674b76cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'abstract_narrative_understanding', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d8692621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what follows, we provide short narratives, each of which illustrates a common proverb. \n",
      "Narrative: Bill noticed that his friends were all getting sick. He did not get sick because he had a diet of plants and veggies. He noticed that all of his friends were obese who went to the doctor over the years. They all had issues with eating.\n",
      "This narrative is a good illustration of the following proverb:\n",
      "An apple a day keeps the doctor away\n",
      "----\n",
      "In what follows, we provide short narratives, each of which illustrates a common proverb. \n",
      "Narrative: The mob boss had no problem killing his enemies, but became emotional when one of his own comrades was shot.\n",
      "This narrative is a good illustration of the following proverb:\n",
      "There's honour among thieves\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaf87c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/auto_debugging/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 302.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'auto_debugging', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a53710bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "if x < 5:\n",
      "\tpass\n",
      "```\n",
      "What error does this program surface?\n",
      "NameError: name 'x' is not defined\n",
      "----\n",
      "```\n",
      "d = {1: 'hello', 2: 'bye', 3: 'good morning'}\n",
      "str_ = ''\n",
      "for i in range(len(d)):\n",
      "\tif i % 2 == 1:\n",
      "\t\tstr_ += d[i]\n",
      "```\n",
      "What is the value of str_ after this program executes?\n",
      "hellogood morning\n",
      "----\n",
      "```\n",
      "class MyClass():\n",
      "\tdef __init__(self, param):\n",
      "\t\tself.param = param\n",
      "x = MyClass(5)\n",
      "y = MyClass(x.param)\n",
      "```\n",
      "What is the value of x.param at the end of this program?\n",
      "5\n",
      "----\n",
      "```\n",
      "def sq(x):\n",
      "\treturn x*x\n",
      "y = sq(5)\n",
      "x = sq(3)\n",
      "```\n",
      "What is the value of x at the end of this program?\n",
      "9\n",
      "----\n",
      "```\n",
      "class MyClass():\n",
      "\tdef __init__(self):\n",
      "\t\tprint(param)\n",
      "x = MyClass(5)\n",
      "```\n",
      "What is the value of x.param at the end of this program?\n",
      "None\n",
      "----\n",
      "```\n",
      "while True\n",
      "\tprint('hello world')\n",
      "```\n",
      "What type of exception does this program produce?\n",
      "SyntaxError: invalid syntax\n",
      "----\n",
      "```\n",
      "sum = 0\n",
      "for i in range(100):\n",
      "\tsum += i\n",
      "```\n",
      "What is the value of sum immediately after the 10th time line 3 is executed?\n",
      "45\n",
      "----\n",
      "```\n",
      "y = list(map(int, ['1', 'hello', '2']))\n",
      "```\n",
      "What error does this program produce?\n",
      "ValueError: invalid literal for int() with base 10: 'hello'\n",
      "----\n",
      "```\n",
      "sum = 0\n",
      "for i in range(100, 0, -2):\n",
      "\tsum += i\n",
      "```\n",
      "What is the value of sum at the end of this program?\n",
      "2550\n",
      "----\n",
      "```\n",
      "def sq(x):\n",
      "\treturn x*x\n",
      "y = sq(5)\n",
      "x = sq(y)\n",
      "```\n",
      "What is the value of x at the end of this program?\n",
      "625\n",
      "----\n",
      "```\n",
      "import numpy as np\n",
      "x = numpy.zeros(10)\n",
      "```\n",
      "What type of exception does this program produce?\n",
      "NameError: name 'numpy' is not defined\n",
      "----\n",
      "```\n",
      "def test(x):\n",
      "\tfor i in range(2, x**(0.5)):\n",
      "\t\tif x % int(i) == 0:\n",
      "\t\t\treturn False\n",
      "\treturn True\n",
      "```\n",
      "What is test(101)?\n",
      "True\n",
      "----\n",
      "```\n",
      "import numpy\n",
      "x = numpy.zeros(10)\n",
      "x += 1\n",
      "```\n",
      "What is the value of x at the end of this program?\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "----\n",
      "```\n",
      "x = [1, 2, 3]\n",
      "print(x[2])\n",
      "```\n",
      "What is the output of this program?\n",
      "3\n",
      "----\n",
      "```\n",
      "x = [1, 2, 3]\n",
      "print(x[4])\n",
      "```\n",
      "What error does this program produce?\n",
      "IndexError: list index out of range\n",
      "----\n",
      "```\n",
      "print('1' + 2)\n",
      "```\n",
      "What type of exception does this program produce?\n",
      "TypeError: must be str, not int\n",
      "----\n",
      "```\n",
      "x = [2*i for i in range(10)]\n",
      "```\n",
      "What is x[7] after this program executes?\n",
      "12\n",
      "----\n",
      "```\n",
      "class MyClass():\n",
      "\tdef __init__(self, param):\n",
      "\t\tself.param = param\n",
      "x = MyClass(5)\n",
      "y = MyClass(x.param)\n",
      "```\n",
      "What is the value of y.param at the end of this program?\n",
      "5\n",
      "----\n",
      "```\n",
      "for i in range(10):\n",
      "\tpass\n",
      "```\n",
      "What is the value of i the third time line 2 is executed?\n",
      "2\n",
      "----\n",
      "```\n",
      "def r(n):\n",
      "\tif n == 1:\n",
      "\t\treturn 1\n",
      "\telse:\n",
      "\t\treturn n + r(n-1)\n",
      "z = r(10)\n",
      "y = r(-10)\n",
      "```\n",
      "What is the value of z at immediately after line 7 has executed?\n",
      "55\n",
      "----\n",
      "```\n",
      "def sq(x):\n",
      "\treturn x*x\n",
      "y = sq(5)\n",
      "x = sq(y)\n",
      "```\n",
      "What is the value of y at the end of this program?\n",
      "25\n",
      "----\n",
      "```\n",
      "x = set([1, 1, 2, 3])\n",
      "```\n",
      "What is the value of x after this program executes?\n",
      "{1, 2, 3}\n",
      "----\n",
      "```\n",
      "x = 5\n",
      "if x > 3:\n",
      "\ty = 'hello'\n",
      "```\n",
      "What is the value of y at line 4?\n",
      "hello\n",
      "----\n",
      "```\n",
      "def f1():\n",
      "\treturn str(x) + 'hello'\n",
      "def f2():\n",
      "\treturn f1(2*x)\n",
      "x = f2(524)\n",
      "```\n",
      "What is the value of x at the end of this program?\n",
      "'1048hello'\n",
      "----\n",
      "```\n",
      "import numpy as np\n",
      "x = np.zeros(10)\n",
      "What is the value of x at the end of this program?\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "----\n",
      "```\n",
      "class MyClass():\n",
      "\tdef __init__(self, param):\n",
      "\t\tself.param = param\n",
      "x = MyClass(5)\n",
      "y = MyClass(x.param)\n",
      "```\n",
      "What type of object is x?\n",
      "MyClass\n",
      "----\n",
      "```\n",
      "x = 1\n",
      "if x > 3:\n",
      "\ty = 'hello'\n",
      "```\n",
      "What is the value of y at line 4?\n",
      "None\n",
      "----\n",
      "```\n",
      "class MySuperClass():\n",
      "\tdef __init(self, superparam):\n",
      "\t\tself.superparam = superparam\n",
      "class MySubClass(MySuperClass):\n",
      "\tdef __init__(self, subparam):\n",
      "\t\tself.subparam = subparam\n",
      "x = MySuperClass(5)\n",
      "y = MySubClass(x.superparam)\n",
      "```\n",
      "What is the value of y.subparam at the end of this program?\n",
      "5\n",
      "----\n",
      "```\n",
      "x = 5\n",
      "y = 2\n",
      "while(x < 7):\n",
      "\ty += 2\n",
      "```\n",
      "What is the value of y at the end of this program?\n",
      "6\n",
      "----\n",
      "```\n",
      "def f1(x):\n",
      "\tx[0] += 1\n",
      "y = f1(1)\n",
      "z = f2([1])\n",
      "r = f1('q')\n",
      "```\n",
      "What type of exception does this program produce?\n",
      "TypeError: 'int' object is not subscriptable\n",
      "----\n",
      "```\n",
      "def r(n):\n",
      "\tif n == 1:\n",
      "\t\treturn 1\n",
      "\telse:\n",
      "\t\treturn n + r(n-1)\n",
      "z = r(10)\n",
      "y = r(-10)\n",
      "```\n",
      "What type of exception does this program produce?\n",
      "RecursionError: maximum recursion depth exceeded in comparison\n",
      "----\n",
      "```\n",
      "sum = 0\n",
      "for i in range(100, 0, -2):\n",
      "\tsum += i\n",
      "```\n",
      "What is the value of after the third time line 3 is executed?\n",
      "294\n",
      "----\n",
      "```\n",
      "x = 5\n",
      "y = 3\n",
      "z = 7\n",
      "x = y + x\n",
      "```\n",
      "What is the value of x at line 2?\n",
      "5\n",
      "----\n",
      "```\n",
      "x = [i for i in range(10)]\n",
      "for x_elem in x:\n",
      "\tx_elem *= 3\n",
      "```\n",
      "What is x at the end of this program?\n",
      "[0, 3, 6, 9, 12, 15, 18, 21, 24, 27]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 5)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "#     if id_ not in idx:\n",
    "#         continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "42346a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/cryobiology_spanish/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d71307eae347d084c99d52cee09c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'cryobiology_spanish', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1fb1e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a la siguiente pregunta del ámbito de la criobiología. \n",
      "Q: ¿Qué tiene una tasa de supervivencia superior al espermatozoide?\n",
      "  choice: el embrión\n",
      "  choice: alga chlorella\n",
      "A:\n",
      "el embrión\n",
      "----\n",
      "Responda a la siguiente pregunta del ámbito de la criobiología. \n",
      "Q: ¿Qué significa DMSO?\n",
      "  choice: Dimetil Sulfóxido\n",
      "  choice: Demencia mesenterico\n",
      "A:\n",
      "Dimetil Sulfóxido\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "607b76f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/hindi_question_answering/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd27e18127b48d7a6c6d9eff6665a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'hindi_question_answering', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ad6af65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: बेक का जन्म जर्मनी के पोमेरनियन कस्बे (वर्तमान में पोलैण्ड के स्लुप्स्क में) में 1944 में हुआ और उनका पालन पोषण हनोवर में हुआ। उन्होंने अपनी उच्च शिक्षा फ्रायबर्ग विश्वविद्यालय में विधि (कानून) में की। सन् 1966 से और उसके बाद उन्होंने मुनिख़ से समाजशास्त्र, दर्शनशास्त्र, मानस शास्त्र और राजनीति विज्ञान में की। पीएचडी की उपाधि प्राप्त करने के बाद वर्ष 1972 से, वो मुनिख़ में समाजशास्त्री के रूप में कार्य आरम्भ किया। वर्ष 1979 में उन्होंने विश्वविद्यालय में व्याख्याता के लिए आधिकारिक अर्हता प्राप्त की। उन्होंने म्युएन्स्टर (1979–1981) तथा बमबर्ग (1979–1981) में विश्वविद्यालय में प्रोफेसर के रूप में नियुक्ति प्राप्त की। सन् 1992 से निधन के समय तक वो मुनिख़ विश्वविद्यालय में समाजशास्त्र के प्रोफेसर तथा इसी विश्वविद्यालय के समाजशास्त्र संस्थान के निदेशक रहे। उन्हें विभिन्न अन्तर्राष्ट्रीय पुरस्कार और सम्मान प्राप्त हुये। उन्हें जर्मन समाजशास्त्र समाज के परिषद और कार्यकारी बोर्ड के लिए भी चुना गया। 1966 में बेक ने किस विश्वविद्यालय में पढ़ाई की थी?\n",
      "A:\n",
      "मुनिख़\n",
      "----\n",
      "Q: 2006 में लिंकिन पार्क नई सामग्री पर काम करने के लिए रिकॉर्डिंग स्टूडियो में लौटे। एल्बम के निर्माण के लिए, बैंड ने निर्माता रिक रूबिन को चुना। हालांकि शुरूआत में 2006 के दौरान एल्बम जारी करने की बात चली थी, मगर यह 2007 तक टल गया। बैंड ने अगस्त, 2006 में तीस से पचास गीत रिकॉर्ड किए थे, जब शिनोडा ने एल्बम का काम आधा ख़त्म होने की बात कही थी। बेनिंगटन ने बाद में कहा कि नया एल्बम, अपनी पिछली न्यू मेटल ध्वनि से दूर हटेगा। वार्नर ब्रदर्स रिकॉर्ड्स ने आधिकारिक तौर पर घोषणा की कि मिनट्स टू मिडनाइट शीर्षक वाला बैंड का तीसरा स्टूडियो एल्बम, संयुक्त राज्य अमेरिका में 15 मई 2007 को जारी होगा। चौदह महीने एल्बम पर काम करते हुए बिताने के बाद, बैंड के सदस्यों ने मूल सत्रह ट्रैक्स में से पांच को निकाल कर, अपने एल्बम को और निखारने का विकल्प चुना। एल्बम के शीर्षक ने, जो डूम्स डे क्लॉक से संदर्भ रखता था, बैंड के नए गीतों के विषय के पूर्वाभास को उद्घाटित किया। मिनट्स टू मिडनाइट की पहले सप्ताह में ही 600,000 से अधिक प्रतियां बिक गईं, जिसने इसे हाल के वर्षों में सबसे सफल प्रथम सप्ताह एल्बम बनाया। एल्बम ने बिलबोर्ड चार्ट पर भी शीर्ष स्थान ग्रहण किया।एल्बम का पहला एकल, \"व्हॉट आई हैव डन\" 2 अप्रैल को जारी किया गया और एक ही सप्ताह के अंदर एमटीवी और फ्यूज़ पर प्रीमियर हुआ। यह एकल श्रोताओं द्वारा प्रशंसित था, जो बिलबोर्ड के मॉडर्न रॉक ट्रैक और मुख्यधारा के रॉक ट्रैक चार्ट पर शीर्ष स्थान पाने वाला गीत बना। यह गीत 2007 की एक्शन फ़िल्म, ट्राँसफॉर्मर्स‎ के साउंडट्रैक में भी प्रयुक्त हुआ। वर्ष के दौरान बाद में, बैंड ने अमेरिकी संगीत पुरस्कार में \"पसंदीदा ऑल्टरनेटिव कलाकार\" पुरस्कार जीता। बैंड ने 2007 और 2008 की शुरूआत में जारी, \"ब्लीड इट आउट\", \"शैडो ऑफ़ द डे\", \"गिवन अप\" और \"लीव आउट ऑल द रेस्ट\" जैसे एकलों के साथ भी सफलता अनुभव किया। बैंड ने बस्टा राइम्स के साथ उनके एकल \"वी मेड इट\" पर सहयोग दिया, जो 29 अप्रैल को जारी किया गया। एल्बम के निर्माण में बैंड ने कितना समय व्यतीत किया?\n",
      "A:\n",
      "चौदह महीने\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18eba6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/parsinlu_qa/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247524ff6fa34ebda0d153bb0f4c2804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'parsinlu_qa', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e9d62799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: حاصل عبارت ۱ - ۳ برابر است با ؟\n",
      "  choice: 4\n",
      "  choice: 7\n",
      "  choice: 2\n",
      "  choice: 6\n",
      "A:\n",
      "2\n",
      "----\n",
      "Q: در اصطلاح به هیجان انگیزترین بخش هر داستان چه می گویند؟\n",
      "  choice: اوج داستان\n",
      "  choice: پیام داستان\n",
      "  choice: آخر داستان\n",
      "  choice: اول داستان\n",
      "A:\n",
      "اوج داستان\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "423fbc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset bigbench (/gscratch/zlab/bparan/projects/cascades/data/bigbench/parsinlu_reading_comprehension/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0d3e282cac46ec8c99cd4dbad2876c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'parsinlu_reading_comprehension', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cf4205bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: passage: 'سیتروئن ژیان (به فرانسوی: Citroën Dyane) یک خودروی سوپرمینی/اقتصادی است که از سال ۱۹۶۷ تا ۱۹۸۳ توسط سیتروئن تولید می‌شد. این خودرو بر پایه مدل مشهور سیتروئن ۲CV بوده و ۱٫۹ میلیون دستگاه از آن تولید شده‌است. سیتروئن دایان پاسخ سیتروئن به مدل رنو 4CV بود که از اتاق مدرن تری نسبت به سیتروئن 2CV بهره می‌برد. سیتروئن دایان یا ژیان یکی از معدود سیتروئن‌هایی بود که طراحی بدنه آن توسط خود سیتروئن صورت نگرفت و طراحی آن به شرکت پنهار سپرده شد. هدف تولید دایان همانند 2CV ساخت خودرویی متناسب با شهرها و نیازهای خانواده و با حداقل قیمت و هزینه نگهداری بود.' question: 'ژیان تا چه سالی تولید شد؟'\n",
      "A:\n",
      "۱۹۸۳\n",
      "----\n",
      "Q: passage: 'در سال ۲۰۱۸، حدود ۸۶ میلیون تن سیب در سراسر جهان به عمل آمد؛ که تقریباً ۴۵٪ از این مقدار را چین تولید کرده بود. ایالات متحده با بیش از ۶٪ تولید دنیا دومین تولیدکنندهٔ مهم بوده‌است. ترکیه جایگاه سوم و ایتالیا، هندوستان و لهستان مقام‌های بعدی را داشته‌اند. سیب معمولاً به صورت خام مصرف می‌شود، اما در بسیاری از غذاها (بویژه دسرها) و نوشیدنی‌ها هم استفاده می‌گردد. در میان عوام آثار مثبت بسیاری به سیب نسبت داده می‌شود. با این حال پروتئین‌های متفاوت موجود در سیب دو نوع آلرژی را سبب می‌شوند. تولید جهانی سیب در سال ۲۰۱۷ برابر با ۸۳٫۱ میلیون تن بوده‌است. چین تقریباً ۵۰ درصد از این مجموع را تولید کرد (جدول). و اروپا به همراه ترکیه ۱۷٪ محصول را تولید کردند، بقیهٔ تولیدکنندگان مهم که هر کدام کمتر از ۳–۵٪ از محصول کل دنیا را تولید کردند، عبارت بودند از: ایالات متحده، ترکیه و لهستان.' question: 'بزرگترین تولید کننده سیب چه کشوریست؟'\n",
      "A:\n",
      "چین\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(inputs), 2)\n",
    "for id_, (inp, label) in enumerate(zip(inputs, labels)):\n",
    "#     if label[0] != 'Unknown':\n",
    "#         continue\n",
    "    if id_ not in idx:\n",
    "        continue\n",
    "    print(inp)\n",
    "    print(label[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca944193",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig boolean_expressions not found. Available: ['abstract_narrative_understanding', 'anachronisms', 'analogical_similarity', 'analytic_entailment', 'arithmetic', 'ascii_word_recognition', 'authorship_verification', 'auto_categorization', 'auto_debugging', 'bbq_lite_json', 'bridging_anaphora_resolution_barqa', 'causal_judgment', 'cause_and_effect', 'checkmate_in_one', 'chess_state_tracking', 'chinese_remainder_theorem', 'cifar10_classification', 'code_line_description', 'codenames', 'color', 'common_morpheme', 'conceptual_combinations', 'conlang_translation', 'contextual_parametric_knowledge_conflicts', 'crash_blossom', 'crass_ai', 'cryobiology_spanish', 'cryptonite', 'cs_algorithms', 'dark_humor_detection', 'date_understanding', 'disambiguation_qa', 'discourse_marker_prediction', 'disfl_qa', 'dyck_languages', 'elementary_math_qa', 'emoji_movie', 'emojis_emotion_prediction', 'empirical_judgments', 'english_proverbs', 'english_russian_proverbs', 'entailed_polarity', 'entailed_polarity_hindi', 'epistemic_reasoning', 'evaluating_information_essentiality', 'fact_checker', 'fantasy_reasoning', 'few_shot_nlg', 'figure_of_speech_detection', 'formal_fallacies_syllogisms_negation', 'gem', 'gender_inclusive_sentences_german', 'general_knowledge', 'geometric_shapes', 'goal_step_wikihow', 'gre_reading_comprehension', 'hhh_alignment', 'hindi_question_answering', 'hindu_knowledge', 'hinglish_toxicity', 'human_organs_senses', 'hyperbaton', 'identify_math_theorems', 'identify_odd_metaphor', 'implicatures', 'implicit_relations', 'intent_recognition', 'international_phonetic_alphabet_nli', 'international_phonetic_alphabet_transliterate', 'intersect_geometry', 'irony_identification', 'kanji_ascii', 'kannada', 'key_value_maps', 'known_unknowns', 'language_games', 'language_identification', 'linguistic_mappings', 'linguistics_puzzles', 'list_functions', 'logic_grid_puzzle', 'logical_args', 'logical_deduction', 'logical_fallacy_detection', 'logical_sequence', 'mathematical_induction', 'matrixshapes', 'metaphor_boolean', 'metaphor_understanding', 'minute_mysteries_qa', 'misconceptions', 'misconceptions_russian', 'mnist_ascii', 'modified_arithmetic', 'moral_permissibility', 'movie_dialog_same_or_different', 'movie_recommendation', 'mult_data_wrangling', 'multiemo', 'natural_instructions', 'navigate', 'nonsense_words_grammar', 'novel_concepts', 'object_counting', 'odd_one_out', 'operators', 'paragraph_segmentation', 'parsinlu_qa', 'parsinlu_reading_comprehension', 'penguins_in_a_table', 'periodic_elements', 'persian_idioms', 'phrase_relatedness', 'physical_intuition', 'physics', 'physics_questions', 'play_dialog_same_or_different', 'polish_sequence_labeling', 'presuppositions_as_nli', 'qa_wikidata', 'question_selection', 'real_or_fake_text', 'reasoning_about_colored_objects', 'repeat_copy_logic', 'rephrase', 'riddle_sense', 'ruin_names', 'salient_translation_error_detection', 'scientific_press_release', 'semantic_parsing_in_context_sparc', 'semantic_parsing_spider', 'sentence_ambiguity', 'similarities_abstraction', 'simp_turing_concept', 'simple_arithmetic_json', 'simple_arithmetic_json_multiple_choice', 'simple_arithmetic_json_subtasks', 'simple_arithmetic_multiple_targets_json', 'simple_ethical_questions', 'simple_text_editing', 'snarks', 'social_iqa', 'social_support', 'sports_understanding', 'strange_stories', 'strategyqa', 'sufficient_information', 'suicide_risk', 'swahili_english_proverbs', 'swedish_to_german_proverbs', 'symbol_interpretation', 'temporal_sequences', 'tense', 'timedial', 'topical_chat', 'tracking_shuffled_objects', 'understanding_fables', 'undo_permutation', 'unit_conversion', 'unit_interpretation', 'unnatural_in_context_learning', 'vitaminc_fact_verification', 'what_is_the_tao', 'which_wiki_edit', 'winowhy', 'word_sorting', 'word_unscrambling']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# international_phonetic_alphabet_transliterate too \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigbench\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboolean_expressions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# inputs = [x.split('\\n')[0] for x in inputs]\u001b[39;00m\n",
      "File \u001b[0;32m/gscratch/cse/bparan/tools/envs/work/lib/python3.9/site-packages/datasets/load.py:1675\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m ignore_verifications \u001b[38;5;241m=\u001b[39m ignore_verifications \u001b[38;5;129;01mor\u001b[39;00m save_infos\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/gscratch/cse/bparan/tools/envs/work/lib/python3.9/site-packages/datasets/load.py:1478\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m/gscratch/cse/bparan/tools/envs/work/lib/python3.9/site-packages/datasets/builder.py:1285\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, writer_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1285\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;66;03m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;66;03m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;66;03m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer_batch_size \u001b[38;5;241m=\u001b[39m writer_batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[0;32m/gscratch/cse/bparan/tools/envs/work/lib/python3.9/site-packages/datasets/builder.py:303\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/gscratch/cse/bparan/tools/envs/work/lib/python3.9/site-packages/datasets/builder.py:444\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS:\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilderConfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# if not using an existing config, then create a new config on the fly with config_kwargs\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m builder_config:\n",
      "\u001b[0;31mValueError\u001b[0m: BuilderConfig boolean_expressions not found. Available: ['abstract_narrative_understanding', 'anachronisms', 'analogical_similarity', 'analytic_entailment', 'arithmetic', 'ascii_word_recognition', 'authorship_verification', 'auto_categorization', 'auto_debugging', 'bbq_lite_json', 'bridging_anaphora_resolution_barqa', 'causal_judgment', 'cause_and_effect', 'checkmate_in_one', 'chess_state_tracking', 'chinese_remainder_theorem', 'cifar10_classification', 'code_line_description', 'codenames', 'color', 'common_morpheme', 'conceptual_combinations', 'conlang_translation', 'contextual_parametric_knowledge_conflicts', 'crash_blossom', 'crass_ai', 'cryobiology_spanish', 'cryptonite', 'cs_algorithms', 'dark_humor_detection', 'date_understanding', 'disambiguation_qa', 'discourse_marker_prediction', 'disfl_qa', 'dyck_languages', 'elementary_math_qa', 'emoji_movie', 'emojis_emotion_prediction', 'empirical_judgments', 'english_proverbs', 'english_russian_proverbs', 'entailed_polarity', 'entailed_polarity_hindi', 'epistemic_reasoning', 'evaluating_information_essentiality', 'fact_checker', 'fantasy_reasoning', 'few_shot_nlg', 'figure_of_speech_detection', 'formal_fallacies_syllogisms_negation', 'gem', 'gender_inclusive_sentences_german', 'general_knowledge', 'geometric_shapes', 'goal_step_wikihow', 'gre_reading_comprehension', 'hhh_alignment', 'hindi_question_answering', 'hindu_knowledge', 'hinglish_toxicity', 'human_organs_senses', 'hyperbaton', 'identify_math_theorems', 'identify_odd_metaphor', 'implicatures', 'implicit_relations', 'intent_recognition', 'international_phonetic_alphabet_nli', 'international_phonetic_alphabet_transliterate', 'intersect_geometry', 'irony_identification', 'kanji_ascii', 'kannada', 'key_value_maps', 'known_unknowns', 'language_games', 'language_identification', 'linguistic_mappings', 'linguistics_puzzles', 'list_functions', 'logic_grid_puzzle', 'logical_args', 'logical_deduction', 'logical_fallacy_detection', 'logical_sequence', 'mathematical_induction', 'matrixshapes', 'metaphor_boolean', 'metaphor_understanding', 'minute_mysteries_qa', 'misconceptions', 'misconceptions_russian', 'mnist_ascii', 'modified_arithmetic', 'moral_permissibility', 'movie_dialog_same_or_different', 'movie_recommendation', 'mult_data_wrangling', 'multiemo', 'natural_instructions', 'navigate', 'nonsense_words_grammar', 'novel_concepts', 'object_counting', 'odd_one_out', 'operators', 'paragraph_segmentation', 'parsinlu_qa', 'parsinlu_reading_comprehension', 'penguins_in_a_table', 'periodic_elements', 'persian_idioms', 'phrase_relatedness', 'physical_intuition', 'physics', 'physics_questions', 'play_dialog_same_or_different', 'polish_sequence_labeling', 'presuppositions_as_nli', 'qa_wikidata', 'question_selection', 'real_or_fake_text', 'reasoning_about_colored_objects', 'repeat_copy_logic', 'rephrase', 'riddle_sense', 'ruin_names', 'salient_translation_error_detection', 'scientific_press_release', 'semantic_parsing_in_context_sparc', 'semantic_parsing_spider', 'sentence_ambiguity', 'similarities_abstraction', 'simp_turing_concept', 'simple_arithmetic_json', 'simple_arithmetic_json_multiple_choice', 'simple_arithmetic_json_subtasks', 'simple_arithmetic_multiple_targets_json', 'simple_ethical_questions', 'simple_text_editing', 'snarks', 'social_iqa', 'social_support', 'sports_understanding', 'strange_stories', 'strategyqa', 'sufficient_information', 'suicide_risk', 'swahili_english_proverbs', 'swedish_to_german_proverbs', 'symbol_interpretation', 'temporal_sequences', 'tense', 'timedial', 'topical_chat', 'tracking_shuffled_objects', 'understanding_fables', 'undo_permutation', 'unit_conversion', 'unit_interpretation', 'unnatural_in_context_learning', 'vitaminc_fact_verification', 'what_is_the_tao', 'which_wiki_edit', 'winowhy', 'word_sorting', 'word_unscrambling']"
     ]
    }
   ],
   "source": [
    "# international_phonetic_alphabet_transliterate too \n",
    "d = datasets.load_dataset('bigbench', 'boolean_expressions', cache_dir=cache_dir)\n",
    "inputs = d['train']['inputs'] + d['validation']['inputs']\n",
    "# inputs = [x.split('\\n')[0] for x in inputs]\n",
    "labels = d['train']['targets'] + d['validation']['targets']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
