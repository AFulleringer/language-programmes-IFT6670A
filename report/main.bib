@inproceedings{cohan-etal-2018-discourse,
  address   = {New Orleans, Louisiana},
  author    = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  doi       = {10.18653/v1/N18-2097},
  month     = {jun},
  pages     = {615-621},
  publisher = {Association for Computational Linguistics},
  title     = {A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents},
  url       = {https://aclanthology.org/N18-2097},
  year      = {2018},
}

@article{ganesan2015rouge,
  author  = {Kavita A. Ganesan},
  journal = {arXiv preprint arXiv: Arxiv-1803.01937},
  title   = {ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks},
  year    = {2015},
}

@inproceedings{lin-hovy-2003-automatic,
  author    = {Lin, Chin-Yew  and
               Hovy, Eduard},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
  pages     = {150--157},
  title     = {Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics},
  url       = {https://aclanthology.org/N03-1020},
  year      = {2003},
}

@article{paranjape2023art,
  author  = {Bhargavi Paranjape and Scott Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},
  journal = {arXiv preprint arXiv: Arxiv-2303.09014},
  title   = {ART: Automatic multi-step reasoning and tool-use for large language models},
  year    = {2023},
}

@misc{schick2023toolformer,
  archiveprefix = {arXiv},
  author        = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  eprint        = {2302.04761},
  primaryclass  = {cs.CL},
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  year          = {2023},
}

@misc{wei2023chainofthought,
  archiveprefix = {arXiv},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  eprint        = {2201.11903},
  primaryclass  = {cs.CL},
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year          = {2023},
}

@misc{kojima2023large,
  archiveprefix = {arXiv},
  author        = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  eprint        = {2205.11916},
  primaryclass  = {cs.CL},
  title         = {Large Language Models are Zero-Shot Reasoners},
  year          = {2023},
}

@misc{imani2023mathprompter,
  archiveprefix = {arXiv},
  author        = {Shima Imani and Liang Du and Harsh Shrivastava},
  eprint        = {2303.05398},
  primaryclass  = {cs.CL},
  title         = {MathPrompter: Mathematical Reasoning using Large Language Models},
  year          = {2023},
}

@misc{shuster2022language,
  archiveprefix = {arXiv},
  author        = {Kurt Shuster and Mojtaba Komeili and Leonard Adolphs and Stephen Roller and Arthur Szlam and Jason Weston},
  eprint        = {2203.13224},
  primaryclass  = {cs.CL},
  title         = {Language Models that Seek for Knowledge: Modular Search \& Generation for Dialogue and Prompt Completion},
  year          = {2022},
}

@misc{pang2022long,
  archiveprefix = {arXiv},
  author        = {Bo Pang and Erik Nijkamp and Wojciech Kryściński and Silvio Savarese and Yingbo Zhou and Caiming Xiong},
  eprint        = {2203.07586},
  primaryclass  = {cs.CL},
  title         = {Long Document Summarization with Top-down and Bottom-up Inference},
  year          = {2022},
}

@misc{guo2022longt5,
  archiveprefix = {arXiv},
  author        = {Mandy Guo and Joshua Ainslie and David Uthus and Santiago Ontanon and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},
  eprint        = {2112.07916},
  primaryclass  = {cs.CL},
  title         = {LongT5: Efficient Text-To-Text Transformer for Long Sequences},
  year          = {2022},
}

@misc{phang2022investigating,
  archiveprefix = {arXiv},
  author        = {Jason Phang and Yao Zhao and Peter J. Liu},
  eprint        = {2208.04347},
  primaryclass  = {cs.CL},
  title         = {Investigating Efficiently Extending Transformers for Long Input Summarization},
  year          = {2022},
}
